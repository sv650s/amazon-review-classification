{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Best deep learning model turned out to be LSTM (with balanced weights) with the following architecture\n",
    "    * Embedding input layer with 300 features\n",
    "    * Spatial Dropout layer - 20%\n",
    "    * LSTM nodes: 64\n",
    "    * Output Layer - 5 nodes with softmax activation\n",
    "    * Class Weight: balanced (calculated using sklearn compute_class_weight method)\n",
    "    * Loss funcation: categorical cross entropy\n",
    "    * Optimizer: adam\n",
    "    * Accuracy Scorer: Categorical Accuracy\n",
    "* Input for this model is 200k samples of our pre-processed review body\n",
    "* With only 200k samples, LSTM did not best Logistic Regression in our score ~ LSTM (~ .33) scored about 13% lower than Logistic Regression with balanced weight (~ .46)\n",
    "* By using Random Under Sampling, we were able to improve the model score by 0.05. Generally, 2-star reviews are the most rare accounting for around 5% of our data. Under sampling would make all classes balanced which would allow us to retain only about 25% (5 classes * 5%) of our data. For our full dataset with 9 million samples, this would only give us 2.2 million samples. Because this drastically reduction of samples, I'm not sure if this will be a viable model. \n",
    "* Further investigation: we did see a plateau in model performance for Logistic Regression at around 500k samples. Perhaps training LSTM with the full dataset (9 million samples), it will be able to exceed the performance of Logistic Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling for LSTM Models (Balanced Weights)\n",
    "\n",
    "LSTM seems to be our best model so far. Since we have class imbalance for our dataset. I experimented with a couple sampling techniques to see if we can improve our model performance\n",
    "\n",
    "### Techniques that we used:\n",
    "* Random Under Sampling - this will randomly remove majority class samples until we have even distribution of all classes in our sample data. Since this technique removes a bunch of samples, I had to start with a dataset that had 1 million entries to end up with a dataset that is around 200k\n",
    "* Random Over Sampling - creates balanced class distribution in our datast by duplicating minority class samples\n",
    "* ADSYN - this is a synthetic over sampling technique where it will generate new samples by interpolating between minority class samples with an emphasis on creating samples near minority class samples that are wrongly classified by KNN classifier\n",
    "\n",
    "### Sampling Result\n",
    "* Random Under Sampling did the best to improve model performance and brought our score up by around 5% to .39 - however, this technique reduces our dataset significantly so I have concerns about viability of this technique\n",
    "* Both methods of over sampling did not beat our model with no sampling at all\n",
    "    * ADASYN actually did about 3% worse than our baseline, whereas random over sampling did roughly the same as if no sampling was done\n",
    "\n",
    "Documentation from imbalanced-learn: https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstms = report[report.model_name == \"LSTMB\"].copy().astype({\"sample_size\":np.object})\n",
    "lstms[\"display_name\"] = lstms[[\"sample_size\", \"sampling_type\"]].fillna(\"none\").apply(lambda x: x.sampling_type + \"-\" + str(x.sample_size), axis=1)\n",
    "lstms[\"ml_score\"] = ml_best.iloc[0].eval_metric\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "_ = sns.lineplot(data = lstms, y = \"eval_metric\", x = \"display_name\", sort=False, label=\"Sampling Score\")\n",
    "_ = sns.lineplot(data = lstms, y = \"ml_score\", x = \"display_name\", sort=False, color=\"r\", label=\"LRB Score\")\n",
    "_ = plt.title(\"Sampling Scores\")\n",
    "_ = plt.xlabel(\"Model + Sample Size\")\n",
    "_ = plt.ylabel(\"Score\")\n",
    "\n",
    "lstms[[\"display_name\", \"eval_metric\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "For random under sampling for LSTM\n",
    "* 2-star ratings are most commonly being misclassified as 1-star or 2-star with more being 1-star\n",
    "* 3-star ratings are most commonly being misclassified as 1-star or 4-star\n",
    "* 4-star ratings are most commonly being misclassified as 5-star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in report_best.iterrows():\n",
    "    print(f'\\n{row.model_name} Sample Size: {row.sample_size} Sampling: {row.sampling_type}')\n",
    "    print(\"Confusion Matrix\")\n",
    "    cm = json.loads(row.confusion_matrix)\n",
    "    print(pd.DataFrame(cm, index=np.arange(1, 6), columns=np.arange(1, 6)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report Histogram\n",
    "\n",
    "* Recall for our base (LRB) is much higher for minority classes - although there is a tradeoff for recall for 1-star and 5-star reviews as they are lower\n",
    "* Precision between LSTM (balanced weight) and Logistic Regression is roughly the same with LSTM using random under sampling doing the best out of all models\n",
    "* Interestingly, recall for 3-star reviews is generally worse that 4-star reviews under our tradition ML models actually does better under LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_best_histo = report_best.copy()\n",
    "report_best_histo[\"display_name\"] = report_best_histo.model_name + \"-\" + report_best_histo.sampling_type\n",
    "pu.plot_score_histograms(report_best_histo, version=2, label=\"display_name\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zraVsfC49Fx2"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "\n",
    "* Overall LSTM using only 200k samples did not do as well as our tradition ML model - logistic regression - although as we saw in previous notebooks logistic regression plateaus at around 500k samples. If we train with the full dataset for LSTM, we may be able to improve on model performance to beat our logistic regression model - will have to try this\n",
    "* With random under sampling, LSTM was able to improve by about 5%, however, this significatly reduces our dataset so I'm not sure if it's a realistic solution to our class imbalance problem\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
