{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Logistic Regression\n",
    "\n",
    "Our full dataset file has around 9 million samples. When trying to run feature_generator.\n",
    "\n",
    "Machine:\n",
    "* 2018 Mac Mini - 6 core\n",
    "\n",
    "Docker Configuration:\n",
    "* 9 CPUs\n",
    "* 24 GB Ram\n",
    "* 3 GB swap\n",
    "\n",
    "\n",
    "# References:\n",
    "\n",
    "* https://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html\n",
    "* https://towardsdatascience.com/countvectorizer-hashingtf-e66f169e2d4e\n",
    "* https://medium.com/@dhiraj.p.rai/logistic-regression-in-spark-ml-8a95b5f5434c\n",
    "* packaging spark job - https://developerzen.com/best-practices-writing-production-grade-pyspark-jobs-cb688ac4d20f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkFiles\n",
    "from pyspark.sql import SparkSession, DataFrameReader, SQLContext\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "N_CLASSES = 5\n",
    "\n",
    "if DEBUG:\n",
    "    MIN_DF = 1\n",
    "    DF_PERCENTAGE = 0.001\n",
    "    DATA_FILE = \"/home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-test1k-preprocessed.csv\"\n",
    "else:\n",
    "    DF_PERCENTAGE = 0.001\n",
    "    DATA_FILE = \"/home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-1m-preprocessed.csv\"\n",
    "#     DATA_FILE = \"/home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-all-preprocessed.csv\"\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .appName(\"Test NGram TFIDF (local)\") \\\n",
    "            .config(\"spark.logConf\", True) \\\n",
    "            .config(\"spark.driver.memory\", \"24g\") \\\n",
    "            .config(\"spark.executor.memory\", \"24g\") \\\n",
    "            .config(\"spark.maxResultSize\", \"20g\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram, CountVectorizer, VectorAssembler, Tokenizer, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from datetime import datetime\n",
    "\n",
    "class Timer(object):\n",
    "    \n",
    "    def __init__(self, description: str):\n",
    "        self.start_time = datetime.now()\n",
    "        self.description = description\n",
    "        \n",
    "    def stop(self):\n",
    "        self.end_time = datetime.now()\n",
    "        self.print_duration_min()\n",
    "        \n",
    "    def print_duration_min(self):\n",
    "        self.duration = int((self.end_time - self.start_time).total_seconds() / 60)\n",
    "        print(f\"{self.description} duration: {self.duration} minutes\")\n",
    "    \n",
    "    \n",
    "def show_df(df, columns: list, rows: int=10, sample=True, truncate=False):\n",
    "    \"\"\"\n",
    "    Prints out DF with sampling\n",
    "    \"\"\"\n",
    "    if sample:\n",
    "        sample_percent = min(rows / df.count(), 1.0)\n",
    "        log.info(f'sampling percentage: {sample_percent}')\n",
    "        df.select(columns).sample(False, sample_percent, seed=1).show(rows, truncate=truncate)\n",
    "    else:\n",
    "        df.select(columns).show(rows, truncate=truncate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file load time duration: 0 minutes\n"
     ]
    }
   ],
   "source": [
    "file_timer = Timer(\"file load time\")\n",
    "df = spark.read.csv(SparkFiles.get(DATA_FILE), \n",
    "                    header=True, \n",
    "                    inferSchema= True)\n",
    "df.collect()\n",
    "file_timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Creating TFIDF using min_df: 995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline time duration: 1 minutes\n"
     ]
    }
   ],
   "source": [
    "def build_ngrams(inputCol, min_df, n=3):\n",
    "    log.info(f'Creating TFIDF using min_df: {min_df}')\n",
    "    \n",
    "    tokenizer = [Tokenizer(inputCol = inputCol, outputCol = \"words\")]\n",
    "    \n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    vectorizers = [\n",
    "        CountVectorizer(minDF=min_df, inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_counts\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_counts\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"raw_features\"\n",
    "    )]\n",
    "    \n",
    "    idf = [IDF(minDocFreq=min_df).setInputCol(\"raw_features\").setOutputCol(\"features\")]\n",
    "\n",
    "    return Pipeline(stages=tokenizer + ngrams + vectorizers + assembler + idf)\n",
    "\n",
    "\n",
    "\n",
    "pipeline_timer = Timer(\"pipeline time\")\n",
    "# calculate a reasonable min_df\n",
    "min_df = max(int(df.count() * DF_PERCENTAGE), 1)\n",
    "\n",
    "df = build_ngrams(\"review_body\", min_df).fit(df).transform(df)\n",
    "pipeline_timer.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: timestamp (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- 1_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 2_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 3_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 1_counts: vector (nullable = true)\n",
      " |-- 2_counts: vector (nullable = true)\n",
      " |-- 3_counts: vector (nullable = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:sampling percentage: 1.004330673865709e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|star_rating|            features|\n",
      "+-----------+--------------------+\n",
      "|          2|(4073,[16,20,35,3...|\n",
      "|          4|(4073,[13,17,18,2...|\n",
      "|          5|(4073,[3],[1.4694...|\n",
      "|          1|(4073,[0,5,12,31,...|\n",
      "|          5|(4073,[1,17,20,22...|\n",
      "|          5|(4073,[0,1,2,3,4,...|\n",
      "|          4|(4073,[3,4,13,39,...|\n",
      "|          4|(4073,[0,7,8,9,14...|\n",
      "|          4|(4073,[0,1,2,3,8,...|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "show_df(df, [\"star_rating\", \"features\"], truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.9, 0.1], seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 896004 Test size: 99684\n"
     ]
    }
   ],
   "source": [
    "train_size = train.count()\n",
    "test_size = test.count()\n",
    "\n",
    "print(f'Training size: {train_size} Test size: {test_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign class weights to handle imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated class weights: [1.4238671487028725, 3.0216301891882775, 2.212055153003913, 1.200225041190575, 0.37292633489134824]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# only do this for small files - takes too long for large datasets - we will custom compute this\n",
    "if DEBUG:\n",
    "    labels = train.select(\"star_rating\").toPandas().astype({\"star_rating\": np.int8})\n",
    "    class_weights_sklearn = compute_class_weight('balanced', sorted(labels.star_rating.unique()), labels.star_rating.tolist())\n",
    "    print(f'sklearn class weights: {class_weights_sklearn}')\n",
    "    \n",
    "\n",
    "# custom calculate class weights\n",
    "n_samples = train.count()\n",
    "class_weights = []\n",
    "\n",
    "for i in np.arange(1, N_CLASSES + 1):\n",
    "    class_samples = train.filter(f\"star_rating == {i}\").count()\n",
    "    class_weights.append(n_samples / (N_CLASSES * class_samples))\n",
    "    \n",
    "print(f'calculated class weights: {class_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:sampling percentage: 2.2321328922638738e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+--------------------+\n",
      "|star_rating|      class_weights|            features|\n",
      "+-----------+-------------------+--------------------+\n",
      "|          1| 1.4238671487028725|(4073,[0,2,9,12,2...|\n",
      "|          1| 1.4238671487028725|(4073,[0,5,6,24,4...|\n",
      "|          5|0.37292633489134824|(4073,[3,5,10,116...|\n",
      "|          5|0.37292633489134824|(4073,[134,169,25...|\n",
      "|          4|  1.200225041190575|(4073,[17,22,176,...|\n",
      "|          5|0.37292633489134824|(4073,[2,20,186,2...|\n",
      "|          5|0.37292633489134824|(4073,[1,2,5,13,1...|\n",
      "|          2| 3.0216301891882775|(4073,[0,1,3,4,5,...|\n",
      "|          5|0.37292633489134824|(4073,[8],[1.7774...|\n",
      "|          4|  1.200225041190575|(4073,[6,8,9,19,2...|\n",
      "|          4|  1.200225041190575|(4073,[6,16,19,11...|\n",
      "|          4|  1.200225041190575|(4073,[2,8,11,25,...|\n",
      "|          4|  1.200225041190575|(4073,[15,16,101,...|\n",
      "|          1| 1.4238671487028725|(4073,[0,19,24,48...|\n",
      "|          4|  1.200225041190575|(4073,[0,6,7,8,9,...|\n",
      "|          4|  1.200225041190575|(4073,[2,8,13,23,...|\n",
      "|          5|0.37292633489134824|(4073,[0,1,3,5,12...|\n",
      "+-----------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, lit, col\n",
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "\n",
    "\n",
    "train = train.withColumn(\"class_weights\", lit(0))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 1, class_weights[0]).otherwise(train.class_weights))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 2, class_weights[1]).otherwise(train.class_weights))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 3, class_weights[2]).otherwise(train.class_weights))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 4, class_weights[3]).otherwise(train.class_weights))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 5, class_weights[4]).otherwise(train.class_weights))\n",
    "\n",
    "\n",
    "show_df(train, [\"star_rating\", \"class_weights\", \"features\"], 20, truncate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traing time duration: 1 minutes\n",
      "predict time duration: 0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:sampling percentage: 0.00010031700172545243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+--------------------+\n",
      "|star_rating|prediction|       rawPrediction|         probability|\n",
      "+-----------+----------+--------------------+--------------------+\n",
      "|          5|       5.0|[-10.081784191813...|[1.04346451398456...|\n",
      "|          5|       5.0|[-10.082017947351...|[1.73272704012591...|\n",
      "|          5|       4.0|[-10.082291993279...|[7.22390572680019...|\n",
      "|          5|       4.0|[-10.081778525523...|[6.41747648247169...|\n",
      "|          5|       5.0|[-10.082146324969...|[1.02695767828834...|\n",
      "|          1|       1.0|[-10.084304030478...|[8.11113555667061...|\n",
      "|          2|       1.0|[-10.081712626159...|[9.94228306826467...|\n",
      "|          5|       5.0|[-10.083307101262...|[9.41287116474760...|\n",
      "|          2|       1.0|[-10.085158843896...|[2.68945012184320...|\n",
      "|          5|       5.0|[-10.088940966399...|[6.95275870100088...|\n",
      "+-----------+----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "train_timer = Timer(\"traing time\")\n",
    "lr = LogisticRegression(labelCol=\"star_rating\", \n",
    "                        featuresCol=\"features\", \n",
    "                        weightCol=\"class_weights\",\n",
    "                        maxIter=100)\n",
    "model=lr.fit(train)\n",
    "train_timer.stop()\n",
    "\n",
    "predict_timer = Timer(\"predict time\")\n",
    "predict_train=model.transform(train)\n",
    "predict_test=model.transform(test)\n",
    "predict_timer.stop()\n",
    "\n",
    "\n",
    "show_df(predict_test, [\"star_rating\", \"prediction\", \"rawPrediction\", \"probability\"], truncate=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: timestamp (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- 1_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 2_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 3_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 1_counts: vector (nullable = true)\n",
      " |-- 2_counts: vector (nullable = true)\n",
      " |-- 3_counts: vector (nullable = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate our Model\n",
    "\n",
    "Reference:\n",
    "* https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html#multiclass-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy = 0.6413165603306449\n",
      "Overall F1 = 0.6551891250685951\n",
      "Weighted Precision = 0.6748378796407741\n",
      "Weighted Recall = 0.6413165603306449\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"star_rating\",\n",
    "                                             predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predict_test, {evaluator.metricName: \"accuracy\"})\n",
    "print(f\"Overall Accuracy = {accuracy}\")\n",
    "f1 = evaluator.evaluate(predict_test, {evaluator.metricName: \"f1\"})\n",
    "print(f\"Overall F1 = {f1}\")\n",
    "weightedPrecision = evaluator.evaluate(predict_test, {evaluator.metricName: \"weightedPrecision\"})\n",
    "print(f\"Weighted Precision = {weightedPrecision}\")\n",
    "weightedRecall = evaluator.evaluate(predict_test, {evaluator.metricName: \"weightedRecall\"})\n",
    "print(f\"Weighted Recall = {weightedRecall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating class specific Recall, Precision, F1-Score\n",
    "\n",
    "$precision = TP / (TP + FP)$\n",
    "\n",
    "$recall = TP / (TP + FN)$\n",
    "\n",
    "$F1 = 2 * (precision * recall) / (precision + recall)$\n",
    "\n",
    "\n",
    "All weighted average metrics are calculated in the following manner:\n",
    "$$weighted=\\frac{1}{total support}\\sum_{i=1}^n{metric}$$\n",
    "\n",
    "support is the number of occurrences of each class in y_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class 1 precision: 0.7026024905554779 recall: 0.7121684867394695 f1: 0.7073531483307508 support: 14102\n",
      "Class 2 precision: 0.2902851843264549 recall: 0.3864793949683593 f1: 0.3315458457464415 support: 6479\n",
      "Class 3 precision: 0.3212204319506342 recall: 0.41272987556436513 f1: 0.3612704226709721 support: 9081\n",
      "Class 4 precision: 0.4012675668228162 recall: 0.43798123195380173 f1: 0.4188213638586097 support: 16624\n",
      "Class 5 precision: 0.8594705118101851 recall: 0.7557024607663209 f1: 0.8042531564838713 support: 53398\n",
      "Total Support: 99684\n",
      "Accuracy: 0.6413165603306449\n",
      "Macro Avg Precision: 0.5149692370931136 Recall: 0.5410122899984633 F1: 0.5246487874181291\n",
      "Weighted Avg Precision: 0.6748378796407741 Recall: 0.6413165603306449 F1: 0.6551891250685951\n",
      "\n",
      "{'1': {'f1-score': 0.7073531483307508,\n",
      "       'precision': 0.7026024905554779,\n",
      "       'recall': 0.7121684867394695,\n",
      "       'support': 14102},\n",
      " '2': {'f1-score': 0.3315458457464415,\n",
      "       'precision': 0.2902851843264549,\n",
      "       'recall': 0.3864793949683593,\n",
      "       'support': 6479},\n",
      " '3': {'f1-score': 0.3612704226709721,\n",
      "       'precision': 0.3212204319506342,\n",
      "       'recall': 0.41272987556436513,\n",
      "       'support': 9081},\n",
      " '4': {'f1-score': 0.4188213638586097,\n",
      "       'precision': 0.4012675668228162,\n",
      "       'recall': 0.43798123195380173,\n",
      "       'support': 16624},\n",
      " '5': {'f1-score': 0.8042531564838713,\n",
      "       'precision': 0.8594705118101851,\n",
      "       'recall': 0.7557024607663209,\n",
      "       'support': 53398},\n",
      " 'accuracy': 0.6413165603306449,\n",
      " 'macro avg': {'f1-score': 0.5246487874181291,\n",
      "               'precision': 0.5149692370931136,\n",
      "               'recall': 0.5410122899984633,\n",
      "               'support': 99684},\n",
      " 'weighted avg': {'f1-score': 0.6551891250685951,\n",
      "                  'precision': 0.6748378796407741,\n",
      "                  'recall': 0.6413165603306449,\n",
      "                  'support': 99684}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from pprint import pprint\n",
    "\n",
    "# check results from sklearn in debug mode\n",
    "if DEBUG:\n",
    "    print(\"sklearn classification report\")\n",
    "    pprint(classification_report(predict_test.select(\"star_rating\").toPandas().star_rating.tolist(),\n",
    "                               predict_test.select(\"prediction\").toPandas().prediction.tolist(),\n",
    "                               output_dict=True))\n",
    "\n",
    "\n",
    "def pyspark_classification_report(test_df, truth_column, prediction_column, classes: int):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    weighted_precisions = []\n",
    "    weighted_recalls = []\n",
    "    weighted_f1s = []\n",
    "    \n",
    "    total_support = test_df.count()\n",
    "    \n",
    "    dr_dict = {}\n",
    "    \n",
    "    print()\n",
    "    for i in np.arange(1, classes + 1):\n",
    "        support = test_df.filter(f'{truth_column} == {i}').count()\n",
    "        \n",
    "        predicted = test_df.filter(f'{prediction_column} == {i}')\n",
    "        tp = predicted.filter(f'{truth_column} == {prediction_column}').count()\n",
    "        fp = predicted.count() - tp\n",
    "        \n",
    "        predicted_count = predicted.count()\n",
    "        precision = 0 if predicted_count == 0 else tp / predicted.count()\n",
    "        precisions.append(precision)\n",
    "        weighted_precisions.append(precision * support)\n",
    "        \n",
    "        recall = tp / support\n",
    "        recalls.append(recall)\n",
    "        weighted_recalls.append(recall * support)\n",
    "        \n",
    "        f1 = 0 if precision == 0 and recall == 0 else 2 * (precision * recall) / (precision + recall)\n",
    "        f1s.append(f1)\n",
    "        weighted_f1s.append(f1 * support)\n",
    "        print(f'Class {i} precision: {precision} recall: {recall} f1: {f1} support: {support}')\n",
    "        \n",
    "        d = {\n",
    "            \"precision\":precision,\n",
    "            \"recall\":recall,\n",
    "            \"f1-score\":f1,\n",
    "            \"support\":support\n",
    "        }\n",
    "        dr_dict[str(i)] = d\n",
    "        \n",
    "    print(f'Total Support: {total_support}')\n",
    "        \n",
    "    accuracy = test_df.filter(\"star_rating == prediction\").count() / total_support\n",
    "    dr_dict[\"accuracy\"] = accuracy\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "        \n",
    "    macro_avg_precision = sum(precisions) / len(precisions)\n",
    "    macro_avg_recall = sum(recalls) / len(recalls)\n",
    "    macro_avg_f1 = sum(f1s) / len(f1s)\n",
    "    \n",
    "    dr_dict[\"macro avg\"] = {\n",
    "        \"precision\": macro_avg_precision,\n",
    "        \"recall\": macro_avg_recall,\n",
    "        \"f1-score\": macro_avg_f1,\n",
    "        \"support\": total_support\n",
    "    }\n",
    "\n",
    "    print(f'Macro Avg Precision: {macro_avg_precision} Recall: {macro_avg_recall} F1: {macro_avg_f1}')\n",
    "    \n",
    "    weighted_avg_precision = sum(weighted_precisions) / total_support\n",
    "    weighted_avg_recall = sum(weighted_recalls) / total_support\n",
    "    weighted_avg_f1 = sum(weighted_f1s) / total_support\n",
    "    \n",
    "    dr_dict[\"weighted avg\"] = {\n",
    "        \"precision\": weighted_avg_precision,\n",
    "        \"recall\": weighted_avg_recall,\n",
    "        \"f1-score\": weighted_avg_f1,\n",
    "        \"support\": total_support\n",
    "    }\n",
    "    \n",
    "    print(f'Weighted Avg Precision: {weighted_avg_precision} Recall: {weighted_avg_recall} F1: {weighted_avg_f1}')\n",
    "    \n",
    "    return dr_dict\n",
    "        \n",
    "        \n",
    "\n",
    "cr = pyspark_classification_report(predict_test, \"star_rating\", \"prediction\", N_CLASSES)\n",
    "\n",
    "print()\n",
    "pprint(cr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10043  2635   928   224   272]\n",
      " [ 1991  2504  1399   334   251]\n",
      " [  962  1887  3748  1697   787]\n",
      " [  431   742  2882  7281  5288]\n",
      " [  867   858  2711  8609 40353]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    print(\"sklearn confusion matrix\")\n",
    "    print(confusion_matrix(predict_test.select(\"star_rating\").toPandas().star_rating.tolist(),\n",
    "                            predict_test.select(\"prediction\").toPandas().prediction.tolist()))\n",
    "    \n",
    "\n",
    "def pyspark_confusion_matrix(test_df, truth_column, prediction_column, n_classes):\n",
    "    \"\"\"\n",
    "    Calculates confusiion matrix like sklearn would using pyspark dataframe\n",
    "    \n",
    "    :param test_df: pyspark dataframe with labels and predictions\n",
    "    :param truth_column: column name with the truth\n",
    "    :param prediction_column: column name for prediction\n",
    "    :param n_classes: number of classes in predictions\n",
    "    :return: np array with confusion matrix\n",
    "    \"\"\"\n",
    "    cm = []\n",
    "    for i in np.arange(1, n_classes + 1):\n",
    "        current = []\n",
    "        for j in np.arange(1, n_classes + 1):\n",
    "            count = test_df.filter(f'({truth_column} == {i}) AND ({prediction_column} == {j})').count()\n",
    "\n",
    "            current.append(count)\n",
    "        cm.append(current)\n",
    "    return np.asarray(cm)\n",
    "\n",
    "cm = pyspark_confusion_matrix(predict_test, \"star_rating\", \"prediction\", N_CLASSES)\n",
    "\n",
    "print(cm)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall score: 0.5070364344705827\n"
     ]
    }
   ],
   "source": [
    "def calculate_score(cr: dict):\n",
    "    \n",
    "    values = []\n",
    "    values.append(cr[\"1\"][\"recall\"])\n",
    "    values.append(cr[\"2\"][\"recall\"])\n",
    "    values.append(cr[\"3\"][\"recall\"])\n",
    "    values.append(cr[\"4\"][\"recall\"])\n",
    "    values.append(cr[\"5\"][\"precision\"])\n",
    "    \n",
    "    mean = 0\n",
    "    for v in values:\n",
    "        if v == 0:\n",
    "            mean = 0\n",
    "            break\n",
    "        else:\n",
    "            mean += 1 / v\n",
    "    if mean > 0:\n",
    "        mean = len(values) / mean\n",
    "\n",
    "    return mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "score = calculate_score(cr)\n",
    "print(f'Overall score: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
