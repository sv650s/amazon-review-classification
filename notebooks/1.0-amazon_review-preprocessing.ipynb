{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Review Pre-processing\n",
    "\n",
    "Most of the work is done in classes outside of this notebook, but I'll outline what I did for pre-processing in this notebook. Running pre-processing in the notebook took a lot longer so running it via command line seems to be quicker so I can iterate\n",
    "\n",
    "Code is located in util folder:\n",
    "* preprocess_amazon.py - python program that calls TextProcessor with parameters set to handle the amazon review file\n",
    "* TextProcessor.py - processor class that uses various utilities to pre-process the file\n",
    "* text_util.py - has functions to do text processing\n",
    "* file_util.py - functiont to handle files (ie, covert tsv to csv)\n",
    "* df_util.py - utility to handle pandas DataFrames\n",
    "\n",
    "\n",
    "Unit Tests:\n",
    "* TestTextUtil.py - tests text_util.py\n",
    "\n",
    "To Be Implemented:\n",
    "* unit tests for file_util.py\n",
    "* unit tests for pd_util.py\n",
    "\n",
    "\n",
    "## General ML Workflow\n",
    "\n",
    "Since most of the work is actually done outside of Jupyter notebook, there is a series of python programs in *tools* directory that were created to pre-process and train our models.\n",
    "\n",
    "The following diagram outlines the process:\n",
    "\n",
    "<img src=\"../images/ml-workflow.png\" alt=\"General ML Workflow\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sibling utilities\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import util.file_util as fu\n",
    "from util.file_util import convert_tsv_to_csv\n",
    "import pandas as pd\n",
    "import util.text_util as tu\n",
    "import util.file_samplers as fs\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# only need to run this one time\n",
    "CONVERT_FILE=False\n",
    "\n",
    "# full 9mil Wireless reviews\n",
    "ORIG_FILE_WIRELESS=\"../dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00.tsv\"\n",
    "\n",
    "# List of potential CSV files to feed pre-processor\n",
    "DATA_FILE_50K = \"../dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-50k.csv\"\n",
    "DATA_FILE_100K = \"../dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-100k.csv\"\n",
    "DATA_FILE_200K = \"../dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-200k.csv\"\n",
    "DATA_FILE_500K = \"../dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-500k.csv\"\n",
    "DATA_FILE_1m = \"../dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-1m.csv\"\n",
    "DATA_FILE_2m = \"../dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-2m.csv\"\n",
    "DATA_FILE_4m = \"../dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-4m.csv\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Amazon files to CSV format and Downsample Our Files\n",
    "\n",
    "Before we pre-process, I had to convert tsv to CSV because Pandas was not reading the columns correctly and was putting multiple rows into a column resulting in headline columns that had over 30k words\n",
    "\n",
    "Original Amazon file had 9mil reviews. I added ability to down-sampling our files in this step since I wanted to reduce the size of files that we use for our prototypes. Currently, the sampling is pretty dumb. It just grabs every nth line in the file and put it in the final csv file. Will probably rewrite this so it's based on random.rand later or use pandas sample function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mfu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tsv_to_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mconvert_tsv_to_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Read a tsv file line by line then covert it into a readable csv file\u001b[0m\n",
       "\u001b[0;34m    :param infile: input TSV file\u001b[0m\n",
       "\u001b[0;34m    :param outfile: output CSV file\u001b[0m\n",
       "\u001b[0;34m    :param sampler - Sampler to reduce the output file size\u001b[0m\n",
       "\u001b[0;34m    :return:\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Converting {infile} to {outfile} with sampling {sampler}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mold_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnew_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mold_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;31m# never sample 0 because that's the header column\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Finished converting {infile} to {outfile} with sampling {sampler}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'File not found: {infile}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Dropbox/0_springboard/capstone/util/file_util.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??fu.convert_tsv_to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_header\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mSimpleSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Simple sampling means that we are going to keep every X entry. X is specified by the sample_rate in the constructor\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Every X samples will be saved to the file\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhas_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m        returns True if we want to keep this file\u001b[0m\n",
       "\u001b[0;34m        :param index:\u001b[0m\n",
       "\u001b[0;34m        :return:\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# if file has header and we are at the first line, then keep\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_header\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/Dropbox/0_springboard/capstone/util/file_samplers.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??fs.SimpleSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert files to CSV only needs to be done once. To rerun, set CONVERT_FILE to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting ../dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00.tsv to ../dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-4m.csv with sampling <util.file_samplers.SimpleSampler object at 0x7fe4107064d0>\n",
      "Finished converting ../dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00.tsv to ../dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-4m.csv with sampling <util.file_samplers.SimpleSampler object at 0x7fe4107064d0>\n"
     ]
    }
   ],
   "source": [
    "if CONVERT_FILE:\n",
    "    convert_tsv_to_csv(ORIG_FILE_WIRELESS, DATA_FILE_4m, fs.SimpleSampler(2))\n",
    "    convert_tsv_to_csv(ORIG_FILE_WIRELESS, DATA_FILE_2m, fs.SimpleSampler(4))\n",
    "    convert_tsv_to_csv(ORIG_FILE_WIRELESS, DATA_FILE_1m, fs.SimpleSampler(9))\n",
    "    convert_tsv_to_csv(ORIG_FILE_WIRELESS, DATA_FILE_500K, fs.SimpleSampler(18))\n",
    "    convert_tsv_to_csv(ORIG_FILE_WIRELESS, DATA_FILE_200K, fs.SimpleSampler(45))\n",
    "    convert_tsv_to_csv(ORIG_FILE_WIRELESS, DATA_FILE_100K, fs.SimpleSampler(90))\n",
    "    convert_tsv_to_csv(ORIG_FILE_WIRELESS, DATA_FILE_50K, fs.SimpleSampler(180))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocssing Amazon Review File\n",
    "\n",
    "## To do pre-processing. Run the following command:\n",
    "```\n",
    "cd ../tools\n",
    "python amazon_review_preprocessor.py -l INFO -o ../dataset/amazon_reviews ../dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-100k.csv\n",
    "```\n",
    "\n",
    "Output file is located at:\n",
    "```\n",
    "../dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-100k-preprocessed.csv\n",
    "```\n",
    "\n",
    "## Pre-processing Steps\n",
    "\n",
    "Pre-processing entails the following steps (in order):\n",
    "* make everything lowercase\n",
    "* remove newlines\n",
    "* remove amazon tags - amazon embeds these [[VIDDEO:dsfljlsjf]] and [[ASIN:sdfjlsjdfl]] tags that need to be removed\n",
    "* remove html tags - line breaks, etc are represented in reviews as HTML tags\n",
    "* remove accent characters\n",
    "* expand contractions - expands contractions like he's but needs to be done before special charaters because we want to expand don't into do not for our text processing\n",
    "* remove special characters - anything that is not alphanumeric or spaces\n",
    "* remove stop words - see text_util.py for stop words that I removed from nltk stop words because I think they will be important for sentiment analysis\n",
    "* lemmatize words - lemmatize using wordnet to use only base words\n",
    "\n",
    "\n",
    "Finally, the pre-processor will remove any entries with 0-length reviews. The output file after pre-processing has 99567 entries instead of 100k because of this.\n",
    "\n",
    "## Class Diagarm\n",
    "\n",
    "<img src=\"../images/amazon_review_preprocessor-class-diagram.png\" alt=\"Amazon Review Preprocessor\" style=\"width: 600px;\"/>\n",
    "\n",
    "# The remaining sections shows code snippets of what each of the pre-processing steps does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_lowercase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mmake_lowercase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Make text lower case\u001b[0m\n",
       "\u001b[0;34m    :param text: original text\u001b[0m\n",
       "\u001b[0;34m    :return: lower case string\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Dropbox/0_springboard/capstone/util/text_util.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??tu.make_lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove new lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_newlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mremove_newlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    remove newlines from text. will stirp both unix and windows\u001b[0m\n",
       "\u001b[0;34m    newline characters\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    :return: string without newlines\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# logger.debug(f'pre-stripped: [{text}]')\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnewtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Dropbox/0_springboard/capstone/util/text_util.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??tu.remove_newlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_html_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mremove_html_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Remove HTML taxs from text usig regex\u001b[0m\n",
       "\u001b[0;34m    :param text: original text\u001b[0m\n",
       "\u001b[0;34m    :return: stripped text\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Dropbox/0_springboard/capstone/util/text_util.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??tu.remove_html_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove accented characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_accented_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mremove_accented_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Remove accent characters and convert to UTF-8\u001b[0m\n",
       "\u001b[0;34m    :param text: original text\u001b[0m\n",
       "\u001b[0;34m    :return: stripped text\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NFKD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Dropbox/0_springboard/capstone/util/text_util.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??tu.remove_accented_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## expand contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_contractions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcontraction_mapping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"ain't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'is not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"aren't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'are not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"can't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'cannot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"can't've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'cannot have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"'cause\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'because'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"could've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'could have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"couldn't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'could not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"couldn't've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'could not have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"didn't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'did not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"doesn't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'does not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"don't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'do not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gonna'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'going to'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hadn't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'had not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hadn't've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'had not have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hasn't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'has not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"haven't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'have not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"he'd\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'he would'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"he'd've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'he would have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"he'll\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'he will'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"he'll've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'he will have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"he's\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'he is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"how'd\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'how did'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"how'd'y\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'how do you'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"how'll\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'how will'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"how's\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'how is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i'd\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'i would'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i'd've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'i would have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i'll\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'i will'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i'll've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'i will have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i'm\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'i am'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'im'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'I am'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'i have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"isn't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'is not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"it'd\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'it would'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"it'd've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'it would have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"it'll\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'it will'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"it'll've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'it will have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"it's\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'it is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"let's\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'let us'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ma'am\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'madam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mayn't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'may not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"might've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'might have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mightn't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'might not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mightn't've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'might not have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"must've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'must have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mustn't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'must not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mustn't've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'must not have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"needn't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'need not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"needn't've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'need not have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"o'clock\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'of the clock'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"oughtn't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ought not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"oughtn't've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ought not have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shan't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'shall not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sha'n't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'shall not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shan't've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'shall not have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"she'd\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'she would'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"she'd've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'she would have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"she'll\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'she will'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"she'll've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'she will have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"she's\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'she is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"should've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'should have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shouldn't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'should not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shouldn't've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'should not have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"so've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'so have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"so's\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'so as'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"that'd\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'that would'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"that'd've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'that would have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"that's\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'that is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"there'd\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'there would'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"there'd've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'there would have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"there's\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'there is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"they'd\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'they would'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"they'd've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'they would have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"they'll\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'they will'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"they'll've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'they will have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"they're\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'they are'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"they've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'they have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'to have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wasn't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'was not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"we'd\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'we would'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"we'd've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'we would have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"we'll\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'we will'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"we'll've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'we will have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"we're\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'we are'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"we've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'we have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"weren't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'were not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"what'll\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'what will'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"what'll've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'what will have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"what're\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'what are'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"what's\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'what is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"what've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'what have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"when's\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'when is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"when've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'when have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"where'd\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'where did'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"where's\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'where is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"where've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'where have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"who'll\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'who will'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"who'll've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'who will have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"who's\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'who is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"who've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'who have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"why's\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'why is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"why've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'why have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"will've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'will have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"won't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'will not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"won't've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'will not have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"would've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'would have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wouldn't\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'would not'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wouldn't've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'would not have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y'all\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'you all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y'all'd\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'you all would'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y'all'd've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'you all would have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y'all're\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'you all are'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y'all've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'you all have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"you'd\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'you would'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"you'd've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'you would have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"you'll\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'you will'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"you'll've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'you will have'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"you're\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'you are'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"you've\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'you have'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mexpand_contractions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_mapping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONTRACTION_MAP\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcontractions_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'({})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontraction_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                                      \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIGNORECASE\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDOTALL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mexpand_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontraction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mfirst_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mexpanded_contraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontraction_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m \\\n",
       "            \u001b[0;32mif\u001b[0m \u001b[0mcontraction_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m \\\n",
       "            \u001b[0;32melse\u001b[0m \u001b[0mcontraction_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mexpanded_contraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_char\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexpanded_contraction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mexpanded_contraction\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mexpanded_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontractions_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_match\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mexpanded_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mexpanded_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Dropbox/0_springboard/capstone/util/text_util.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??tu.expand_contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_special_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mremove_special_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Remove anything that is not alphanumeric or spaces\u001b[0m\n",
       "\u001b[0;34m    :param text:\u001b[0m\n",
       "\u001b[0;34m    :return:\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[^a-zA-Z0-9\\s]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mI\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_newlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Dropbox/0_springboard/capstone/util/text_util.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??tu.remove_special_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove alphanumeric characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_alphanumeric_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mremove_alphanumeric_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    In amazon reviews especially for wireless categories you have some alpha numeric word\u001b[0m\n",
       "\u001b[0;34m    which represent model numbers, we want to remove this\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Will also remove words that are basically only numbers\u001b[0m\n",
       "\u001b[0;34m    :param x:\u001b[0m\n",
       "\u001b[0;34m    :return:\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# remove mixed words\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\s*([a-z]+[\\d]+[\\w\\d]*|[\\d]+[a-z]+[\\d\\w]*|[\\d]{2,})'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIGNORECASE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# remove numbers\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# x = re.sub(r'\\s(\\d+)', '', x)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Dropbox/0_springboard/capstone/util/text_util.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??tu.remove_alphanumeric_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove stop words\n",
    "\n",
    "I removed the following stop words from NLTK because in a review context, if I say 'like' or 'not like' makes a difference in a review context\n",
    "\n",
    "The following words were removed from NLTK's default stop word list:\n",
    "\n",
    "[    'no',\n",
    "    'not',\n",
    "    'do',\n",
    "    'don',\n",
    "    \"don't\",\n",
    "    'does',\n",
    "    'did',\n",
    "    'does',\n",
    "    'doesn',\n",
    "    \"doesn't\",\n",
    "    'should',\n",
    "    'very',\n",
    "    'will']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    remove stop words from string\u001b[0m\n",
       "\u001b[0;34m    :param text: original text\u001b[0m\n",
       "\u001b[0;34m    :return: string without stop words\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mfiltered_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"got error trying to tokenize this string [{text}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Dropbox/0_springboard/capstone/util/text_util.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??tu.remove_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lemmatize words\n",
    "\n",
    "lemmatization changes the word to the base word - ie, car, cars, car's to car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mlemmatize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    lemmatize work\u001b[0m\n",
       "\u001b[0;34m    :param text:\u001b[0m\n",
       "\u001b[0;34m    :return:\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'lemmatizing text: {text}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mword_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlemmatized_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mlemmatized_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Dropbox/0_springboard/capstone/util/text_util.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??tu.lemmatize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect results in our Pre-processed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PREPROCESSED_CSV = \"../dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-100k-preprocessed.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the output file back in to look at some data\n",
    "\n",
    "* there should be no empty columns or rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99567 entries, 0 to 99566\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   star_rating      99567 non-null  int64         \n",
      " 1   helpful_votes    99567 non-null  int64         \n",
      " 2   total_votes      99567 non-null  int64         \n",
      " 3   review_headline  99567 non-null  object        \n",
      " 4   review_body      99567 non-null  object        \n",
      " 5   review_date      99567 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(3), object(2)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "review_df = pd.read_csv(PREPROCESSED_CSV, parse_dates=[\"review_date\"])\n",
    "review_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['case really smart looking little slippery hand car charger doe not fit opening take using charger',\n",
       "       'various tough phone nextel lastest samsun meet ti amely di amise eventually thats got warranty used day le test abuse warranty warranty awesome week turn around destruction replacement call quality pretty good speaker get bit muffled dunk make call otherwise ringer loud vibrate worthless battery life 6 7 day texting day constant flashlight useage do not net can not comment use phone camera work great software come worthless plug use usb storage device transfer photo also no built photo editing option hand durable phone ever 3 year will trash get new one dy warranty will buy another will never buy regular phone did mention water proof ever get call shower take easiest way clean grit grease use hand nail brush scrub clean week',\n",
       "       'grandaughter 2 year old glide tricycle not expected truly looked forward seeng glide normal height sits down leg bend much uncomfortable position handle bar close leg thhink not age 1 4 height should able de adjusted well lengthen leg unfortunately can not return regular 2 year old say mine return shipping cost high think probably fit 1 year old disappointed grandma',\n",
       "       'great speaker exactly advertised',\n",
       "       'product changed much since owned one several year ago many feature newer version make very difficult operate no instruction included shipping box even line tutorial various function not clear brief get good understanding operates wanted si ample hand held gps old man young child could use without much difficulty not received three people gps unit could not make one operate properly tried return refused ti ame li amit expired purchased christmas gift min early dec gift not given dec battery not installed unit ti ame early january think return policy item like suck',\n",
       "       'love took 2 day shipp',\n",
       "       'not made tempered glass flexible will not protect screen breakage false advertising clai aming tempered glass',\n",
       "       'fit really nice liked',\n",
       "       'product doe supposed do no complaint would buy shipped good amount ti ame thanks',\n",
       "       'easy install still phone 9 mo later'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's sample the dataframe so we can look at some of the data\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "review_df.review_body.sample(10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
