{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Logistic Regression\n",
    "\n",
    "Our full dataset file has around 9 million samples. When trying to run feature_generator.\n",
    "\n",
    "Machine:\n",
    "* 2018 Mac Mini - 6 core\n",
    "\n",
    "Docker Configuration:\n",
    "* 9 CPUs\n",
    "* 24 GB Ram\n",
    "* 3 GB swap\n",
    "\n",
    "\n",
    "# References:\n",
    "\n",
    "* https://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html\n",
    "* https://towardsdatascience.com/countvectorizer-hashingtf-e66f169e2d4e\n",
    "* https://medium.com/@dhiraj.p.rai/logistic-regression-in-spark-ml-8a95b5f5434c\n",
    "* packaging spark job - https://developerzen.com/best-practices-writing-production-grade-pyspark-jobs-cb688ac4d20f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkFiles\n",
    "from pyspark.sql import SparkSession, DataFrameReader, SQLContext\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import NGram, CountVectorizer, VectorAssembler, Tokenizer, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "import logging\n",
    "\n",
    "import util.pyspark_util as pyu\n",
    "import util.model_wrapper as mw\n",
    "\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "# allow DEBUG to be set by command line\n",
    "DEBUG = bool(os.environ.get(\"IPYNB_DEBUG\", False))\n",
    "\n",
    "N_CLASSES = 5\n",
    "FEATURE_COLUMN = \"review_body\"\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    MIN_DF = 1\n",
    "    DF_PERCENTAGE = 0.001\n",
    "    DATA_FILE = \"/home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-test1k-preprocessed.csv\"\n",
    "    TEST_STR=\"-test\"\n",
    "else:\n",
    "    DF_PERCENTAGE = 0.001\n",
    "    DATA_FILE = \"/home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-100k-preprocessed.csv\"\n",
    "    TEST_STR=\"\"\n",
    "\n",
    "REPORT_FILE = f\"../../reports/201911-pyspark-report{TEST_STR}.csv\"\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .appName(\"Pyspark Wrapper Test (local)\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Timer(object):\n",
    "    \n",
    "    def __init__(self, description: str):\n",
    "        self.start_time = datetime.now()\n",
    "        self.description = description\n",
    "        \n",
    "    def stop(self):\n",
    "        self.end_time = datetime.now()\n",
    "        self.print_duration_min()\n",
    "        \n",
    "    def print_duration_min(self):\n",
    "        self.duration = int((self.end_time - self.start_time).total_seconds() / 60)\n",
    "        print(f\"{self.description} duration: {self.duration} minutes\")\n",
    "        \n",
    "    def get_duraction_min(self):\n",
    "        return self.duration\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file load time duration: 0 minutes\n"
     ]
    }
   ],
   "source": [
    "file_timer = Timer(\"file load time\")\n",
    "df = spark.read.csv(SparkFiles.get(DATA_FILE), \n",
    "                    header=True, \n",
    "                    inferSchema= True)\n",
    "df.collect()\n",
    "file_timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Creating TFIDF using min_df: 99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline time duration: 0 minutes\n"
     ]
    }
   ],
   "source": [
    "def build_ngrams(inputCol, min_df, n=3):\n",
    "    log.info(f'Creating TFIDF using min_df: {min_df}')\n",
    "    \n",
    "    tokenizer = [Tokenizer(inputCol = inputCol, outputCol = \"words\")]\n",
    "    \n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    vectorizers = [\n",
    "        CountVectorizer(minDF=min_df, inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_counts\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_counts\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"raw_features\"\n",
    "    )]\n",
    "    \n",
    "    idf = [IDF().setInputCol(\"raw_features\").setOutputCol(\"features\")]\n",
    "#     idf = [IDF(minDocFreq=min_df).setInputCol(\"raw_features\").setOutputCol(\"features\")]\n",
    "\n",
    "    return Pipeline(stages=tokenizer + ngrams + vectorizers + assembler + idf)\n",
    "\n",
    "\n",
    "\n",
    "pipeline_timer = Timer(\"pipeline time\")\n",
    "# calculate a reasonable min_df\n",
    "min_df = max(int(df.count() * DF_PERCENTAGE), 1)\n",
    "\n",
    "pipeline = build_ngrams(FEATURE_COLUMN, min_df)\n",
    "df = pipeline.fit(df).transform(df)\n",
    "pipeline_timer.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: timestamp (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- 1_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 2_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 3_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 1_counts: vector (nullable = true)\n",
      " |-- 2_counts: vector (nullable = true)\n",
      " |-- 3_counts: vector (nullable = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|star_rating|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|5          |(4159,[4,8,27,31,2224,2261,3164,4036],[1.591649534588715,1.779700065004915,2.3450329263917453,2.570720841479737,3.8372351836183554,4.478623195102613,6.497960812712743,6.50464980086354])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|5          |(4159,[21,30,42,67,91,108,150,196,297,313,331,404,849,1095,1977],[2.21161942817327,2.599090593862898,2.6865687841234154,6.079506352667616,3.211301736542082,3.583800192852564,3.62892619220471,3.9647932393074896,4.159365282195664,4.3037035966043256,4.5664394011095295,4.644792715356044,5.552758737344168,5.814863968006299,6.721104364026953])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|5          |(4159,[3,10,159,222,338,1113,2223,2909,3166],[1.4680895006147834,1.9140823621922742,3.551068704578226,3.8233524988331657,4.290419268405592,5.737154983678983,3.730803480470169,6.27215414397905,6.4913162699940745])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|5          |(4159,[6,79,2291],[1.6760139677886452,3.06504945556106,4.836563161347932])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|2          |(4159,[6,7,10,19,39,46,48,52,72,76,111,119,130,144,158,165,178,228,283,301,323,454,564,693,728,2473,2902,2975,3179],[1.6760139677886452,1.8416540112772721,1.9140823621922742,2.138776420287494,2.6664250581972837,2.77693690527224,2.8105824847250744,2.873442117759197,3.0492439150826116,3.190585829262128,3.415745079270615,3.414217661836039,3.450901911993412,7.3436626470898645,11.555358045985862,3.6266586175369295,3.630440770305675,4.014165891777433,4.21082782427762,4.152316230258251,4.287491008626503,4.646884766328268,4.939114686394703,5.140408920458507,5.211486786875064,5.5843403093944675,6.288240281730674,6.34954080759447,6.545751476549092])                                                                                                                                                                                                            |\n",
      "|5          |(4159,[6,8,10,18,704,1175,2225,2259,2415,3413],[1.6760139677886452,1.779700065004915,1.9140823621922742,2.046097218450082,5.529710341907878,6.096950054953959,3.841905906728914,4.4671844430141885,5.430953863459965,6.864205207667626])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|5          |(4159,[0,7,8,11,12,17,19,20,22,37,39,62,68,72,88,91,100,104,108,109,126,127,134,141,154,351,424,449,560,743,1245,1308,1466,1789,2210,2211,2494,2540],[0.8701478708995153,1.8416540112772721,1.779700065004915,6.101910720562294,2.0283812810310002,2.094498897318797,2.138776420287494,2.079682612033368,2.295060843936297,2.6266209225601322,2.6664250581972837,3.4139124581101843,3.2253546654235747,3.0492439150826116,3.3375619176035185,3.211301736542082,3.2650505988807397,3.260328659362099,3.583800192852564,3.2836286278944153,3.4452182844419728,3.460766749351158,3.4989007490660775,3.6116714505401344,7.3405055825037655,4.469812565420458,4.881878357559974,4.6241094547662165,4.899246863641618,5.314190715704327,5.91388472720716,6.019658380652313,6.128698753268539,6.439691904588767,2.3726560574090665,2.375352785217783,5.647809883343133,5.740275111015227])|\n",
      "|5          |(4159,[3,10,25,132,211,2223,3350],[1.4680895006147834,1.9140823621922742,2.437517802166324,3.4023832041890363,3.8155703583911107,3.730803480470169,6.625794184222628])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|5          |(4159,[3,29,35,249,2260,2284],[1.4680895006147834,2.438437765432679,2.5955841843364027,4.044086272172471,4.475089622521302,4.785966311953551])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|5          |(4159,[5,9,10,23,33,76,79,92,109,131,234,392,472,587,670,2556,2798,3165,3207],[1.7562803420746098,1.8649160899586723,1.9140823621922742,2.3221386752961486,2.549155963192069,6.381171658524256,3.06504945556106,6.466948953184981,3.2836286278944153,3.497572727622558,8.13807359535136,4.5879246025603155,4.779967493724297,4.9293448947988985,5.2059771310640945,11.911273043774763,6.133317699124834,6.511383833044884,6.518163520030263])                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "pyu.show_df(df, [\"star_rating\", \"features\"], truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test split duration: 0 minutes\n",
      "Training size: 89544 Test size: 10023\n"
     ]
    }
   ],
   "source": [
    "split_timer = Timer(\"train test split\")\n",
    "train, test = df.randomSplit([0.9, 0.1], seed=1)\n",
    "split_timer.stop()\n",
    "\n",
    "\n",
    "train_size = train.count()\n",
    "test_size = test.count()\n",
    "\n",
    "print(f'Training size: {train_size} Test size: {test_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4159"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train.select(\"features\").limit(1).toPandas().features.values[0])\n",
    "train.select(\"features\").limit(1).toPandas().features.values[0].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign class weights to handle imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn class weights: [1.43534503 3.06342798 2.17815617 1.1864847  0.37383989]\n",
      "skelarn class weight duration: 0 minutes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# only do this for small files - takes too long for large datasets - we will custom compute this\n",
    "# if DEBUG:\n",
    "cw_timer = Timer(\"skelarn class weight\")\n",
    "labels = train.select(\"star_rating\").toPandas().astype({\"star_rating\": np.int8})\n",
    "class_weights = compute_class_weight('balanced', \n",
    "                                                  np.arange(1, N_CLASSES+1), \n",
    "                                                  labels.star_rating.tolist())\n",
    "print(f'sklearn class weights: {class_weights}')\n",
    "cw_timer.stop()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:util.pyspark_util:sampling percentage: 0.00022335388189046726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+--------------------+\n",
      "|star_rating|     class_weights|            features|\n",
      "+-----------+------------------+--------------------+\n",
      "|          1|  1.43534503486415|(4159,[4,6,17,21,...|\n",
      "|          4|1.1864846959056579|(4159,[20,40,48,1...|\n",
      "|          4|1.1864846959056579|(4159,[7,18,38,81...|\n",
      "|          5|0.3738398914518318|(4159,[5,47,52,73...|\n",
      "|          5|0.3738398914518318|(4159,[20,2476,40...|\n",
      "|          5|0.3738398914518318|(4159,[0,1,7,8,11...|\n",
      "|          1|  1.43534503486415|(4159,[0,11,23,45...|\n",
      "|          4|1.1864846959056579|(4159,[8,31,77,90...|\n",
      "|          4|1.1864846959056579|(4159,[54,55,64,2...|\n",
      "|          5|0.3738398914518318|(4159,[20,37,268,...|\n",
      "|          5|0.3738398914518318|(4159,[2,3,29,52,...|\n",
      "|          5|0.3738398914518318|(4159,[9],[1.8649...|\n",
      "|          5|0.3738398914518318|(4159,[9],[1.8649...|\n",
      "|          5|0.3738398914518318|(4159,[0,2,6,7,9,...|\n",
      "|          5|0.3738398914518318|(4159,[61,79,92,1...|\n",
      "|          5|0.3738398914518318|(4159,[0,1,2,13,6...|\n",
      "|          5|0.3738398914518318|(4159,[0,1,2,4,7,...|\n",
      "|          5|0.3738398914518318|(4159,[0,7,8,9,11...|\n",
      "|          1|  1.43534503486415|(4159,[0,6,17,22,...|\n",
      "|          1|  1.43534503486415|(4159,[1,4,8,11,2...|\n",
      "+-----------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = train.withColumn(\"class_weights\", lit(0))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 1, class_weights[0]).otherwise(train.class_weights))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 2, class_weights[1]).otherwise(train.class_weights))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 3, class_weights[2]).otherwise(train.class_weights))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 4, class_weights[3]).otherwise(train.class_weights))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 5, class_weights[4]).otherwise(train.class_weights))\n",
    "\n",
    "\n",
    "pyu.show_df(train, [\"star_rating\", \"class_weights\", \"features\"], 20, sample=True, truncate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:util.model_wrapper:derived name: LogisticRegression\n",
      "INFO:util.model_wrapper:########################################\n",
      "INFO:util.model_wrapper:Running model: name: LogisticRegression\n",
      "\twith file: /home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-100k-preprocessed.csv\n",
      "\twith description: review_body-tfidf-df_none-ngram13-99567-4159-nolda-sampling_none-LogisticRegression-star_rating\n",
      "\tstatus: new\n",
      "INFO:util.model_wrapper:########################################\n",
      "INFO:util.time_util:Start timer for: train_time_min\n",
      "INFO:util.time_util:End timer for: train_time_min\n",
      "INFO:util.time_util:Total time for train_time_min: 0.32\n",
      "INFO:util.time_util:Start timer for: model_save_time_min\n",
      "INFO:util.pyspark_util:Saving model to file: ../../models/review_body-tfidf-df_none-ngram13-99567-4159-nolda-sampling_none-LogisticRegression-star_rating.pyspark\n",
      "INFO:util.pyspark_util:Saving pipeline to file: ../../models/review_body-tfidf-df_none-ngram13-99567-4159-nolda-sampling_none-LogisticRegression-star_rating.pipeline\n",
      "INFO:util.time_util:End timer for: model_save_time_min\n",
      "INFO:util.time_util:Total time for model_save_time_min: 0.04\n",
      "INFO:util.time_util:Start timer for: predict_time_min\n",
      "INFO:util.time_util:End timer for: predict_time_min\n",
      "INFO:util.time_util:Total time for predict_time_min: 0.0\n",
      "INFO:util.model_wrapper:Finished running model: name: LogisticRegression\n",
      "\twith file: /home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-100k-preprocessed.csv\n",
      "\twith description: review_body-tfidf-df_none-ngram13-99567-4159-nolda-sampling_none-LogisticRegression-star_rating\n",
      "\tstatus: new\n",
      "INFO:util.model_wrapper:calculating classification report...\n",
      "INFO:util.time_util:Start timer for: cr_time_min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: timestamp (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- 1_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 2_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 3_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 1_counts: vector (nullable = true)\n",
      " |-- 2_counts: vector (nullable = true)\n",
      " |-- 3_counts: vector (nullable = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:util.time_util:End timer for: cr_time_min\n",
      "INFO:util.time_util:Total time for cr_time_min: 0.41\n",
      "INFO:util.model_wrapper:calculating confusion matrix...\n",
      "INFO:util.time_util:Start timer for: cm_time_min\n",
      "INFO:util.time_util:End timer for: cm_time_min\n",
      "INFO:util.time_util:Total time for cm_time_min: 0.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traing time duration: 1 minutes\n",
      "+-----------+----------+--------------------+--------------------+\n",
      "|star_rating|prediction|       rawPrediction|         probability|\n",
      "+-----------+----------+--------------------+--------------------+\n",
      "|          1|       2.0|[-8.1948791419873...|[1.14526774859801...|\n",
      "|          1|       1.0|[-8.1860780942214...|[3.76223062941299...|\n",
      "|          1|       1.0|[-8.2341209709348...|[4.44985968567783...|\n",
      "|          1|       1.0|[-8.1863929047636...|[5.44609080299316...|\n",
      "|          1|       2.0|[-8.1943006037097...|[3.35845441805422...|\n",
      "|          1|       1.0|[-8.1813635687497...|[2.05734359570510...|\n",
      "|          1|       1.0|[-8.1803152040421...|[3.98463868179808...|\n",
      "|          1|       1.0|[-8.2019638612832...|[2.90404724242306...|\n",
      "|          1|       2.0|[-8.1879056376964...|[5.63631355110019...|\n",
      "|          1|       2.0|[-8.1947269795437...|[6.37813046297268...|\n",
      "+-----------+----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import importlib\n",
    "import util.constants as c\n",
    "importlib.reload(pyu)\n",
    "importlib.reload(mw)\n",
    "importlib.reload(c)\n",
    "\n",
    "feature_count = train.select(\"features\").limit(1).toPandas().features.values[0].size\n",
    "\n",
    "train_timer = Timer(\"traing time\")\n",
    "lr = LogisticRegression(labelCol=\"star_rating\", \n",
    "                        featuresCol=\"features\", \n",
    "                        weightCol=\"class_weights\",\n",
    "                        maxIter=100)\n",
    "\n",
    "\n",
    "model = pyu.PysparkModel(model = lr,\n",
    "                    train_df = train,\n",
    "                    test_df = test,\n",
    "                    label_column = \"star_rating\",\n",
    "                    feature_column = \"features\",\n",
    "                    n_classes = 5,\n",
    "                         pipeline = pipeline,\n",
    "                         file = DATA_FILE,\n",
    "                         description=f'review_body-tfidf-df_none-ngram13-{df.count()}-{feature_count}-nolda-sampling_none{TEST_STR}',\n",
    "                        model_dir=\"../../models\")\n",
    "\n",
    "report_dict, predict_test = model.run()\n",
    "train_timer.stop()\n",
    "\n",
    "pyu.show_df(predict_test, [\"star_rating\", \"prediction\", \"rawPrediction\", \"probability\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: timestamp (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- 1_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 2_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 3_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 1_counts: vector (nullable = true)\n",
      " |-- 2_counts: vector (nullable = true)\n",
      " |-- 3_counts: vector (nullable = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate our Model\n",
    "\n",
    "Reference:\n",
    "* https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html#multiclass-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'LogisticRegression',\n",
       " 'description': 'review_body-tfidf-df_none-ngram13-99567-4159-nolda-sampling_none-LogisticRegression-star_rating',\n",
       " 'library': 'pyspark',\n",
       " 'file': '/home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-100k-preprocessed.csv',\n",
       " 'model_file': '../../models/review_body-tfidf-df_none-ngram13-99567-4159-nolda-sampling_none-LogisticRegression-star_rating.pyspark',\n",
       " 'pipeline_file': '../../models/review_body-tfidf-df_none-ngram13-99567-4159-nolda-sampling_none-LogisticRegression-star_rating.pipeline',\n",
       " 'status': 'success',\n",
       " 'status_date': '2019-11-22 18:26:20',\n",
       " 'classification_report': '{\"1\": {\"precision\": 0.6406469760900141, \"recall\": 0.6483985765124555, \"f1-score\": 0.644499469402193, \"support\": 1405}, \"2\": {\"precision\": 0.23336968375136313, \"recall\": 0.31892697466467956, \"f1-score\": 0.26952141057934503, \"support\": 671}, \"3\": {\"precision\": 0.26875515251442705, \"recall\": 0.3622222222222222, \"f1-score\": 0.3085660198769522, \"support\": 900}, \"4\": {\"precision\": 0.34980430528375733, \"recall\": 0.4188635032220269, \"f1-score\": 0.3812316715542522, \"support\": 1707}, \"5\": {\"precision\": 0.8518183871696409, \"recall\": 0.7061797752808989, \"f1-score\": 0.77219207535579, \"support\": 5340}, \"accuracy\": 0.5923376234660281, \"macro avg\": {\"precision\": 0.4688789009618405, \"recall\": 0.49091821038045663, \"f1-score\": 0.4752021293537065, \"support\": 10023}, \"weighted avg\": {\"precision\": 0.6429617712333509, \"recall\": 0.5923376234660281, \"f1-score\": 0.6124262381164428, \"support\": 10023}}',\n",
       " 'confusion_matrix': '[[911, 280, 132, 46, 36], [221, 214, 150, 61, 25], [115, 185, 326, 195, 79], [57, 112, 307, 715, 516], [118, 126, 298, 1027, 3771]]',\n",
       " 'train_examples': 89544,\n",
       " 'train_features': 4159,\n",
       " 'test_examples': 10023,\n",
       " 'test_features': 4159,\n",
       " 'train_time_min': 0.32,\n",
       " 'total_time_min': 1.1400000000000001,\n",
       " 'model_save_time_min': 0.04,\n",
       " 'predict_time_min': 0.0,\n",
       " 'cr_time_min': 0.41,\n",
       " 'cm_time_min': 0.37}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'f1-score': 0.644499469402193,\n",
      "       'precision': 0.6406469760900141,\n",
      "       'recall': 0.6483985765124555,\n",
      "       'support': 1405},\n",
      " '2': {'f1-score': 0.26952141057934503,\n",
      "       'precision': 0.23336968375136313,\n",
      "       'recall': 0.31892697466467956,\n",
      "       'support': 671},\n",
      " '3': {'f1-score': 0.3085660198769522,\n",
      "       'precision': 0.26875515251442705,\n",
      "       'recall': 0.3622222222222222,\n",
      "       'support': 900},\n",
      " '4': {'f1-score': 0.3812316715542522,\n",
      "       'precision': 0.34980430528375733,\n",
      "       'recall': 0.4188635032220269,\n",
      "       'support': 1707},\n",
      " '5': {'f1-score': 0.77219207535579,\n",
      "       'precision': 0.8518183871696409,\n",
      "       'recall': 0.7061797752808989,\n",
      "       'support': 5340},\n",
      " 'accuracy': 0.5923376234660281,\n",
      " 'macro avg': {'f1-score': 0.4752021293537065,\n",
      "               'precision': 0.4688789009618405,\n",
      "               'recall': 0.49091821038045663,\n",
      "               'support': 10023},\n",
      " 'weighted avg': {'f1-score': 0.6124262381164428,\n",
      "                  'precision': 0.6429617712333509,\n",
      "                  'recall': 0.5923376234660281,\n",
      "                  'support': 10023}}\n"
     ]
    }
   ],
   "source": [
    "pprint(json.loads(report_dict[\"classification_report\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 911  280  132   46   36]\n",
      " [ 221  214  150   61   25]\n",
      " [ 115  185  326  195   79]\n",
      " [  57  112  307  715  516]\n",
      " [ 118  126  298 1027 3771]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(json.loads(report_dict[\"confusion_matrix\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall score: 0.45455028038919754\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def calculate_score(cr: dict):\n",
    "    \n",
    "    values = []\n",
    "    values.append(cr[\"1\"][\"recall\"])\n",
    "    values.append(cr[\"2\"][\"recall\"])\n",
    "    values.append(cr[\"3\"][\"recall\"])\n",
    "    values.append(cr[\"4\"][\"recall\"])\n",
    "    values.append(cr[\"5\"][\"precision\"])\n",
    "    \n",
    "    mean = 0\n",
    "    for v in values:\n",
    "        if v == 0:\n",
    "            mean = 0\n",
    "            break\n",
    "        else:\n",
    "            mean += 1 / v\n",
    "    if mean > 0:\n",
    "        mean = len(values) / mean\n",
    "\n",
    "    return mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "score = calculate_score(json.loads(report_dict['classification_report']))\n",
    "print(f'Overall score: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification_report</th>\n",
       "      <th>cm_time_min</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>cr_time_min</th>\n",
       "      <th>description</th>\n",
       "      <th>file</th>\n",
       "      <th>library</th>\n",
       "      <th>model_file</th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_save_time_min</th>\n",
       "      <th>pipeline_file</th>\n",
       "      <th>predict_time_min</th>\n",
       "      <th>status</th>\n",
       "      <th>status_date</th>\n",
       "      <th>test_examples</th>\n",
       "      <th>test_features</th>\n",
       "      <th>total_time_min</th>\n",
       "      <th>train_examples</th>\n",
       "      <th>train_features</th>\n",
       "      <th>train_time_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>152</td>\n",
       "      <td>526.00</td>\n",
       "      <td>1885]]'</td>\n",
       "      <td>0.35</td>\n",
       "      <td>review_body-tfidf-df_none-ngram13-49784-4254-n...</td>\n",
       "      <td>/home/jupyter/dataset/amazon_reviews/amazon_re...</td>\n",
       "      <td>pyspark</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.06</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>success</td>\n",
       "      <td>2019-11-22 07:50:22</td>\n",
       "      <td>5058.0</td>\n",
       "      <td>4254.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>44726.0</td>\n",
       "      <td>4254.0</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"1\": {\"precision\": 0.6406469760900141, \"recal...</td>\n",
       "      <td>0.32</td>\n",
       "      <td>[[911, 280, 132, 46, 36], [221, 214, 150, 61, ...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>review_body-tfidf-df_none-ngram13-99567-4159-n...</td>\n",
       "      <td>/home/jupyter/dataset/amazon_reviews/amazon_re...</td>\n",
       "      <td>pyspark</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>success</td>\n",
       "      <td>2019-11-22 07:53:26</td>\n",
       "      <td>10023.0</td>\n",
       "      <td>4159.0</td>\n",
       "      <td>1.15</td>\n",
       "      <td>89544.0</td>\n",
       "      <td>4159.0</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\"1\": {\"precision\": 0.6127167630057804, \"recal...</td>\n",
       "      <td>0.28</td>\n",
       "      <td>[[424, 173, 80, 31, 27], [90, 97, 73, 35, 19],...</td>\n",
       "      <td>0.30</td>\n",
       "      <td>review_body-tfidf-df_none-ngram13-49784-4254-n...</td>\n",
       "      <td>/home/jupyter/dataset/amazon_reviews/amazon_re...</td>\n",
       "      <td>pyspark</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.04</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>success</td>\n",
       "      <td>2019-11-22 18:23:18</td>\n",
       "      <td>5058.0</td>\n",
       "      <td>4254.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>44726.0</td>\n",
       "      <td>4254.0</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{\"1\": {\"precision\": 0.6406469760900141, \"recal...</td>\n",
       "      <td>0.37</td>\n",
       "      <td>[[911, 280, 132, 46, 36], [221, 214, 150, 61, ...</td>\n",
       "      <td>0.41</td>\n",
       "      <td>review_body-tfidf-df_none-ngram13-99567-4159-n...</td>\n",
       "      <td>/home/jupyter/dataset/amazon_reviews/amazon_re...</td>\n",
       "      <td>pyspark</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.04</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>success</td>\n",
       "      <td>2019-11-22 18:26:20</td>\n",
       "      <td>10023.0</td>\n",
       "      <td>4159.0</td>\n",
       "      <td>1.14</td>\n",
       "      <td>89544.0</td>\n",
       "      <td>4159.0</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               classification_report  cm_time_min  \\\n",
       "0                                                152       526.00   \n",
       "1  {\"1\": {\"precision\": 0.6406469760900141, \"recal...         0.32   \n",
       "2  {\"1\": {\"precision\": 0.6127167630057804, \"recal...         0.28   \n",
       "3  {\"1\": {\"precision\": 0.6406469760900141, \"recal...         0.37   \n",
       "\n",
       "                                    confusion_matrix  cr_time_min  \\\n",
       "0                                            1885]]'         0.35   \n",
       "1  [[911, 280, 132, 46, 36], [221, 214, 150, 61, ...         0.45   \n",
       "2  [[424, 173, 80, 31, 27], [90, 97, 73, 35, 19],...         0.30   \n",
       "3  [[911, 280, 132, 46, 36], [221, 214, 150, 61, ...         0.41   \n",
       "\n",
       "                                         description  \\\n",
       "0  review_body-tfidf-df_none-ngram13-49784-4254-n...   \n",
       "1  review_body-tfidf-df_none-ngram13-99567-4159-n...   \n",
       "2  review_body-tfidf-df_none-ngram13-49784-4254-n...   \n",
       "3  review_body-tfidf-df_none-ngram13-99567-4159-n...   \n",
       "\n",
       "                                                file  library  \\\n",
       "0  /home/jupyter/dataset/amazon_reviews/amazon_re...  pyspark   \n",
       "1  /home/jupyter/dataset/amazon_reviews/amazon_re...  pyspark   \n",
       "2  /home/jupyter/dataset/amazon_reviews/amazon_re...  pyspark   \n",
       "3  /home/jupyter/dataset/amazon_reviews/amazon_re...  pyspark   \n",
       "\n",
       "                                          model_file          model_name  \\\n",
       "0  ../../models/review_body-tfidf-df_none-ngram13...  LogisticRegression   \n",
       "1  ../../models/review_body-tfidf-df_none-ngram13...  LogisticRegression   \n",
       "2  ../../models/review_body-tfidf-df_none-ngram13...  LogisticRegression   \n",
       "3  ../../models/review_body-tfidf-df_none-ngram13...  LogisticRegression   \n",
       "\n",
       "   model_save_time_min                                      pipeline_file  \\\n",
       "0                 0.06  ../../models/review_body-tfidf-df_none-ngram13...   \n",
       "1                 0.05  ../../models/review_body-tfidf-df_none-ngram13...   \n",
       "2                 0.04  ../../models/review_body-tfidf-df_none-ngram13...   \n",
       "3                 0.04  ../../models/review_body-tfidf-df_none-ngram13...   \n",
       "\n",
       "   predict_time_min   status          status_date  test_examples  \\\n",
       "0               0.0  success  2019-11-22 07:50:22         5058.0   \n",
       "1               0.0  success  2019-11-22 07:53:26        10023.0   \n",
       "2               0.0  success  2019-11-22 18:23:18         5058.0   \n",
       "3               0.0  success  2019-11-22 18:26:20        10023.0   \n",
       "\n",
       "   test_features  total_time_min  train_examples  train_features  \\\n",
       "0         4254.0            1.00         44726.0          4254.0   \n",
       "1         4159.0            1.15         89544.0          4159.0   \n",
       "2         4254.0            0.89         44726.0          4254.0   \n",
       "3         4159.0            1.14         89544.0          4159.0   \n",
       "\n",
       "   train_time_min  \n",
       "0            0.30  \n",
       "1            0.33  \n",
       "2            0.27  \n",
       "3            0.32  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(REPORT_FILE):\n",
    "    report_df = pd.read_csv(REPORT_FILE, quotechar=\"'\")\n",
    "else:\n",
    "    report_df = pd.DataFrame()\n",
    "report_df = report_df.append(report_dict, ignore_index=True)\n",
    "report_df.to_csv(REPORT_FILE, index=False, quotechar=\"'\")\n",
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
