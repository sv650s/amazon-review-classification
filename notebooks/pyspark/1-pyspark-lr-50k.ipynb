{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Logistic Regression\n",
    "\n",
    "Our full dataset file has around 9 million samples. When trying to run feature_generator.\n",
    "\n",
    "Machine:\n",
    "* 2018 Mac Mini - 6 core\n",
    "\n",
    "Docker Configuration:\n",
    "* 9 CPUs\n",
    "* 24 GB Ram\n",
    "* 3 GB swap\n",
    "\n",
    "\n",
    "# References:\n",
    "\n",
    "* https://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html\n",
    "* https://towardsdatascience.com/countvectorizer-hashingtf-e66f169e2d4e\n",
    "* https://medium.com/@dhiraj.p.rai/logistic-regression-in-spark-ml-8a95b5f5434c\n",
    "* packaging spark job - https://developerzen.com/best-practices-writing-production-grade-pyspark-jobs-cb688ac4d20f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkFiles\n",
    "from pyspark.sql import SparkSession, DataFrameReader, SQLContext\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import NGram, CountVectorizer, VectorAssembler, Tokenizer, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "import logging\n",
    "\n",
    "import util.pyspark_util as pyu\n",
    "import util.model_wrapper as mw\n",
    "\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "# allow DEBUG to be set by command line\n",
    "DEBUG = bool(os.environ.get(\"IPYNB_DEBUG\", False))\n",
    "\n",
    "N_CLASSES = 5\n",
    "FEATURE_COLUMN = \"review_body\"\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    MIN_DF = 1\n",
    "    DF_PERCENTAGE = 0.001\n",
    "    DATA_FILE = \"/home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-test1k-preprocessed.csv\"\n",
    "    TEST_STR=\"-test\"\n",
    "else:\n",
    "    DF_PERCENTAGE = 0.001\n",
    "    DATA_FILE = \"/home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-50k-preprocessed.csv\"\n",
    "    TEST_STR=\"\"\n",
    "\n",
    "REPORT_FILE = f\"../../reports/201911-pyspark-report{TEST_STR}.csv\"\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .appName(\"Pyspark Wrapper Test (local)\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Timer(object):\n",
    "    \n",
    "    def __init__(self, description: str):\n",
    "        self.start_time = datetime.now()\n",
    "        self.description = description\n",
    "        \n",
    "    def stop(self):\n",
    "        self.end_time = datetime.now()\n",
    "        self.print_duration_min()\n",
    "        \n",
    "    def print_duration_min(self):\n",
    "        self.duration = int((self.end_time - self.start_time).total_seconds() / 60)\n",
    "        print(f\"{self.description} duration: {self.duration} minutes\")\n",
    "        \n",
    "    def get_duraction_min(self):\n",
    "        return self.duration\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file load time duration: 0 minutes\n"
     ]
    }
   ],
   "source": [
    "file_timer = Timer(\"file load time\")\n",
    "df = spark.read.csv(SparkFiles.get(DATA_FILE), \n",
    "                    header=True, \n",
    "                    inferSchema= True)\n",
    "df.collect()\n",
    "file_timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Creating TFIDF using min_df: 49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline time duration: 0 minutes\n"
     ]
    }
   ],
   "source": [
    "def build_ngrams(inputCol, min_df, n=3):\n",
    "    log.info(f'Creating TFIDF using min_df: {min_df}')\n",
    "    \n",
    "    tokenizer = [Tokenizer(inputCol = inputCol, outputCol = \"words\")]\n",
    "    \n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    vectorizers = [\n",
    "        CountVectorizer(minDF=min_df, inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_counts\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_counts\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"raw_features\"\n",
    "    )]\n",
    "    \n",
    "    idf = [IDF().setInputCol(\"raw_features\").setOutputCol(\"features\")]\n",
    "#     idf = [IDF(minDocFreq=min_df).setInputCol(\"raw_features\").setOutputCol(\"features\")]\n",
    "\n",
    "    return Pipeline(stages=tokenizer + ngrams + vectorizers + assembler + idf)\n",
    "\n",
    "\n",
    "\n",
    "pipeline_timer = Timer(\"pipeline time\")\n",
    "# calculate a reasonable min_df\n",
    "min_df = max(int(df.count() * DF_PERCENTAGE), 1)\n",
    "\n",
    "pipeline = build_ngrams(FEATURE_COLUMN, min_df)\n",
    "df = pipeline.fit(df).transform(df)\n",
    "pipeline_timer.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: timestamp (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- 1_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 2_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 3_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 1_counts: vector (nullable = true)\n",
      " |-- 2_counts: vector (nullable = true)\n",
      " |-- 3_counts: vector (nullable = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|star_rating|features                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|5          |(4254,[21,30,42,70,91,107,161,198,306,307,325,410,833,1046,2077,2186],[2.2011491106752246,2.5958735586451045,2.6874739570502397,6.10659681136796,3.214566553280102,3.5841820084945692,3.647659828505741,3.9558541091679826,4.1782109815377275,4.280227741808526,4.5091937258741694,4.669139755153287,5.517151646274148,5.746564810601953,6.82648496625791,6.883643380097859])                                                               |\n",
      "|5          |(4254,[6,83,2313],[1.6743715435881628,3.0704662093063453,4.872669637695483])                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|5          |(4254,[6,8,10,18,731,1150,2239,2268,2442,3510],[1.6743715435881628,1.7654147707793442,1.9017841880968913,2.040228554083857,5.5843603959675985,6.1059388115098505,3.8447389346786593,4.421878258871553,5.468361482104716,6.864225294240757])                                                                                                                                                                                                 |\n",
      "|5          |(4254,[3,10,25,130,222,2236,3317],[1.466149613564551,1.9017841880968913,2.4204434553801546,3.4099733496227125,3.860830147941197,3.7278953072642116,6.581362508224926])                                                                                                                                                                                                                                                                      |\n",
      "|5          |(4254,[5,9,10,23,33,75,83,90,109,133,229,403,489,576,646,2604,2909,3181,3284],[1.7528169137456058,1.8659740593442231,1.9017841880968913,2.323798778637033,2.5485336652117287,6.373902772494258,3.0704662093063453,6.477718491698294,3.2957766087056455,3.523812803647724,8.112427484316983,4.604868935797532,4.821507585515615,4.904672368781657,5.180679409652935,12.08956877671304,6.251120821354348,6.471663590968501,6.552789135780869])|\n",
      "|5          |(4254,[1,3,10,18,86,126,243,1076,1893,2236,2295,2340,2759,4147],[1.1811689501016733,1.466149613564551,3.8035683761937826,2.040228554083857,3.3072302381435215,3.4148484354510495,3.921812658219549,5.679670575771923,6.566973770772826,3.7278953072642116,4.726424137375338,5.010334043905696,6.1059388115098505,6.552789135780869])                                                                                                        |\n",
      "|5          |(4254,[1,18,20,25,243,2295,2443,2615],[1.1811689501016733,2.040228554083857,2.0714590247254416,2.4204434553801546,3.921812658219549,4.726424137375338,5.4779309331208665,5.866709122444016])                                                                                                                                                                                                                                                |\n",
      "|4          |(4254,[0,2,9,48,60,100,103,152,260,1047,2164,2551,3019,3894],[0.870551650522917,1.3806260134941613,3.7319481186884462,2.807103442509265,2.9779146519411004,3.261658160813953,3.2600870685819117,3.6632001567896455,4.033276956815393,5.667974536008732,6.790117322087036,5.715602584997987,6.361121716568677,6.864225294240757])                                                                                                            |\n",
      "|5          |(4254,[8,13,25,2357,3969],[1.7654147707793442,1.9372503547299569,2.4204434553801546,5.131889245483503,6.903446007394039])                                                                                                                                                                                                                                                                                                                   |\n",
      "|4          |(4254,[10,31,2689],[1.9017841880968913,2.549047539837631,5.9634387489025675])                                                                                                                                                                                                                                                                                                                                                               |\n",
      "+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "pyu.show_df(df, [\"star_rating\", \"features\"], truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test split duration: 0 minutes\n",
      "Training size: 44726 Test size: 5058\n"
     ]
    }
   ],
   "source": [
    "split_timer = Timer(\"train test split\")\n",
    "train, test = df.randomSplit([0.9, 0.1], seed=1)\n",
    "split_timer.stop()\n",
    "\n",
    "\n",
    "train_size = train.count()\n",
    "test_size = test.count()\n",
    "\n",
    "print(f'Training size: {train_size} Test size: {test_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4254"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train.select(\"features\").limit(1).toPandas().features.values[0])\n",
    "train.select(\"features\").limit(1).toPandas().features.values[0].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign class weights to handle imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn class weights: [1.44184397 3.07184066 2.12273374 1.17917216 0.37569089]\n",
      "skelarn class weight duration: 0 minutes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# only do this for small files - takes too long for large datasets - we will custom compute this\n",
    "# if DEBUG:\n",
    "cw_timer = Timer(\"skelarn class weight\")\n",
    "labels = train.select(\"star_rating\").toPandas().astype({\"star_rating\": np.int8})\n",
    "class_weights = compute_class_weight('balanced', \n",
    "                                                  np.arange(1, N_CLASSES+1), \n",
    "                                                  labels.star_rating.tolist())\n",
    "print(f'sklearn class weights: {class_weights}')\n",
    "cw_timer.stop()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:util.pyspark_util:sampling percentage: 0.00044716719581451504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+--------------------+\n",
      "|star_rating|      class_weights|            features|\n",
      "+-----------+-------------------+--------------------+\n",
      "|          1| 1.4418439716312057|(4254,[0,8,30,32,...|\n",
      "|          3|  2.122733744660655|(4254,[0,13,15,94...|\n",
      "|          3|  2.122733744660655|(4254,[8,10,2268]...|\n",
      "|          3|  2.122733744660655|(4254,[0,1,5,6,8,...|\n",
      "|          4| 1.1791721592407065|(4254,[10,26,142,...|\n",
      "|          4| 1.1791721592407065|(4254,[0,1,2,4,8,...|\n",
      "|          4| 1.1791721592407065|(4254,[1,6,7,13,1...|\n",
      "|          5|0.37569088618227636|(4254,[0,1,2,14,1...|\n",
      "|          5|0.37569088618227636|(4254,[1143],[5.7...|\n",
      "|          5|0.37569088618227636|(4254,[132,193,16...|\n",
      "|          5|0.37569088618227636|(4254,[0,16,21,26...|\n",
      "|          5|0.37569088618227636|(4254,[4,8,20,32,...|\n",
      "|          2| 3.0718406593406593|(4254,[0,3,6,17,2...|\n",
      "|          4| 1.1791721592407065|(4254,[0,2,3,4,5,...|\n",
      "|          4| 1.1791721592407065|(4254,[3,5,48,50,...|\n",
      "|          5|0.37569088618227636|(4254,[1,2,18,19,...|\n",
      "|          5|0.37569088618227636|(4254,[4,8,9,11,3...|\n",
      "|          5|0.37569088618227636|(4254,[0,2,28,37,...|\n",
      "|          5|0.37569088618227636|(4254,[0,5,9,11,2...|\n",
      "|          5|0.37569088618227636|(4254,[6,7,21,27,...|\n",
      "+-----------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = train.withColumn(\"class_weights\", lit(0))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 1, class_weights[0]).otherwise(train.class_weights))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 2, class_weights[1]).otherwise(train.class_weights))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 3, class_weights[2]).otherwise(train.class_weights))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 4, class_weights[3]).otherwise(train.class_weights))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 5, class_weights[4]).otherwise(train.class_weights))\n",
    "\n",
    "\n",
    "pyu.show_df(train, [\"star_rating\", \"class_weights\", \"features\"], 20, sample=True, truncate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:util.model_wrapper:derived name: LogisticRegression\n",
      "INFO:util.model_wrapper:########################################\n",
      "INFO:util.model_wrapper:Running model: name: LogisticRegression\n",
      "\twith file: /home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-50k-preprocessed.csv\n",
      "\twith description: review_body-tfidf-df_none-ngram13-49784-4254-nolda-sampling_none-LogisticRegression-star_rating\n",
      "\tstatus: new\n",
      "INFO:util.model_wrapper:########################################\n",
      "INFO:util.time_util:Start timer for: train_time_min\n",
      "INFO:util.time_util:End timer for: train_time_min\n",
      "INFO:util.time_util:Total time for train_time_min: 0.27\n",
      "INFO:util.time_util:Start timer for: model_save_time_min\n",
      "INFO:util.pyspark_util:Saving model to file: ../../models/review_body-tfidf-df_none-ngram13-49784-4254-nolda-sampling_none-LogisticRegression-star_rating.pyspark\n",
      "INFO:util.pyspark_util:Saving pipeline to file: ../../models/review_body-tfidf-df_none-ngram13-49784-4254-nolda-sampling_none-LogisticRegression-star_rating.pipeline\n",
      "INFO:util.time_util:End timer for: model_save_time_min\n",
      "INFO:util.time_util:Total time for model_save_time_min: 0.04\n",
      "INFO:util.time_util:Start timer for: predict_time_min\n",
      "INFO:util.time_util:End timer for: predict_time_min\n",
      "INFO:util.time_util:Total time for predict_time_min: 0.0\n",
      "INFO:util.model_wrapper:Finished running model: name: LogisticRegression\n",
      "\twith file: /home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-50k-preprocessed.csv\n",
      "\twith description: review_body-tfidf-df_none-ngram13-49784-4254-nolda-sampling_none-LogisticRegression-star_rating\n",
      "\tstatus: new\n",
      "INFO:util.model_wrapper:calculating classification report...\n",
      "INFO:util.time_util:Start timer for: cr_time_min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: timestamp (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- 1_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 2_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 3_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 1_counts: vector (nullable = true)\n",
      " |-- 2_counts: vector (nullable = true)\n",
      " |-- 3_counts: vector (nullable = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:util.time_util:End timer for: cr_time_min\n",
      "INFO:util.time_util:Total time for cr_time_min: 0.3\n",
      "INFO:util.model_wrapper:calculating confusion matrix...\n",
      "INFO:util.time_util:Start timer for: cm_time_min\n",
      "INFO:util.time_util:End timer for: cm_time_min\n",
      "INFO:util.time_util:Total time for cm_time_min: 0.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traing time duration: 1 minutes\n",
      "+-----------+----------+--------------------+--------------------+\n",
      "|star_rating|prediction|       rawPrediction|         probability|\n",
      "+-----------+----------+--------------------+--------------------+\n",
      "|          1|       1.0|[-7.6362799614976...|[8.34402339204206...|\n",
      "|          1|       1.0|[-7.6256963701932...|[1.04760082690356...|\n",
      "|          1|       4.0|[-7.6501462506985...|[8.03360525043608...|\n",
      "|          1|       1.0|[-7.7807623610401...|[2.01165931554026...|\n",
      "|          1|       1.0|[-7.6921277793741...|[1.33531683134521...|\n",
      "|          1|       1.0|[-7.6112744642495...|[6.81083623904764...|\n",
      "|          1|       2.0|[-7.6178056389892...|[4.67529538542913...|\n",
      "|          1|       1.0|[-7.6414397495410...|[1.30420401297447...|\n",
      "|          1|       2.0|[-7.6609078996627...|[5.17876044339633...|\n",
      "|          1|       2.0|[-7.6698479662183...|[1.65296690962550...|\n",
      "+-----------+----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import importlib\n",
    "import util.constants as c\n",
    "importlib.reload(pyu)\n",
    "importlib.reload(mw)\n",
    "importlib.reload(c)\n",
    "\n",
    "feature_count = train.select(\"features\").limit(1).toPandas().features.values[0].size\n",
    "\n",
    "train_timer = Timer(\"traing time\")\n",
    "lr = LogisticRegression(labelCol=\"star_rating\", \n",
    "                        featuresCol=\"features\", \n",
    "                        weightCol=\"class_weights\",\n",
    "                        maxIter=100)\n",
    "\n",
    "\n",
    "model = pyu.PysparkModel(model = lr,\n",
    "                    train_df = train,\n",
    "                    test_df = test,\n",
    "                    label_column = \"star_rating\",\n",
    "                    feature_column = \"features\",\n",
    "                    n_classes = 5,\n",
    "                         pipeline = pipeline,\n",
    "                         file = DATA_FILE,\n",
    "                         description=f'review_body-tfidf-df_none-ngram13-{df.count()}-{feature_count}-nolda-sampling_none{TEST_STR}',\n",
    "                        model_dir=\"../../models\")\n",
    "\n",
    "report_dict, predict_test = model.run()\n",
    "train_timer.stop()\n",
    "\n",
    "pyu.show_df(predict_test, [\"star_rating\", \"prediction\", \"rawPrediction\", \"probability\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: timestamp (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- 1_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 2_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 3_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 1_counts: vector (nullable = true)\n",
      " |-- 2_counts: vector (nullable = true)\n",
      " |-- 3_counts: vector (nullable = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate our Model\n",
    "\n",
    "Reference:\n",
    "* https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html#multiclass-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'LogisticRegression',\n",
       " 'description': 'review_body-tfidf-df_none-ngram13-49784-4254-nolda-sampling_none-LogisticRegression-star_rating',\n",
       " 'library': 'pyspark',\n",
       " 'file': '/home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-50k-preprocessed.csv',\n",
       " 'model_file': '../../models/review_body-tfidf-df_none-ngram13-49784-4254-nolda-sampling_none-LogisticRegression-star_rating.pyspark',\n",
       " 'pipeline_file': '../../models/review_body-tfidf-df_none-ngram13-49784-4254-nolda-sampling_none-LogisticRegression-star_rating.pipeline',\n",
       " 'status': 'success',\n",
       " 'status_date': '2019-11-22 18:23:18',\n",
       " 'classification_report': '{\"1\": {\"precision\": 0.6127167630057804, \"recall\": 0.5768707482993197, \"f1-score\": 0.5942536790469518, \"support\": 735}, \"2\": {\"precision\": 0.19477911646586346, \"recall\": 0.3089171974522293, \"f1-score\": 0.23891625615763545, \"support\": 314}, \"3\": {\"precision\": 0.2491638795986622, \"recall\": 0.3274725274725275, \"f1-score\": 0.28300094966761635, \"support\": 455}, \"4\": {\"precision\": 0.30935960591133005, \"recall\": 0.38106796116504854, \"f1-score\": 0.3414899401848831, \"support\": 824}, \"5\": {\"precision\": 0.835920177383592, \"recall\": 0.6904761904761905, \"f1-score\": 0.7562688064192579, \"support\": 2730}, \"accuracy\": 0.5672202451561882, \"macro avg\": {\"precision\": 0.44038790847304565, \"recall\": 0.45696092497306307, \"f1-score\": 0.442785926295269, \"support\": 5058}, \"weighted avg\": {\"precision\": 0.6251189063117958, \"recall\": 0.5672202451561882, \"f1-score\": 0.590463254817851, \"support\": 5058}}',\n",
       " 'confusion_matrix': '[[424, 173, 80, 31, 27], [90, 97, 73, 35, 19], [61, 85, 149, 109, 51], [32, 61, 144, 314, 273], [85, 82, 152, 526, 1885]]',\n",
       " 'train_examples': 44726,\n",
       " 'train_features': 4254,\n",
       " 'test_examples': 5058,\n",
       " 'test_features': 4254,\n",
       " 'train_time_min': 0.27,\n",
       " 'total_time_min': 0.89,\n",
       " 'model_save_time_min': 0.04,\n",
       " 'predict_time_min': 0.0,\n",
       " 'cr_time_min': 0.3,\n",
       " 'cm_time_min': 0.28}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'f1-score': 0.5942536790469518,\n",
      "       'precision': 0.6127167630057804,\n",
      "       'recall': 0.5768707482993197,\n",
      "       'support': 735},\n",
      " '2': {'f1-score': 0.23891625615763545,\n",
      "       'precision': 0.19477911646586346,\n",
      "       'recall': 0.3089171974522293,\n",
      "       'support': 314},\n",
      " '3': {'f1-score': 0.28300094966761635,\n",
      "       'precision': 0.2491638795986622,\n",
      "       'recall': 0.3274725274725275,\n",
      "       'support': 455},\n",
      " '4': {'f1-score': 0.3414899401848831,\n",
      "       'precision': 0.30935960591133005,\n",
      "       'recall': 0.38106796116504854,\n",
      "       'support': 824},\n",
      " '5': {'f1-score': 0.7562688064192579,\n",
      "       'precision': 0.835920177383592,\n",
      "       'recall': 0.6904761904761905,\n",
      "       'support': 2730},\n",
      " 'accuracy': 0.5672202451561882,\n",
      " 'macro avg': {'f1-score': 0.442785926295269,\n",
      "               'precision': 0.44038790847304565,\n",
      "               'recall': 0.45696092497306307,\n",
      "               'support': 5058},\n",
      " 'weighted avg': {'f1-score': 0.590463254817851,\n",
      "                  'precision': 0.6251189063117958,\n",
      "                  'recall': 0.5672202451561882,\n",
      "                  'support': 5058}}\n"
     ]
    }
   ],
   "source": [
    "pprint(json.loads(report_dict[\"classification_report\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 424  173   80   31   27]\n",
      " [  90   97   73   35   19]\n",
      " [  61   85  149  109   51]\n",
      " [  32   61  144  314  273]\n",
      " [  85   82  152  526 1885]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(json.loads(report_dict[\"confusion_matrix\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall score: 0.4221266805001182\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def calculate_score(cr: dict):\n",
    "    \n",
    "    values = []\n",
    "    values.append(cr[\"1\"][\"recall\"])\n",
    "    values.append(cr[\"2\"][\"recall\"])\n",
    "    values.append(cr[\"3\"][\"recall\"])\n",
    "    values.append(cr[\"4\"][\"recall\"])\n",
    "    values.append(cr[\"5\"][\"precision\"])\n",
    "    \n",
    "    mean = 0\n",
    "    for v in values:\n",
    "        if v == 0:\n",
    "            mean = 0\n",
    "            break\n",
    "        else:\n",
    "            mean += 1 / v\n",
    "    if mean > 0:\n",
    "        mean = len(values) / mean\n",
    "\n",
    "    return mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "score = calculate_score(json.loads(report_dict['classification_report']))\n",
    "print(f'Overall score: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification_report</th>\n",
       "      <th>cm_time_min</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>cr_time_min</th>\n",
       "      <th>description</th>\n",
       "      <th>file</th>\n",
       "      <th>library</th>\n",
       "      <th>model_file</th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_save_time_min</th>\n",
       "      <th>pipeline_file</th>\n",
       "      <th>predict_time_min</th>\n",
       "      <th>status</th>\n",
       "      <th>status_date</th>\n",
       "      <th>test_examples</th>\n",
       "      <th>test_features</th>\n",
       "      <th>total_time_min</th>\n",
       "      <th>train_examples</th>\n",
       "      <th>train_features</th>\n",
       "      <th>train_time_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>152</td>\n",
       "      <td>526.00</td>\n",
       "      <td>1885]]'</td>\n",
       "      <td>0.35</td>\n",
       "      <td>review_body-tfidf-df_none-ngram13-49784-4254-n...</td>\n",
       "      <td>/home/jupyter/dataset/amazon_reviews/amazon_re...</td>\n",
       "      <td>pyspark</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.06</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>success</td>\n",
       "      <td>2019-11-22 07:50:22</td>\n",
       "      <td>5058.0</td>\n",
       "      <td>4254.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>44726.0</td>\n",
       "      <td>4254.0</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"1\": {\"precision\": 0.6406469760900141, \"recal...</td>\n",
       "      <td>0.32</td>\n",
       "      <td>[[911, 280, 132, 46, 36], [221, 214, 150, 61, ...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>review_body-tfidf-df_none-ngram13-99567-4159-n...</td>\n",
       "      <td>/home/jupyter/dataset/amazon_reviews/amazon_re...</td>\n",
       "      <td>pyspark</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>success</td>\n",
       "      <td>2019-11-22 07:53:26</td>\n",
       "      <td>10023.0</td>\n",
       "      <td>4159.0</td>\n",
       "      <td>1.15</td>\n",
       "      <td>89544.0</td>\n",
       "      <td>4159.0</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\"1\": {\"precision\": 0.6127167630057804, \"recal...</td>\n",
       "      <td>0.28</td>\n",
       "      <td>[[424, 173, 80, 31, 27], [90, 97, 73, 35, 19],...</td>\n",
       "      <td>0.30</td>\n",
       "      <td>review_body-tfidf-df_none-ngram13-49784-4254-n...</td>\n",
       "      <td>/home/jupyter/dataset/amazon_reviews/amazon_re...</td>\n",
       "      <td>pyspark</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.04</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>success</td>\n",
       "      <td>2019-11-22 18:23:18</td>\n",
       "      <td>5058.0</td>\n",
       "      <td>4254.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>44726.0</td>\n",
       "      <td>4254.0</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               classification_report  cm_time_min  \\\n",
       "0                                                152       526.00   \n",
       "1  {\"1\": {\"precision\": 0.6406469760900141, \"recal...         0.32   \n",
       "2  {\"1\": {\"precision\": 0.6127167630057804, \"recal...         0.28   \n",
       "\n",
       "                                    confusion_matrix  cr_time_min  \\\n",
       "0                                            1885]]'         0.35   \n",
       "1  [[911, 280, 132, 46, 36], [221, 214, 150, 61, ...         0.45   \n",
       "2  [[424, 173, 80, 31, 27], [90, 97, 73, 35, 19],...         0.30   \n",
       "\n",
       "                                         description  \\\n",
       "0  review_body-tfidf-df_none-ngram13-49784-4254-n...   \n",
       "1  review_body-tfidf-df_none-ngram13-99567-4159-n...   \n",
       "2  review_body-tfidf-df_none-ngram13-49784-4254-n...   \n",
       "\n",
       "                                                file  library  \\\n",
       "0  /home/jupyter/dataset/amazon_reviews/amazon_re...  pyspark   \n",
       "1  /home/jupyter/dataset/amazon_reviews/amazon_re...  pyspark   \n",
       "2  /home/jupyter/dataset/amazon_reviews/amazon_re...  pyspark   \n",
       "\n",
       "                                          model_file          model_name  \\\n",
       "0  ../../models/review_body-tfidf-df_none-ngram13...  LogisticRegression   \n",
       "1  ../../models/review_body-tfidf-df_none-ngram13...  LogisticRegression   \n",
       "2  ../../models/review_body-tfidf-df_none-ngram13...  LogisticRegression   \n",
       "\n",
       "   model_save_time_min                                      pipeline_file  \\\n",
       "0                 0.06  ../../models/review_body-tfidf-df_none-ngram13...   \n",
       "1                 0.05  ../../models/review_body-tfidf-df_none-ngram13...   \n",
       "2                 0.04  ../../models/review_body-tfidf-df_none-ngram13...   \n",
       "\n",
       "   predict_time_min   status          status_date  test_examples  \\\n",
       "0               0.0  success  2019-11-22 07:50:22         5058.0   \n",
       "1               0.0  success  2019-11-22 07:53:26        10023.0   \n",
       "2               0.0  success  2019-11-22 18:23:18         5058.0   \n",
       "\n",
       "   test_features  total_time_min  train_examples  train_features  \\\n",
       "0         4254.0            1.00         44726.0          4254.0   \n",
       "1         4159.0            1.15         89544.0          4159.0   \n",
       "2         4254.0            0.89         44726.0          4254.0   \n",
       "\n",
       "   train_time_min  \n",
       "0            0.30  \n",
       "1            0.33  \n",
       "2            0.27  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(REPORT_FILE):\n",
    "    report_df = pd.read_csv(REPORT_FILE, quotechar=\"'\")\n",
    "else:\n",
    "    report_df = pd.DataFrame()\n",
    "report_df = report_df.append(report_dict, ignore_index=True)\n",
    "report_df.to_csv(REPORT_FILE, index=False, quotechar=\"'\")\n",
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
