{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Logistic Regression\n",
    "\n",
    "Our full dataset file has around 9 million samples. When trying to run feature_generator.\n",
    "\n",
    "Machine:\n",
    "* 2018 Mac Mini - 6 core\n",
    "\n",
    "Docker Configuration:\n",
    "* 9 CPUs\n",
    "* 24 GB Ram\n",
    "* 3 GB swap\n",
    "\n",
    "\n",
    "# References:\n",
    "\n",
    "* https://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html\n",
    "* https://towardsdatascience.com/countvectorizer-hashingtf-e66f169e2d4e\n",
    "* https://medium.com/@dhiraj.p.rai/logistic-regression-in-spark-ml-8a95b5f5434c\n",
    "* packaging spark job - https://developerzen.com/best-practices-writing-production-grade-pyspark-jobs-cb688ac4d20f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkFiles\n",
    "from pyspark.sql import SparkSession, DataFrameReader, SQLContext\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import NGram, CountVectorizer, VectorAssembler, Tokenizer, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "import logging\n",
    "\n",
    "import util.pyspark_util as pyu\n",
    "import util.model_wrapper as mw\n",
    "\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "# allow DEBUG to be set by command line\n",
    "DEBUG = bool(os.environ.get(\"IPYNB_DEBUG\", False))\n",
    "\n",
    "N_CLASSES = 5\n",
    "FEATURE_COLUMN = \"review_body\"\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    MIN_DF = 1\n",
    "    DF_PERCENTAGE = 0.001\n",
    "    DATA_FILE = \"/home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-test1k-preprocessed.csv\"\n",
    "    TEST_STR=\"-test\"\n",
    "else:\n",
    "    DF_PERCENTAGE = 0.001\n",
    "    DATA_FILE = \"/home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-200k-preprocessed.csv\"\n",
    "    TEST_STR=\"\"\n",
    "\n",
    "REPORT_FILE = f\"../../reports/201911-pyspark-report{TEST_STR}.csv\"\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .appName(\"Pyspark Wrapper Test (local)\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Timer(object):\n",
    "    \n",
    "    def __init__(self, description: str):\n",
    "        self.start_time = datetime.now()\n",
    "        self.description = description\n",
    "        \n",
    "    def stop(self):\n",
    "        self.end_time = datetime.now()\n",
    "        self.print_duration_min()\n",
    "        \n",
    "    def print_duration_min(self):\n",
    "        self.duration = int((self.end_time - self.start_time).total_seconds() / 60)\n",
    "        print(f\"{self.description} duration: {self.duration} minutes\")\n",
    "        \n",
    "    def get_duraction_min(self):\n",
    "        return self.duration\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file load time duration: 0 minutes\n"
     ]
    }
   ],
   "source": [
    "file_timer = Timer(\"file load time\")\n",
    "df = spark.read.csv(SparkFiles.get(DATA_FILE), \n",
    "                    header=True, \n",
    "                    inferSchema= True)\n",
    "df.collect()\n",
    "file_timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Creating TFIDF using min_df: 199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline time duration: 0 minutes\n"
     ]
    }
   ],
   "source": [
    "def build_ngrams(inputCol, min_df, n=3):\n",
    "    log.info(f'Creating TFIDF using min_df: {min_df}')\n",
    "    \n",
    "    tokenizer = [Tokenizer(inputCol = inputCol, outputCol = \"words\")]\n",
    "    \n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    vectorizers = [\n",
    "        CountVectorizer(minDF=min_df, inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_counts\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_counts\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"raw_features\"\n",
    "    )]\n",
    "    \n",
    "    idf = [IDF().setInputCol(\"raw_features\").setOutputCol(\"features\")]\n",
    "#     idf = [IDF(minDocFreq=min_df).setInputCol(\"raw_features\").setOutputCol(\"features\")]\n",
    "\n",
    "    return Pipeline(stages=tokenizer + ngrams + vectorizers + assembler + idf)\n",
    "\n",
    "\n",
    "\n",
    "pipeline_timer = Timer(\"pipeline time\")\n",
    "# calculate a reasonable min_df\n",
    "min_df = max(int(df.count() * DF_PERCENTAGE), 1)\n",
    "\n",
    "pipeline = build_ngrams(FEATURE_COLUMN, min_df)\n",
    "df = pipeline.fit(df).transform(df)\n",
    "pipeline_timer.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: timestamp (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- 1_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 2_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 3_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 1_counts: vector (nullable = true)\n",
      " |-- 2_counts: vector (nullable = true)\n",
      " |-- 3_counts: vector (nullable = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|star_rating|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+-----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|3          |(4082,[30,95],[2.5618257102350386,3.243069528615184])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|5          |(4082,[4,8,27,30,2187,2217,3096,3941],[1.5937927414206787,1.7748248073847617,2.3520735198239975,2.5618257102350386,3.8217403134242263,4.441697584782239,6.481426489055207,6.405680514897247])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|5          |(4082,[2,3,4,6,13,39,202,218,255,266,318,2104,2179,2316,2413,2526,2779,3009],[1.3795030020991612,2.9349921667178593,1.5937927414206787,1.6757137827340978,1.9242074750266664,2.6760033826681195,3.9783793661833604,3.9366026357252335,3.9981605287246667,4.15743285867198,4.32206835105833,6.85463073494515,3.2179221531601234,5.156833148533248,5.546297915294971,5.79320947460312,6.185581105964265,6.4179130833328815])                                                                                                                                                                                                                                                     |\n",
      "|5          |(4082,[21,31,40,66,87,107,160,196,293,309,335,406,829,1099,1976],[2.22579052639723,2.607973916455152,2.6919601902238015,6.089386381489319,3.2033542556100816,3.587599868189902,3.6692624507152245,3.97409555775819,4.152311208551925,4.308912739411501,4.573220639087563,4.6750207043099135,5.5398835251173075,5.830126418430762,6.737906460637009])                                                                                                                                                                                                                                                                                                                           |\n",
      "|1          |(4082,[0,12,30,84,166,203,300,830,1045,2174,2305,2514,3619],[0.8770115946592212,2.0357710706999077,2.5618257102350386,3.161000678072616,4.071090297502034,3.798385890670141,4.252646765832102,5.748113266769927,5.619713126769793,2.381088991435495,5.114164560104646,5.768798172923439,6.864200185961301])                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|5          |(4082,[3,10,167,219,339,1166,2186,2914,3228],[1.4674960833589297,1.9185137535910577,3.5937907109779172,3.8231177241103214,4.30332717285102,5.808147511711987,3.7423860739362316,6.329620476187203,6.574117151971982])                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|1          |(4082,[5,21,50,52,204,539,897,946,1139,2397,3684],[1.7718076784654238,2.22579052639723,2.883171488615565,2.883889685971151,3.8598512961464313,5.036244790601772,5.43900875873074,5.507176207141524,5.8064766675471695,5.5398835251173075,6.840446099953194])                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|5          |(4082,[6,79,2254],[1.6757137827340978,3.057430864290874,4.837191251406977])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|4          |(4082,[0,26,32,37,40,70,85,96,129,131,2251,2295,3406],[0.8770115946592212,2.6410931967555475,2.55404659671595,2.947381008269662,2.6919601902238015,3.2370427103471595,3.2157922269022983,3.2728328533777504,3.4119258794716463,3.4891427779138966,4.986498286932521,5.076455174151907,6.712800539505933])                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|2          |(4082,[6,7,10,19,39,46,48,52,72,76,111,125,130,147,149,163,173,229,284,290,329,434,552,657,731,2431,2852,2880,3062],[1.6757137827340978,1.8525397764497997,1.9185137535910577,2.1321008833324013,2.6760033826681195,2.7809749067816067,2.8239361997995056,2.883889685971151,3.0551900267742154,3.1984220631207627,3.4119258794716463,3.456294585564027,3.4695949979606993,11.439181489445426,7.379517282598537,3.6397629314744884,3.614832461835081,4.025346669028823,4.2165943344639985,4.130832176874801,4.319800776390549,4.622570298266543,4.932818137468897,5.1075034197378635,5.215171806256192,5.589697230829527,6.290941621622092,6.329620476187203,6.497955791006418])|\n",
      "+-----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "pyu.show_df(df, [\"star_rating\", \"features\"], truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test split duration: 0 minutes\n",
      "Training size: 179168 Test size: 19966\n"
     ]
    }
   ],
   "source": [
    "split_timer = Timer(\"train test split\")\n",
    "train, test = df.randomSplit([0.9, 0.1], seed=1)\n",
    "split_timer.stop()\n",
    "\n",
    "\n",
    "train_size = train.count()\n",
    "test_size = test.count()\n",
    "\n",
    "print(f'Training size: {train_size} Test size: {test_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4082"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train.select(\"features\").limit(1).toPandas().features.values[0])\n",
    "train.select(\"features\").limit(1).toPandas().features.values[0].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign class weights to handle imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn class weights: [1.43979428 3.08299062 2.18697589 1.19425429 0.3722314 ]\n",
      "skelarn class weight duration: 0 minutes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# only do this for small files - takes too long for large datasets - we will custom compute this\n",
    "# if DEBUG:\n",
    "cw_timer = Timer(\"skelarn class weight\")\n",
    "labels = train.select(\"star_rating\").toPandas().astype({\"star_rating\": np.int8})\n",
    "class_weights = compute_class_weight('balanced', \n",
    "                                                  np.arange(1, N_CLASSES+1), \n",
    "                                                  labels.star_rating.tolist())\n",
    "print(f'sklearn class weights: {class_weights}')\n",
    "cw_timer.stop()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:util.pyspark_util:sampling percentage: 0.0001116270762636185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+--------------------+\n",
      "|star_rating|     class_weights|            features|\n",
      "+-----------+------------------+--------------------+\n",
      "|          4|1.1942542909515081|(4082,[0,2,9,11,1...|\n",
      "|          4|1.1942542909515081|(4082,[0,11,80,13...|\n",
      "|          5|0.3722313980907268|(4082,[0,8,113,27...|\n",
      "|          5|0.3722313980907268|(4082,[1,12,13,23...|\n",
      "|          5|0.3722313980907268|(4082,[293,368],[...|\n",
      "|          5|0.3722313980907268|(4082,[343],[4.36...|\n",
      "|          5|0.3722313980907268|(4082,[20],[2.080...|\n",
      "|          5|0.3722313980907268|        (4082,[],[])|\n",
      "|          5|0.3722313980907268|(4082,[32],[2.554...|\n",
      "|          1|1.4397942783670845|(4082,[0,1,2,11,2...|\n",
      "|          1|1.4397942783670845|(4082,[0,6,7,10,1...|\n",
      "|          1|1.4397942783670845|(4082,[0,8,11,44,...|\n",
      "|          4|1.1942542909515081|(4082,[1,2,9,19,5...|\n",
      "|          5|0.3722313980907268|(4082,[0,13,33,51...|\n",
      "|          1|1.4397942783670845|(4082,[0,1,5,9,14...|\n",
      "|          3| 2.186975892584681|(4082,[0,1,5,9,11...|\n",
      "|          4|1.1942542909515081|(4082,[6,17,70,78...|\n",
      "|          4|1.1942542909515081|(4082,[0,1,7,9,12...|\n",
      "|          4|1.1942542909515081|(4082,[0,1,2,5,11...|\n",
      "|          5|0.3722313980907268|(4082,[2,5,20,23,...|\n",
      "+-----------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = train.withColumn(\"class_weights\", lit(0))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 1, class_weights[0]).otherwise(train.class_weights))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 2, class_weights[1]).otherwise(train.class_weights))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 3, class_weights[2]).otherwise(train.class_weights))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 4, class_weights[3]).otherwise(train.class_weights))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 5, class_weights[4]).otherwise(train.class_weights))\n",
    "\n",
    "\n",
    "pyu.show_df(train, [\"star_rating\", \"class_weights\", \"features\"], 20, sample=True, truncate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:util.model_wrapper:derived name: LogisticRegression\n",
      "INFO:util.model_wrapper:########################################\n",
      "INFO:util.model_wrapper:Running model: name: LogisticRegression\n",
      "\twith file: /home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-200k-preprocessed.csv\n",
      "\twith description: review_body-tfidf-df_none-ngram13-199134-4082-nolda-sampling_none-LogisticRegression-star_rating\n",
      "\tstatus: new\n",
      "INFO:util.model_wrapper:########################################\n",
      "INFO:util.time_util:Start timer for: train_time_min\n",
      "INFO:util.time_util:End timer for: train_time_min\n",
      "INFO:util.time_util:Total time for train_time_min: 0.52\n",
      "INFO:util.time_util:Start timer for: model_save_time_min\n",
      "INFO:util.pyspark_util:Saving model to file: ../../models/review_body-tfidf-df_none-ngram13-199134-4082-nolda-sampling_none-LogisticRegression-star_rating.pyspark\n",
      "INFO:util.pyspark_util:Saving pipeline to file: ../../models/review_body-tfidf-df_none-ngram13-199134-4082-nolda-sampling_none-LogisticRegression-star_rating.pipeline\n",
      "INFO:util.time_util:End timer for: model_save_time_min\n",
      "INFO:util.time_util:Total time for model_save_time_min: 0.04\n",
      "INFO:util.time_util:Start timer for: predict_time_min\n",
      "INFO:util.time_util:End timer for: predict_time_min\n",
      "INFO:util.time_util:Total time for predict_time_min: 0.0\n",
      "INFO:util.model_wrapper:Finished running model: name: LogisticRegression\n",
      "\twith file: /home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-200k-preprocessed.csv\n",
      "\twith description: review_body-tfidf-df_none-ngram13-199134-4082-nolda-sampling_none-LogisticRegression-star_rating\n",
      "\tstatus: new\n",
      "INFO:util.model_wrapper:calculating classification report...\n",
      "INFO:util.time_util:Start timer for: cr_time_min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: timestamp (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- 1_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 2_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 3_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 1_counts: vector (nullable = true)\n",
      " |-- 2_counts: vector (nullable = true)\n",
      " |-- 3_counts: vector (nullable = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:util.time_util:End timer for: cr_time_min\n",
      "INFO:util.time_util:Total time for cr_time_min: 0.59\n",
      "INFO:util.model_wrapper:calculating confusion matrix...\n",
      "INFO:util.time_util:Start timer for: cm_time_min\n",
      "INFO:util.time_util:End timer for: cm_time_min\n",
      "INFO:util.time_util:Total time for cm_time_min: 0.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traing time duration: 2 minutes\n",
      "+-----------+----------+--------------------+--------------------+\n",
      "|star_rating|prediction|       rawPrediction|         probability|\n",
      "+-----------+----------+--------------------+--------------------+\n",
      "|          1|       1.0|[-8.7439690243909...|[2.68625533284920...|\n",
      "|          1|       1.0|[-8.7499552556462...|[2.49190025905387...|\n",
      "|          1|       1.0|[-8.8159588509832...|[8.82266752656489...|\n",
      "|          1|       1.0|[-8.7425709950127...|[5.33921453749919...|\n",
      "|          1|       1.0|[-8.7461843059868...|[5.37054888667441...|\n",
      "|          1|       1.0|[-8.7777670928528...|[1.24752771848106...|\n",
      "|          1|       1.0|[-8.7606456737791...|[2.94697189561904...|\n",
      "|          1|       1.0|[-8.7585157476143...|[2.68773006233847...|\n",
      "|          1|       1.0|[-8.7494074513319...|[3.10780043044600...|\n",
      "|          1|       1.0|[-8.7549459388690...|[2.64643488867715...|\n",
      "+-----------+----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import importlib\n",
    "import util.constants as c\n",
    "importlib.reload(pyu)\n",
    "importlib.reload(mw)\n",
    "importlib.reload(c)\n",
    "\n",
    "feature_count = train.select(\"features\").limit(1).toPandas().features.values[0].size\n",
    "\n",
    "train_timer = Timer(\"traing time\")\n",
    "lr = LogisticRegression(labelCol=\"star_rating\", \n",
    "                        featuresCol=\"features\", \n",
    "                        weightCol=\"class_weights\",\n",
    "                        maxIter=100)\n",
    "\n",
    "\n",
    "model = pyu.PysparkModel(model = lr,\n",
    "                    train_df = train,\n",
    "                    test_df = test,\n",
    "                    label_column = \"star_rating\",\n",
    "                    feature_column = \"features\",\n",
    "                    n_classes = 5,\n",
    "                         pipeline = pipeline,\n",
    "                         file = DATA_FILE,\n",
    "                         description=f'review_body-tfidf-df_none-ngram13-{df.count()}-{feature_count}-nolda-sampling_none{TEST_STR}',\n",
    "                        model_dir=\"../../models\")\n",
    "\n",
    "report_dict, predict_test = model.run()\n",
    "train_timer.stop()\n",
    "\n",
    "pyu.show_df(predict_test, [\"star_rating\", \"prediction\", \"rawPrediction\", \"probability\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: timestamp (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- 1_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 2_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 3_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 1_counts: vector (nullable = true)\n",
      " |-- 2_counts: vector (nullable = true)\n",
      " |-- 3_counts: vector (nullable = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate our Model\n",
    "\n",
    "Reference:\n",
    "* https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html#multiclass-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'LogisticRegression',\n",
       " 'description': 'review_body-tfidf-df_none-ngram13-199134-4082-nolda-sampling_none-LogisticRegression-star_rating',\n",
       " 'library': 'pyspark',\n",
       " 'file': '/home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-200k-preprocessed.csv',\n",
       " 'model_file': '../../models/review_body-tfidf-df_none-ngram13-199134-4082-nolda-sampling_none-LogisticRegression-star_rating.pyspark',\n",
       " 'pipeline_file': '../../models/review_body-tfidf-df_none-ngram13-199134-4082-nolda-sampling_none-LogisticRegression-star_rating.pipeline',\n",
       " 'status': 'success',\n",
       " 'status_date': '2019-11-22 18:30:24',\n",
       " 'classification_report': '{\"1\": {\"precision\": 0.6762513312034079, \"recall\": 0.670774647887324, \"f1-score\": 0.6735018561074774, \"support\": 2840}, \"2\": {\"precision\": 0.25162337662337664, \"recall\": 0.3538812785388128, \"f1-score\": 0.29411764705882354, \"support\": 1314}, \"3\": {\"precision\": 0.2925030725112659, \"recall\": 0.3936052921719956, \"f1-score\": 0.33560517038777904, \"support\": 1814}, \"4\": {\"precision\": 0.3743104806934594, \"recall\": 0.4298642533936652, \"f1-score\": 0.4001684919966302, \"support\": 3315}, \"5\": {\"precision\": 0.8598254722191538, \"recall\": 0.7286342787606478, \"f1-score\": 0.7888123226591001, \"support\": 10683}, \"accuracy\": 0.6156966843634178, \"macro avg\": {\"precision\": 0.4909027466501327, \"recall\": 0.5153519501504891, \"f1-score\": 0.49844109764196204, \"support\": 19966}, \"weighted avg\": {\"precision\": 0.6615317156291831, \"recall\": 0.6156966843634178, \"f1-score\": 0.6341502670840408, \"support\": 19966}}',\n",
       " 'confusion_matrix': '[[1905, 566, 257, 48, 64], [408, 465, 326, 82, 33], [204, 377, 714, 370, 149], [94, 201, 572, 1425, 1023], [206, 239, 572, 1882, 7784]]',\n",
       " 'train_examples': 179168,\n",
       " 'train_features': 4082,\n",
       " 'test_examples': 19966,\n",
       " 'test_features': 4082,\n",
       " 'train_time_min': 0.52,\n",
       " 'total_time_min': 1.7,\n",
       " 'model_save_time_min': 0.04,\n",
       " 'predict_time_min': 0.0,\n",
       " 'cr_time_min': 0.59,\n",
       " 'cm_time_min': 0.55}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'f1-score': 0.6735018561074774,\n",
      "       'precision': 0.6762513312034079,\n",
      "       'recall': 0.670774647887324,\n",
      "       'support': 2840},\n",
      " '2': {'f1-score': 0.29411764705882354,\n",
      "       'precision': 0.25162337662337664,\n",
      "       'recall': 0.3538812785388128,\n",
      "       'support': 1314},\n",
      " '3': {'f1-score': 0.33560517038777904,\n",
      "       'precision': 0.2925030725112659,\n",
      "       'recall': 0.3936052921719956,\n",
      "       'support': 1814},\n",
      " '4': {'f1-score': 0.4001684919966302,\n",
      "       'precision': 0.3743104806934594,\n",
      "       'recall': 0.4298642533936652,\n",
      "       'support': 3315},\n",
      " '5': {'f1-score': 0.7888123226591001,\n",
      "       'precision': 0.8598254722191538,\n",
      "       'recall': 0.7286342787606478,\n",
      "       'support': 10683},\n",
      " 'accuracy': 0.6156966843634178,\n",
      " 'macro avg': {'f1-score': 0.49844109764196204,\n",
      "               'precision': 0.4909027466501327,\n",
      "               'recall': 0.5153519501504891,\n",
      "               'support': 19966},\n",
      " 'weighted avg': {'f1-score': 0.6341502670840408,\n",
      "                  'precision': 0.6615317156291831,\n",
      "                  'recall': 0.6156966843634178,\n",
      "                  'support': 19966}}\n"
     ]
    }
   ],
   "source": [
    "pprint(json.loads(report_dict[\"classification_report\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1905  566  257   48   64]\n",
      " [ 408  465  326   82   33]\n",
      " [ 204  377  714  370  149]\n",
      " [  94  201  572 1425 1023]\n",
      " [ 206  239  572 1882 7784]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(json.loads(report_dict[\"confusion_matrix\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall score: 0.4832515239028695\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def calculate_score(cr: dict):\n",
    "    \n",
    "    values = []\n",
    "    values.append(cr[\"1\"][\"recall\"])\n",
    "    values.append(cr[\"2\"][\"recall\"])\n",
    "    values.append(cr[\"3\"][\"recall\"])\n",
    "    values.append(cr[\"4\"][\"recall\"])\n",
    "    values.append(cr[\"5\"][\"precision\"])\n",
    "    \n",
    "    mean = 0\n",
    "    for v in values:\n",
    "        if v == 0:\n",
    "            mean = 0\n",
    "            break\n",
    "        else:\n",
    "            mean += 1 / v\n",
    "    if mean > 0:\n",
    "        mean = len(values) / mean\n",
    "\n",
    "    return mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "score = calculate_score(json.loads(report_dict['classification_report']))\n",
    "print(f'Overall score: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification_report</th>\n",
       "      <th>cm_time_min</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>cr_time_min</th>\n",
       "      <th>description</th>\n",
       "      <th>file</th>\n",
       "      <th>library</th>\n",
       "      <th>model_file</th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_save_time_min</th>\n",
       "      <th>pipeline_file</th>\n",
       "      <th>predict_time_min</th>\n",
       "      <th>status</th>\n",
       "      <th>status_date</th>\n",
       "      <th>test_examples</th>\n",
       "      <th>test_features</th>\n",
       "      <th>total_time_min</th>\n",
       "      <th>train_examples</th>\n",
       "      <th>train_features</th>\n",
       "      <th>train_time_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>152</td>\n",
       "      <td>526.00</td>\n",
       "      <td>1885]]'</td>\n",
       "      <td>0.35</td>\n",
       "      <td>review_body-tfidf-df_none-ngram13-49784-4254-n...</td>\n",
       "      <td>/home/jupyter/dataset/amazon_reviews/amazon_re...</td>\n",
       "      <td>pyspark</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.06</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>success</td>\n",
       "      <td>2019-11-22 07:50:22</td>\n",
       "      <td>5058.0</td>\n",
       "      <td>4254.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>44726.0</td>\n",
       "      <td>4254.0</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"1\": {\"precision\": 0.6406469760900141, \"recal...</td>\n",
       "      <td>0.32</td>\n",
       "      <td>[[911, 280, 132, 46, 36], [221, 214, 150, 61, ...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>review_body-tfidf-df_none-ngram13-99567-4159-n...</td>\n",
       "      <td>/home/jupyter/dataset/amazon_reviews/amazon_re...</td>\n",
       "      <td>pyspark</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.05</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>success</td>\n",
       "      <td>2019-11-22 07:53:26</td>\n",
       "      <td>10023.0</td>\n",
       "      <td>4159.0</td>\n",
       "      <td>1.15</td>\n",
       "      <td>89544.0</td>\n",
       "      <td>4159.0</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\"1\": {\"precision\": 0.6127167630057804, \"recal...</td>\n",
       "      <td>0.28</td>\n",
       "      <td>[[424, 173, 80, 31, 27], [90, 97, 73, 35, 19],...</td>\n",
       "      <td>0.30</td>\n",
       "      <td>review_body-tfidf-df_none-ngram13-49784-4254-n...</td>\n",
       "      <td>/home/jupyter/dataset/amazon_reviews/amazon_re...</td>\n",
       "      <td>pyspark</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.04</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>success</td>\n",
       "      <td>2019-11-22 18:23:18</td>\n",
       "      <td>5058.0</td>\n",
       "      <td>4254.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>44726.0</td>\n",
       "      <td>4254.0</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{\"1\": {\"precision\": 0.6406469760900141, \"recal...</td>\n",
       "      <td>0.37</td>\n",
       "      <td>[[911, 280, 132, 46, 36], [221, 214, 150, 61, ...</td>\n",
       "      <td>0.41</td>\n",
       "      <td>review_body-tfidf-df_none-ngram13-99567-4159-n...</td>\n",
       "      <td>/home/jupyter/dataset/amazon_reviews/amazon_re...</td>\n",
       "      <td>pyspark</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.04</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>success</td>\n",
       "      <td>2019-11-22 18:26:20</td>\n",
       "      <td>10023.0</td>\n",
       "      <td>4159.0</td>\n",
       "      <td>1.14</td>\n",
       "      <td>89544.0</td>\n",
       "      <td>4159.0</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{\"1\": {\"precision\": 0.6762513312034079, \"recal...</td>\n",
       "      <td>0.55</td>\n",
       "      <td>[[1905, 566, 257, 48, 64], [408, 465, 326, 82,...</td>\n",
       "      <td>0.59</td>\n",
       "      <td>review_body-tfidf-df_none-ngram13-199134-4082-...</td>\n",
       "      <td>/home/jupyter/dataset/amazon_reviews/amazon_re...</td>\n",
       "      <td>pyspark</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.04</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>success</td>\n",
       "      <td>2019-11-22 18:30:24</td>\n",
       "      <td>19966.0</td>\n",
       "      <td>4082.0</td>\n",
       "      <td>1.70</td>\n",
       "      <td>179168.0</td>\n",
       "      <td>4082.0</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               classification_report  cm_time_min  \\\n",
       "0                                                152       526.00   \n",
       "1  {\"1\": {\"precision\": 0.6406469760900141, \"recal...         0.32   \n",
       "2  {\"1\": {\"precision\": 0.6127167630057804, \"recal...         0.28   \n",
       "3  {\"1\": {\"precision\": 0.6406469760900141, \"recal...         0.37   \n",
       "4  {\"1\": {\"precision\": 0.6762513312034079, \"recal...         0.55   \n",
       "\n",
       "                                    confusion_matrix  cr_time_min  \\\n",
       "0                                            1885]]'         0.35   \n",
       "1  [[911, 280, 132, 46, 36], [221, 214, 150, 61, ...         0.45   \n",
       "2  [[424, 173, 80, 31, 27], [90, 97, 73, 35, 19],...         0.30   \n",
       "3  [[911, 280, 132, 46, 36], [221, 214, 150, 61, ...         0.41   \n",
       "4  [[1905, 566, 257, 48, 64], [408, 465, 326, 82,...         0.59   \n",
       "\n",
       "                                         description  \\\n",
       "0  review_body-tfidf-df_none-ngram13-49784-4254-n...   \n",
       "1  review_body-tfidf-df_none-ngram13-99567-4159-n...   \n",
       "2  review_body-tfidf-df_none-ngram13-49784-4254-n...   \n",
       "3  review_body-tfidf-df_none-ngram13-99567-4159-n...   \n",
       "4  review_body-tfidf-df_none-ngram13-199134-4082-...   \n",
       "\n",
       "                                                file  library  \\\n",
       "0  /home/jupyter/dataset/amazon_reviews/amazon_re...  pyspark   \n",
       "1  /home/jupyter/dataset/amazon_reviews/amazon_re...  pyspark   \n",
       "2  /home/jupyter/dataset/amazon_reviews/amazon_re...  pyspark   \n",
       "3  /home/jupyter/dataset/amazon_reviews/amazon_re...  pyspark   \n",
       "4  /home/jupyter/dataset/amazon_reviews/amazon_re...  pyspark   \n",
       "\n",
       "                                          model_file          model_name  \\\n",
       "0  ../../models/review_body-tfidf-df_none-ngram13...  LogisticRegression   \n",
       "1  ../../models/review_body-tfidf-df_none-ngram13...  LogisticRegression   \n",
       "2  ../../models/review_body-tfidf-df_none-ngram13...  LogisticRegression   \n",
       "3  ../../models/review_body-tfidf-df_none-ngram13...  LogisticRegression   \n",
       "4  ../../models/review_body-tfidf-df_none-ngram13...  LogisticRegression   \n",
       "\n",
       "   model_save_time_min                                      pipeline_file  \\\n",
       "0                 0.06  ../../models/review_body-tfidf-df_none-ngram13...   \n",
       "1                 0.05  ../../models/review_body-tfidf-df_none-ngram13...   \n",
       "2                 0.04  ../../models/review_body-tfidf-df_none-ngram13...   \n",
       "3                 0.04  ../../models/review_body-tfidf-df_none-ngram13...   \n",
       "4                 0.04  ../../models/review_body-tfidf-df_none-ngram13...   \n",
       "\n",
       "   predict_time_min   status          status_date  test_examples  \\\n",
       "0               0.0  success  2019-11-22 07:50:22         5058.0   \n",
       "1               0.0  success  2019-11-22 07:53:26        10023.0   \n",
       "2               0.0  success  2019-11-22 18:23:18         5058.0   \n",
       "3               0.0  success  2019-11-22 18:26:20        10023.0   \n",
       "4               0.0  success  2019-11-22 18:30:24        19966.0   \n",
       "\n",
       "   test_features  total_time_min  train_examples  train_features  \\\n",
       "0         4254.0            1.00         44726.0          4254.0   \n",
       "1         4159.0            1.15         89544.0          4159.0   \n",
       "2         4254.0            0.89         44726.0          4254.0   \n",
       "3         4159.0            1.14         89544.0          4159.0   \n",
       "4         4082.0            1.70        179168.0          4082.0   \n",
       "\n",
       "   train_time_min  \n",
       "0            0.30  \n",
       "1            0.33  \n",
       "2            0.27  \n",
       "3            0.32  \n",
       "4            0.52  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(REPORT_FILE):\n",
    "    report_df = pd.read_csv(REPORT_FILE, quotechar=\"'\")\n",
    "else:\n",
    "    report_df = pd.DataFrame()\n",
    "report_df = report_df.append(report_dict, ignore_index=True)\n",
    "report_df.to_csv(REPORT_FILE, index=False, quotechar=\"'\")\n",
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
