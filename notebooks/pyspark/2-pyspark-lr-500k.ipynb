{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Logistic Regression\n",
    "\n",
    "Our full dataset file has around 9 million samples. When trying to run feature_generator.\n",
    "\n",
    "Machine:\n",
    "* 2018 Mac Mini - 6 core\n",
    "\n",
    "Docker Configuration:\n",
    "* 9 CPUs\n",
    "* 24 GB Ram\n",
    "* 3 GB swap\n",
    "\n",
    "\n",
    "# References:\n",
    "\n",
    "* https://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html\n",
    "* https://towardsdatascience.com/countvectorizer-hashingtf-e66f169e2d4e\n",
    "* https://medium.com/@dhiraj.p.rai/logistic-regression-in-spark-ml-8a95b5f5434c\n",
    "* packaging spark job - https://developerzen.com/best-practices-writing-production-grade-pyspark-jobs-cb688ac4d20f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkFiles\n",
    "from pyspark.sql import SparkSession, DataFrameReader, SQLContext\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import NGram, CountVectorizer, VectorAssembler, Tokenizer, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "import logging\n",
    "\n",
    "import util.pyspark_util as pyu\n",
    "import util.model_wrapper as mw\n",
    "\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "# allow DEBUG to be set by command line\n",
    "DEBUG = bool(os.environ.get(\"IPYNB_DEBUG\", False))\n",
    "\n",
    "N_CLASSES = 5\n",
    "FEATURE_COLUMN = \"review_body\"\n",
    "\n",
    "if DEBUG:\n",
    "    MIN_DF = 1\n",
    "    DF_PERCENTAGE = 0.001\n",
    "    DATA_FILE = \"/home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-test1k-preprocessed.csv\"\n",
    "    TEST_STR=\"-test\"\n",
    "else:\n",
    "    DF_PERCENTAGE = 0.001\n",
    "    DATA_FILE = \"/home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-500k-preprocessed.csv\"\n",
    "    TEST_STR=\"\"\n",
    "\n",
    "REPORT_FILE = f\"../../reports/201911-pyspark-report{TEST_STR}.csv\"\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .appName(\"Pyspark Wrapper Test (local)\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Timer(object):\n",
    "    \n",
    "    def __init__(self, description: str):\n",
    "        self.start_time = datetime.now()\n",
    "        self.description = description\n",
    "        \n",
    "    def stop(self):\n",
    "        self.end_time = datetime.now()\n",
    "        self.print_duration_min()\n",
    "        \n",
    "    def print_duration_min(self):\n",
    "        self.duration = int((self.end_time - self.start_time).total_seconds() / 60)\n",
    "        print(f\"{self.description} duration: {self.duration} minutes\")\n",
    "        \n",
    "    def get_duraction_min(self):\n",
    "        return self.duration\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file load time duration: 0 minutes\n"
     ]
    }
   ],
   "source": [
    "file_timer = Timer(\"file load time\")\n",
    "df = spark.read.csv(SparkFiles.get(DATA_FILE), \n",
    "                    header=True, \n",
    "                    inferSchema= True)\n",
    "df.collect()\n",
    "file_timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Creating TFIDF using min_df: 995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline time duration: 1 minutes\n"
     ]
    }
   ],
   "source": [
    "def build_ngrams(inputCol, min_df, n=3):\n",
    "    log.info(f'Creating TFIDF using min_df: {min_df}')\n",
    "    \n",
    "    tokenizer = [Tokenizer(inputCol = inputCol, outputCol = \"words\")]\n",
    "    \n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    vectorizers = [\n",
    "        CountVectorizer(minDF=min_df, inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_counts\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_counts\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"raw_features\"\n",
    "    )]\n",
    "    \n",
    "    idf = [IDF().setInputCol(\"raw_features\").setOutputCol(\"features\")]\n",
    "#     idf = [IDF(minDocFreq=min_df).setInputCol(\"raw_features\").setOutputCol(\"features\")]\n",
    "\n",
    "    return Pipeline(stages=tokenizer + ngrams + vectorizers + assembler + idf)\n",
    "\n",
    "\n",
    "\n",
    "pipeline_timer = Timer(\"pipeline time\")\n",
    "# calculate a reasonable min_df\n",
    "min_df = max(int(df.count() * DF_PERCENTAGE), 1)\n",
    "\n",
    "pipeline = build_ngrams(FEATURE_COLUMN, min_df)\n",
    "df = pipeline.fit(df).transform(df)\n",
    "pipeline_timer.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: timestamp (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- 1_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 2_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 3_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 1_counts: vector (nullable = true)\n",
      " |-- 2_counts: vector (nullable = true)\n",
      " |-- 3_counts: vector (nullable = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|star_rating|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|5          |(4073,[5,8,70,99,103,113,162,289,429,449,580,1803,2286,2408],[5.315500659888828,1.7774729777628142,3.03932150642275,3.2642185507339105,3.278854055647774,3.4155707049560187,3.6388212432866918,4.278186825487674,4.595464394558363,4.758088643261192,4.922847369701602,6.511392872052722,5.030095503608268,5.523661815699121])                                                                                                                                                                                                                                                                          |\n",
      "|5          |(4073,[32,592],[2.556090353737218,5.077274063883358])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|5          |(4073,[0,1,2,6,19,55,68,72,77,79,83,92,106,177,336,388,403,495,525,616,2196,2345,2356,2593,2647,2672],[1.7588176835033948,1.1918562879817414,1.3836033685985332,1.6796364190539914,2.139263022368317,2.9326526005270694,3.053627841608528,3.0661355005965367,3.120063842622092,3.0591556064871197,3.1859194591784776,3.218136911298508,3.2873681633046266,3.7065184863149723,4.322460841294512,4.522223546379511,4.748538139734304,4.725279945212877,4.812188372699148,5.01213186026443,3.9966978478463173,5.3199304294211505,5.387428992187198,5.932277326513751,6.022978680963807,6.0541390967788695])|\n",
      "|5          |(4073,[1,2,13,79,129,203,294,457,1636,2201,2208,3877],[1.1918562879817414,1.3836033685985332,1.9216519176395193,3.0591556064871197,3.4124578144757876,3.824327077353704,4.148628407384763,4.673958338655339,6.332455413243008,4.145325558853799,4.250474725044496,6.026717003074414])                                                                                                                                                                                                                                                                                                                   |\n",
      "|3          |(4073,[31,95],[2.5613192764677057,3.2496128975302123])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|1          |(4073,[0,16,19,24,40,121,314,409,632,741,2180,2198,3858],[0.8794088417516974,2.1135876602169987,2.139263022368317,2.3259443429473974,2.7055415219780867,3.412275004688861,4.243245355078036,4.54041349803087,5.0331724291127475,5.216480604966818,2.6179192255357084,4.1058487934156815,5.755715097053608])                                                                                                                                                                                                                                                                                             |\n",
      "|3          |(4073,[0,2,6,13,19,24,36,113,116,142,210,240,482,602,1866,1943,2180,2187,2375,3827,3856],[0.8794088417516974,1.3836033685985332,3.359272838107983,1.9216519176395193,2.139263022368317,4.651888685894795,2.9514991730894335,3.4155707049560187,3.3779277403503536,11.429822905476778,4.0522083623844845,3.9082528375802363,4.680542635743949,5.034868782360926,6.591548198680148,13.39121622525286,2.6179192255357084,3.562660137065779,5.430962902467803,4.578305902399806,5.743414043031993])                                                                                                         |\n",
      "|5          |(4073,[0,12,24,87,131,140,151,179,328,1248,2177,2180,2349,2538],[1.7588176835033948,4.072878231868202,2.3259443429473974,3.3418776442901597,3.4769597709823223,3.525267407281282,3.654961498521602,3.715925970710045,4.299930791350924,5.945234824877382,2.3827339304671047,2.6179192255357084,5.358428907554035,5.835281878645345])                                                                                                                                                                                                                                                                    |\n",
      "|5          |(4073,[20,76,169,357,1123,2578],[2.075481287342001,3.1560036187651943,3.597941168680095,4.340102293123124,5.769777329417836,5.8889291804576365])                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|5          |(4073,[4,8,27,31,2191,2222,3027,3927],[1.5935942428460665,1.7774729777628142,2.353555521091441,2.5613192764677057,3.8132110231820375,4.432542721426325,6.4290658730733705,6.353580949095277])                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "pyu.show_df(df, [\"star_rating\", \"features\"], truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test split duration: 0 minutes\n",
      "Training size: 896004 Test size: 99684\n"
     ]
    }
   ],
   "source": [
    "split_timer = Timer(\"train test split\")\n",
    "train, test = df.randomSplit([0.9, 0.1], seed=1)\n",
    "split_timer.stop()\n",
    "\n",
    "\n",
    "train_size = train.count()\n",
    "test_size = test.count()\n",
    "\n",
    "print(f'Training size: {train_size} Test size: {test_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4073"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train.select(\"features\").limit(1).toPandas().features.values[0])\n",
    "train.select(\"features\").limit(1).toPandas().features.values[0].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign class weights to handle imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn class weights: [1.42386715 3.02163019 2.21205515 1.20022504 0.37292633]\n",
      "skelarn class weight duration: 1 minutes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# only do this for small files - takes too long for large datasets - we will custom compute this\n",
    "# if DEBUG:\n",
    "cw_timer = Timer(\"skelarn class weight\")\n",
    "labels = train.select(\"star_rating\").toPandas().astype({\"star_rating\": np.int8})\n",
    "class_weights = compute_class_weight('balanced', \n",
    "                                                  np.arange(1, N_CLASSES+1), \n",
    "                                                  labels.star_rating.tolist())\n",
    "print(f'sklearn class weights: {class_weights}')\n",
    "cw_timer.stop()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+--------------------+\n",
      "|star_rating|     class_weights|            features|\n",
      "+-----------+------------------+--------------------+\n",
      "|          1|1.4238671487028725|(4073,[1,45,61,72...|\n",
      "|          1|1.4238671487028725|(4073,[0,6,7,24,5...|\n",
      "|          1|1.4238671487028725|(4073,[0,1,2,4,13...|\n",
      "|          1|1.4238671487028725|(4073,[1,4,5,43,7...|\n",
      "|          1|1.4238671487028725|(4073,[10,12,35,4...|\n",
      "|          1|1.4238671487028725|(4073,[0,1,4,15,2...|\n",
      "|          1|1.4238671487028725|(4073,[0,1,2,13,3...|\n",
      "|          1|1.4238671487028725|(4073,[43,61,108,...|\n",
      "|          1|1.4238671487028725|(4073,[0,6,9,15,1...|\n",
      "|          1|1.4238671487028725|(4073,[93,114,799...|\n",
      "|          1|1.4238671487028725|(4073,[5,6,8,13,1...|\n",
      "|          1|1.4238671487028725|(4073,[0,1,14,26,...|\n",
      "|          1|1.4238671487028725|(4073,[4,6,8,10,1...|\n",
      "|          1|1.4238671487028725|(4073,[0,3,7,10,2...|\n",
      "|          1|1.4238671487028725|(4073,[0,1,2,19,2...|\n",
      "|          1|1.4238671487028725|(4073,[0,2,273,44...|\n",
      "|          1|1.4238671487028725|(4073,[0,6,11,24,...|\n",
      "|          1|1.4238671487028725|(4073,[21,26,82,1...|\n",
      "|          1|1.4238671487028725|(4073,[0,1,5,6,15...|\n",
      "|          1|1.4238671487028725|(4073,[0,1,14,40,...|\n",
      "+-----------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = train.withColumn(\"class_weights\", lit(0))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 1, class_weights[0]).otherwise(train.class_weights))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 2, class_weights[1]).otherwise(train.class_weights))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 3, class_weights[2]).otherwise(train.class_weights))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 4, class_weights[3]).otherwise(train.class_weights))\n",
    "train = train.withColumn(\"class_weights\", when(train.star_rating == 5, class_weights[4]).otherwise(train.class_weights))\n",
    "\n",
    "\n",
    "pyu.show_df(train, [\"star_rating\", \"class_weights\", \"features\"], 20, sample=True, truncate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:util.model_wrapper:derived name: LogisticRegression\n",
      "INFO:util.model_wrapper:########################################\n",
      "INFO:util.model_wrapper:Running model: name: LogisticRegression\n",
      "\twith file: /home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-1m-preprocessed.csv\n",
      "\twith description: review_body-tfidf-df_none-ngram13-995688-4073-nolda-sampling_none-LogisticRegression-star_rating\n",
      "\tstatus: new\n",
      "INFO:util.model_wrapper:########################################\n",
      "INFO:util.time_util:Start timer for: train_time_min\n",
      "INFO:util.time_util:End timer for: train_time_min\n",
      "INFO:util.time_util:Total time for train_time_min: 1.82\n",
      "INFO:util.time_util:Start timer for: model_save_time_min\n",
      "INFO:util.pyspark_util:Saving model to file: ../../models/review_body-tfidf-df_none-ngram13-995688-4073-nolda-sampling_none-LogisticRegression-star_rating.pyspark\n",
      "INFO:util.pyspark_util:Saving pipeline to file: ../../models/review_body-tfidf-df_none-ngram13-995688-4073-nolda-sampling_none-LogisticRegression-star_rating.pipeline\n",
      "INFO:util.time_util:End timer for: model_save_time_min\n",
      "INFO:util.time_util:Total time for model_save_time_min: 0.04\n",
      "INFO:util.time_util:Start timer for: predict_time_min\n",
      "INFO:util.time_util:End timer for: predict_time_min\n",
      "INFO:util.time_util:Total time for predict_time_min: 0.0\n",
      "INFO:util.model_wrapper:Finished running model: name: LogisticRegression\n",
      "\twith file: /home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-1m-preprocessed.csv\n",
      "\twith description: review_body-tfidf-df_none-ngram13-995688-4073-nolda-sampling_none-LogisticRegression-star_rating\n",
      "\tstatus: new\n",
      "INFO:util.model_wrapper:calculating classification report...\n",
      "INFO:util.time_util:Start timer for: cr_time_min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: timestamp (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- 1_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 2_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 3_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 1_counts: vector (nullable = true)\n",
      " |-- 2_counts: vector (nullable = true)\n",
      " |-- 3_counts: vector (nullable = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:util.time_util:End timer for: cr_time_min\n",
      "INFO:util.time_util:Total time for cr_time_min: 2.71\n",
      "INFO:util.model_wrapper:calculating confusion matrix...\n",
      "INFO:util.time_util:Start timer for: cm_time_min\n",
      "INFO:util.time_util:End timer for: cm_time_min\n",
      "INFO:util.time_util:Total time for cm_time_min: 2.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traing time duration: 9 minutes\n",
      "+-----------+----------+--------------------+--------------------+\n",
      "|star_rating|prediction|       rawPrediction|         probability|\n",
      "+-----------+----------+--------------------+--------------------+\n",
      "|          1|       1.0|[-10.091957087099...|[5.84746444455491...|\n",
      "|          1|       2.0|[-10.083085280747...|[1.09334159549751...|\n",
      "|          1|       1.0|[-10.090517278128...|[1.98889104851819...|\n",
      "|          1|       2.0|[-10.082841452073...|[1.04122582886483...|\n",
      "|          1|       1.0|[-10.080770708826...|[1.11457771854265...|\n",
      "|          1|       1.0|[-10.085338365458...|[4.89190950577153...|\n",
      "|          1|       1.0|[-10.086358743623...|[2.57308332421388...|\n",
      "|          1|       1.0|[-10.083879619971...|[3.50538647464572...|\n",
      "|          1|       1.0|[-10.083623954277...|[2.60633226101958...|\n",
      "|          1|       4.0|[-10.082998265473...|[1.03107807974704...|\n",
      "+-----------+----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import importlib\n",
    "import util.constants as c\n",
    "importlib.reload(pyu)\n",
    "importlib.reload(mw)\n",
    "importlib.reload(c)\n",
    "\n",
    "feature_count = train.select(\"features\").limit(1).toPandas().features.values[0].size\n",
    "\n",
    "train_timer = Timer(\"traing time\")\n",
    "lr = LogisticRegression(labelCol=\"star_rating\", \n",
    "                        featuresCol=\"features\", \n",
    "                        weightCol=\"class_weights\",\n",
    "                        maxIter=100)\n",
    "\n",
    "\n",
    "model = pyu.PysparkModel(model = lr,\n",
    "                    train_df = train,\n",
    "                    test_df = test,\n",
    "                    label_column = \"star_rating\",\n",
    "                    feature_column = \"features\",\n",
    "                    n_classes = 5,\n",
    "                         pipeline = pipeline,\n",
    "                         file = DATA_FILE,\n",
    "                         description=f'review_body-tfidf-df_none-ngram13-{df.count()}-{feature_count}-nolda-sampling_none{TEST_STR}',\n",
    "                        model_dir=\"../../models\")\n",
    "\n",
    "report_dict, predict_test = model.run()\n",
    "train_timer.stop()\n",
    "\n",
    "pyu.show_df(predict_test, [\"star_rating\", \"prediction\", \"rawPrediction\", \"probability\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: timestamp (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- 1_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 2_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 3_grams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 1_counts: vector (nullable = true)\n",
      " |-- 2_counts: vector (nullable = true)\n",
      " |-- 3_counts: vector (nullable = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate our Model\n",
    "\n",
    "Reference:\n",
    "* https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html#multiclass-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'LogisticRegression',\n",
       " 'description': 'review_body-tfidf-df_none-ngram13-995688-4073-nolda-sampling_none-LogisticRegression-star_rating',\n",
       " 'library': 'pyspark',\n",
       " 'file': '/home/jupyter/dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-1m-preprocessed.csv',\n",
       " 'model_file': '../../models/review_body-tfidf-df_none-ngram13-995688-4073-nolda-sampling_none-LogisticRegression-star_rating.pyspark',\n",
       " 'pipeline_file': '../../models/review_body-tfidf-df_none-ngram13-995688-4073-nolda-sampling_none-LogisticRegression-star_rating.pipeline',\n",
       " 'status': 'success',\n",
       " 'status_date': '2019-11-21 18:53:49',\n",
       " 'classification_report': '{\"1\": {\"precision\": 0.7026024905554779, \"recall\": 0.7121684867394695, \"f1-score\": 0.7073531483307508, \"support\": 14102}, \"2\": {\"precision\": 0.2902851843264549, \"recall\": 0.3864793949683593, \"f1-score\": 0.3315458457464415, \"support\": 6479}, \"3\": {\"precision\": 0.3212204319506342, \"recall\": 0.41272987556436513, \"f1-score\": 0.3612704226709721, \"support\": 9081}, \"4\": {\"precision\": 0.4012675668228162, \"recall\": 0.43798123195380173, \"f1-score\": 0.4188213638586097, \"support\": 16624}, \"5\": {\"precision\": 0.8594705118101851, \"recall\": 0.7557024607663209, \"f1-score\": 0.8042531564838713, \"support\": 53398}, \"accuracy\": 0.6413165603306449, \"macro avg\": {\"precision\": 0.5149692370931136, \"recall\": 0.5410122899984633, \"f1-score\": 0.5246487874181291, \"support\": 99684}, \"weighted avg\": {\"precision\": 0.6748378796407741, \"recall\": 0.6413165603306449, \"f1-score\": 0.6551891250685951, \"support\": 99684}}',\n",
       " 'confusion_matrix': '[[10043, 2635, 928, 224, 272], [1991, 2504, 1399, 334, 251], [962, 1887, 3748, 1697, 787], [431, 742, 2882, 7281, 5288], [867, 858, 2711, 8609, 40353]]',\n",
       " 'train_examples': 896004,\n",
       " 'train_features': 4073,\n",
       " 'test_examples': 99684,\n",
       " 'test_features': 4073,\n",
       " 'train_time_min': 1.82,\n",
       " 'total_time_min': 7.16,\n",
       " 'model_save_time_min': 0.04,\n",
       " 'predict_time_min': 0.0,\n",
       " 'cr_time_min': 2.71,\n",
       " 'cm_time_min': 2.59}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'f1-score': 0.7073531483307508,\n",
      "       'precision': 0.7026024905554779,\n",
      "       'recall': 0.7121684867394695,\n",
      "       'support': 14102},\n",
      " '2': {'f1-score': 0.3315458457464415,\n",
      "       'precision': 0.2902851843264549,\n",
      "       'recall': 0.3864793949683593,\n",
      "       'support': 6479},\n",
      " '3': {'f1-score': 0.3612704226709721,\n",
      "       'precision': 0.3212204319506342,\n",
      "       'recall': 0.41272987556436513,\n",
      "       'support': 9081},\n",
      " '4': {'f1-score': 0.4188213638586097,\n",
      "       'precision': 0.4012675668228162,\n",
      "       'recall': 0.43798123195380173,\n",
      "       'support': 16624},\n",
      " '5': {'f1-score': 0.8042531564838713,\n",
      "       'precision': 0.8594705118101851,\n",
      "       'recall': 0.7557024607663209,\n",
      "       'support': 53398},\n",
      " 'accuracy': 0.6413165603306449,\n",
      " 'macro avg': {'f1-score': 0.5246487874181291,\n",
      "               'precision': 0.5149692370931136,\n",
      "               'recall': 0.5410122899984633,\n",
      "               'support': 99684},\n",
      " 'weighted avg': {'f1-score': 0.6551891250685951,\n",
      "                  'precision': 0.6748378796407741,\n",
      "                  'recall': 0.6413165603306449,\n",
      "                  'support': 99684}}\n"
     ]
    }
   ],
   "source": [
    "pprint(json.loads(report_dict[\"classification_report\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10043  2635   928   224   272]\n",
      " [ 1991  2504  1399   334   251]\n",
      " [  962  1887  3748  1697   787]\n",
      " [  431   742  2882  7281  5288]\n",
      " [  867   858  2711  8609 40353]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(json.loads(report_dict[\"confusion_matrix\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall score: 0.5070364344705827\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def calculate_score(cr: dict):\n",
    "    \n",
    "    values = []\n",
    "    values.append(cr[\"1\"][\"recall\"])\n",
    "    values.append(cr[\"2\"][\"recall\"])\n",
    "    values.append(cr[\"3\"][\"recall\"])\n",
    "    values.append(cr[\"4\"][\"recall\"])\n",
    "    values.append(cr[\"5\"][\"precision\"])\n",
    "    \n",
    "    mean = 0\n",
    "    for v in values:\n",
    "        if v == 0:\n",
    "            mean = 0\n",
    "            break\n",
    "        else:\n",
    "            mean += 1 / v\n",
    "    if mean > 0:\n",
    "        mean = len(values) / mean\n",
    "\n",
    "    return mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "score = calculate_score(json.loads(report_dict['classification_report']))\n",
    "print(f'Overall score: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification_report</th>\n",
       "      <th>cm_time_min</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>cr_time_min</th>\n",
       "      <th>description</th>\n",
       "      <th>file</th>\n",
       "      <th>library</th>\n",
       "      <th>model_file</th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_save_time_min</th>\n",
       "      <th>pipeline_file</th>\n",
       "      <th>predict_time_min</th>\n",
       "      <th>status</th>\n",
       "      <th>status_date</th>\n",
       "      <th>test_examples</th>\n",
       "      <th>test_features</th>\n",
       "      <th>total_time_min</th>\n",
       "      <th>train_examples</th>\n",
       "      <th>train_features</th>\n",
       "      <th>train_time_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"1\": {\"precision\": 0.7026024905554779, \"recal...</td>\n",
       "      <td>2.59</td>\n",
       "      <td>[[10043, 2635, 928, 224, 272], [1991, 2504, 13...</td>\n",
       "      <td>2.71</td>\n",
       "      <td>review_body-tfidf-df_none-ngram13-995688-4073-...</td>\n",
       "      <td>/home/jupyter/dataset/amazon_reviews/amazon_re...</td>\n",
       "      <td>pyspark</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.04</td>\n",
       "      <td>../../models/review_body-tfidf-df_none-ngram13...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>success</td>\n",
       "      <td>2019-11-21 18:53:49</td>\n",
       "      <td>99684.0</td>\n",
       "      <td>4073.0</td>\n",
       "      <td>7.16</td>\n",
       "      <td>896004.0</td>\n",
       "      <td>4073.0</td>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               classification_report  cm_time_min  \\\n",
       "0  {\"1\": {\"precision\": 0.7026024905554779, \"recal...         2.59   \n",
       "\n",
       "                                    confusion_matrix  cr_time_min  \\\n",
       "0  [[10043, 2635, 928, 224, 272], [1991, 2504, 13...         2.71   \n",
       "\n",
       "                                         description  \\\n",
       "0  review_body-tfidf-df_none-ngram13-995688-4073-...   \n",
       "\n",
       "                                                file  library  \\\n",
       "0  /home/jupyter/dataset/amazon_reviews/amazon_re...  pyspark   \n",
       "\n",
       "                                          model_file          model_name  \\\n",
       "0  ../../models/review_body-tfidf-df_none-ngram13...  LogisticRegression   \n",
       "\n",
       "   model_save_time_min                                      pipeline_file  \\\n",
       "0                 0.04  ../../models/review_body-tfidf-df_none-ngram13...   \n",
       "\n",
       "   predict_time_min   status          status_date  test_examples  \\\n",
       "0               0.0  success  2019-11-21 18:53:49        99684.0   \n",
       "\n",
       "   test_features  total_time_min  train_examples  train_features  \\\n",
       "0         4073.0            7.16        896004.0          4073.0   \n",
       "\n",
       "   train_time_min  \n",
       "0            1.82  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(REPORT_FILE):\n",
    "    report_df = pd.read_csv(REPORT_FILE)\n",
    "else:\n",
    "    report_df = pd.DataFrame()\n",
    "report_df = report_df.append(report_dict, ignore_index=True)\n",
    "report_df.to_csv(REPORT_FILE, index=False)\n",
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
