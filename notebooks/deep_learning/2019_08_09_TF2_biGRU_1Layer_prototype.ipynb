{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2019-08-09-TF2-biGRU_1Layer_prototype.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sv650s/sb-capstone/blob/master/2019_08_09_TF2_biGRU_1Layer_prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYIgDlPPzDPh",
        "colab_type": "text"
      },
      "source": [
        "# 1 Layer biDirectional GRU with Attention using TensorFlow 2\n",
        "\n",
        "In our [Deep Learning Summary](https://github.com/sv650s/sb-capstone/blob/master/2019_07_30_deep_learning_summary.ipynb) notebook, we determined that biDirectional GRU with 1 Layer with Attention gave us the best classification results based on our Amazon Reviews dataset. In this [notebook](https://github.com/sv650s/sb-capstone/blob/master/2019_08_02_biGRU_1layer_random_embedding_vs_pretrained.ipynb), we also determined that using this model we also determined that using random embedding actually give us better results compared to using pre-trained embeddings.\n",
        "\n",
        "Our previous implementation was implemented in Keras\n",
        "\n",
        "We will re-implement this using Keras on Tensor Flow 2.0 beta so that we can package up the model and deploy this on GCP. The following were updated in the code:\n",
        "\n",
        "* sklearn's OneHotEncoder -> tf.one_hot\n",
        "* keras Tokenizer -> tf Keras Tokenizer\n",
        "* keras sequence -> tf Keras sequence\n",
        "* all layers that previously used keras reference impl now use tf's keras implementation\n",
        "* previously, we used Keras' Sequential layer - I had trouble saving the model when using this, so I replaced it with the Keras functional API to create the model\n",
        "\n",
        "Since we will loading models and then using them do inference as a REST API, we will save our model and then reload them and do some quick predictions to make sure that we can re-create this networking on GCP\n",
        "\n",
        "\n",
        "TODO:\n",
        "\n",
        "* TF recommends that we use the Dataset api for large datasets. This notebook does not use that yet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox3fMqDHy7JW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_k5CJnDzleX",
        "colab_type": "code",
        "outputId": "e0b330ec-2ee5-4457-fe78-71e8e7091124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_DIR = \"drive/My Drive/Springboard/capstone\"\n",
        "\n",
        "# add this to sys patch so we can import utility functions\n",
        "sys.path.append(DRIVE_DIR)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmKgMFvAzleF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q tensorflow==2.0.0-beta1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrYGJW_V2kkN",
        "colab_type": "code",
        "outputId": "e0e8f08d-2ee1-4664-9cea-c27e29b1f367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x  # Colab only.\n",
        "except Exception:\n",
        "  pass\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
            "You set: `2.x  # Colab only.`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_udVOUia9Oo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cejIhV8G2TA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, GRU, Dropout, Bidirectional, Embedding\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set()\n",
        "\n",
        "# import our utility functions\n",
        "import util.keras_util as ku\n",
        "import util.dict_util as du\n",
        "import util.plot_util as pu\n",
        "\n",
        "\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# check to see if we are using GPU - must be placed at the beginning of the program\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
        "\n",
        "print(f'Tensorflow Version: {tf.version.VERSION}')\n",
        "print(f'Keras Version: {tf.keras.__version__}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEWsQB-v4rVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATE_FORMAT = '%Y-%m-%d'\n",
        "TIME_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
        "DATA_FILE = f\"{DRIVE_DIR}/data/amazon_reviews_us_Wireless_v1_00-preprocessed-110k.csv\"\n",
        "LABEL_COLUMN = \"star_rating\"\n",
        "FEATURE_COLUMN = \"review_body\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mQJffiK5PAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(DATA_FILE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7JNNyDp9nDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# X_train, X_test, y_train, y_test, tokenizer, max_sequence_length = \\\n",
        "#                                   ku.preprocess_file(data_df=df, \n",
        "#                                                       feature_column=FEATURE_COLUMN, \n",
        "#                                                       label_column=LABEL_COLUMN, \n",
        "#                                                       keep_percentile=0.99)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X88WVRLDcTb9",
        "colab_type": "text"
      },
      "source": [
        "# Pre-process our data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ly-nDtgcLmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "labels = df[LABEL_COLUMN]\n",
        "features = df[FEATURE_COLUMN]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ju1XkdBcLps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# couldn't find a TF versio of this\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "features_train, features_test, y_train, y_test = train_test_split(features, labels, random_state=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pt0AJ1ucLta",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "b3fb3d82-8052-4c9c-ce13-4fc33eb095da"
      },
      "source": [
        "# TF expects the indeces to be actual indexes - since our star ratings starts at 1, \n",
        "# should subtract 1 from it to create 0-based index\n",
        "y_train_encoded = tf.one_hot(y_train.apply(lambda x: x-1).tolist(), depth=5, axis=-1)\n",
        "y_test_encoded = tf.one_hot(y_test.apply(lambda x: x-1).tolist(), depth=5, axis=-1)\n",
        "print(y_train[:5])\n",
        "print(type(y_train_encoded))\n",
        "print(y_train_encoded.shape)\n",
        "y_train_encoded[:5]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op OneHot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "7993     4\n",
            "2317     5\n",
            "50296    5\n",
            "31341    5\n",
            "28373    5\n",
            "Name: star_rating, dtype: int64\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "(84032, 5)\n",
            "Executing op StridedSlice in device /job:localhost/replica:0/task:0/device:GPU:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=13, shape=(5, 5), dtype=float32, numpy=\n",
              "array([[0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhkrajWPfnxV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6c4b1776-c63a-4b35-9c0f-b7066c1d82ad"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(lower=True, oov_token=\"<UNK>\")\n",
        "tokenizer.fit_on_texts(features_train)\n",
        "\n",
        "x_train_sequences = tokenizer.texts_to_sequences(features_train)\n",
        "x_test_sequences = tokenizer.texts_to_sequences(features_test)\n",
        "\n",
        "\n",
        "vocab_size = len(tokenizer.word_index)\n",
        "                 \n",
        "print(f'Vocabulary size={vocab_size}')\n",
        "print(f'Number of Documents={tokenizer.document_count}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size=40789\n",
            "Number of Documents=84032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX7yeluwgXSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "a276b56b-4f15-40f3-deb6-dce1344dd29a"
      },
      "source": [
        "import tensorflow.keras.preprocessing.sequence\n",
        "\n",
        "\n",
        "# from our previous notebook, we saw that 99% of our reviews have 184 or less\n",
        "# words - we will use 200 as a round number\n",
        "MAX_FEATURES = 200\n",
        "\n",
        "X_train = sequence.pad_sequences(x_train_sequences, \n",
        "                                 maxlen=MAX_FEATURES,\n",
        "                                padding='post',\n",
        "                                truncating='post')\n",
        "X_test = sequence.pad_sequences(x_test_sequences, \n",
        "                                 maxlen=MAX_FEATURES,\n",
        "                                padding='post',\n",
        "                                truncating='post')\n",
        "\n",
        "X_test[:1]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  10,  609, 2585,  671,  438,  247,    3,  324,    6,   32,  204,\n",
              "         341, 1415,  644,  343,  148,  243,  273,   55,  129,  279,  909,\n",
              "          28,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PclL9RTPjx_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys_q5xFAcL10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBZS5MQEjzE2",
        "colab_type": "text"
      },
      "source": [
        "# Implement our Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvBAES8hMXZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "# define our attention layer for later\n",
        "class AttentionLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "\n",
        "        \"\"\"\n",
        "        Keras Layer that implements an Attention mechanism for temporal data.\n",
        "        Supports Masking.\n",
        "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
        "        # Input shape\n",
        "            3D tensor with shape: `(samples, steps, features)`.\n",
        "        # Output shape\n",
        "            2D tensor with shape: `(samples, features)`.\n",
        "        :param kwargs:\n",
        "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "        The dimensions are inferred based on the output shape of the RNN.\n",
        "        \"\"\"\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.init = keras.initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = keras.regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = keras.regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = keras.constraints.get(W_constraint)\n",
        "        self.b_constraint = keras.constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        \n",
        "        print(f'build.input_shape {input_shape}')\n",
        "        print(f'build.input_shape[-1] {input_shape[-1]}')\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "        \n",
        "        print(f'build.features_dim {self.features_dim}')\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape=(input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        # TF backend doesn't support it\n",
        "        # eij = K.dot(x, self.W)\n",
        "        # features_dim = self.W.shape[0]\n",
        "        # step_dim = x._keras_shape[1]\n",
        "\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
        "                              K.reshape(self.W, (features_dim, 1))),\n",
        "                        (-1, step_dim))\n",
        "        \n",
        "        print(f'call.eij {eij}')\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        \n",
        "        print(f'call.weighted_input {weighted_input}')\n",
        "\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], self.features_dim\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'step_dim': self.step_dim}\n",
        "        base_config = super(AttentionLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6J5GAr9BDZV",
        "colab_type": "code",
        "outputId": "b9dba7c8-6a4c-4bc9-db21-0be8841e8012",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "\n",
        "from tensorflow.keras.layers import GRU\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "MODEL_NAME = \"TF2-biGRU_1layer_attention\"\n",
        "EMBED_SIZE = 300\n",
        "EPOCHS  = 50\n",
        "BATCH_SIZE = 128\n",
        "VOCAB_SIZE = len(tokenizer.word_index)\n",
        "GRU_DIM = 250 # total GRU units\n",
        "\n",
        "\n",
        "# use functional syntax to create our graph\n",
        "inp = layers.Input(shape=(MAX_FEATURES, ))\n",
        "x = Embedding(VOCAB_SIZE + 1, EMBED_SIZE, trainable=True)(inp)\n",
        "x = Bidirectional(GRU(units=GRU_DIM*2, return_sequences=True))(x)\n",
        "x = AttentionLayer(MAX_FEATURES)(x)\n",
        "x = Dense(GRU_DIM*2, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(GRU_DIM, activation='relu')(x)\n",
        "outp = Dense(5, activation='softmax')(x)\n",
        "\n",
        "model = keras.models.Model(inputs=inp, outputs=outp)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer=keras.optimizers.Adam(), \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op Sub in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op Mul in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op Add in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op Assert in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Qr in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op DiagPart in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Sign in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Transpose in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "build.input_shape (None, 200, 1000)\n",
            "build.input_shape[-1] 1000\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "build.features_dim 1000\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "call.eij Tensor(\"attention_layer/Reshape_2:0\", shape=(None, 200), dtype=float32)\n",
            "call.weighted_input Tensor(\"attention_layer/mul:0\", shape=(None, 200, 1000), dtype=float32)\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QL-kgLzIiL3",
        "colab_type": "code",
        "outputId": "85c58868-8c50-4438-e6cf-b555039c5878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 200)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 200, 300)          12237000  \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 200, 1000)         2406000   \n",
            "_________________________________________________________________\n",
            "attention_layer (AttentionLa (None, 1000)              1200      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 500)               500500    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 250)               125250    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 1255      \n",
            "=================================================================\n",
            "Total params: 15,271,205\n",
            "Trainable params: 15,271,205\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOr3tNRuJm9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYB-5-t1AFBF",
        "colab_type": "code",
        "outputId": "3eb3d0d5-229c-48b8-ef53-5c9f69c65983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# mw = ku.ModelWrapper(model, \n",
        "#                      MODEL_NAME,\n",
        "#                      LABEL_COLUMN,\n",
        "#                      DATA_FILE,\n",
        "#                      embedding=EMBED_SIZE,\n",
        "#                      tokenizer=tokenizer,\n",
        "#                      description=\"TF2 1 layer biDirectional GRU\")\n",
        "\n",
        "history = model.fit(X_train, \n",
        "       y_train_encoded,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        verbose=1,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[early_stop])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op StridedSlice in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op StridedSlice in device /job:localhost/replica:0/task:0/device:GPU:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0813 06:48:08.758161 140396788017024 deprecation.py:323] From /tensorflow-2.0.0b1/python3.6/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Train on 67225 samples, validate on 16807 samples\n",
            "Epoch 1/50\n",
            "Executing op __inference_keras_scratch_graph_4048 in device <unspecified>\n",
            "67200/67225 [============================>.] - ETA: 0s - loss: 0.9873 - accuracy: 0.6267Executing op __inference_keras_scratch_graph_4048 in device <unspecified>\n",
            "Executing op __inference_keras_scratch_graph_7956 in device <unspecified>\n",
            "Executing op __inference_keras_scratch_graph_7956 in device <unspecified>\n",
            "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "67225/67225 [==============================] - 172s 3ms/sample - loss: 0.9872 - accuracy: 0.6267 - val_loss: 0.8720 - val_accuracy: 0.6594\n",
            "Epoch 2/50\n",
            "67225/67225 [==============================] - 168s 2ms/sample - loss: 0.7898 - accuracy: 0.6922 - val_loss: 0.8507 - val_accuracy: 0.6729\n",
            "Epoch 3/50\n",
            "67225/67225 [==============================] - 168s 2ms/sample - loss: 0.7002 - accuracy: 0.7264 - val_loss: 0.8462 - val_accuracy: 0.6700\n",
            "Epoch 4/50\n",
            "67225/67225 [==============================] - 168s 2ms/sample - loss: 0.6116 - accuracy: 0.7635 - val_loss: 0.8991 - val_accuracy: 0.6617\n",
            "Epoch 5/50\n",
            "67200/67225 [============================>.] - ETA: 0s - loss: 0.5389 - accuracy: 0.7963Restoring model weights from the end of the best epoch.\n",
            "67225/67225 [==============================] - 169s 3ms/sample - loss: 0.5389 - accuracy: 0.7963 - val_loss: 1.0143 - val_accuracy: 0.6543\n",
            "Epoch 00005: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-7O8VuFJUUa",
        "colab_type": "code",
        "outputId": "8331c849-4d1e-4ecb-ecb2-8367af36eb5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "scores = model.evaluate(X_test, y_test_encoded)\n",
        "print(f'Model Accuracy: {scores[1]}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op __inference_keras_scratch_graph_7956 in device <unspecified>\n",
            "28000/28011 [============================>.] - ETA: 0s - loss: 0.8362 - accuracy: 0.6764Executing op __inference_keras_scratch_graph_7956 in device <unspecified>\n",
            "28011/28011 [==============================] - 34s 1ms/sample - loss: 0.8363 - accuracy: 0.6764\n",
            "Model Accuracy: 0.67641282081604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXH44TeCplCv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0ed8c1e3-2c6f-4d37-ca5d-67ca46b28484"
      },
      "source": [
        "# make some predictions so we can test our model later when we load it back in\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "y_predicted = np.argmax(model.predict(X_test[:20]), axis=1)\n",
        "y_predicted"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op __inference_keras_scratch_graph_32250 in device <unspecified>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 4, 4, 0, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 3, 4, 0, 4, 4, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45HR9tBDmFx2",
        "colab_type": "text"
      },
      "source": [
        "# Save off files so we can load later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQN_7QjbWQjG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "777c193c-56e5-4164-b983-83266f211f04"
      },
      "source": [
        "# save the model file to load late\n",
        "MODEL_FILE = f'{DRIVE_DIR}/models/amazon_reviews_us_Wireless_v1_00-preprocessed-110k-TF2-biGRU_1layer_attention-186-star_rating-model.h5'\n",
        "model.save(MODEL_FILE)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--q1YIOLWZO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the weights so we can reload this\n",
        "WEIGHTS_FILE = f'{DRIVE_DIR}/models/amazon_reviews_us_Wireless_v1_00-preprocessed-110k-TF2-biGRU_1layer_attention-186-star_rating-weights.h5'\n",
        "model.save_weights(WEIGHTS_FILE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc4UvX4GWunU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save to json so we don't have to re-create the architecture\n",
        "MODEL_JSON_FILE = f'{DRIVE_DIR}/models/amazon_reviews_us_Wireless_v1_00-preprocessed-110k-TF2-biGRU_1layer_attention-186-star_rating-model_json.h5'\n",
        "model_json = model.to_json()\n",
        "with open(MODEL_JSON_FILE, 'w') as json_file:\n",
        "  json_file.write(model_json)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8jlCdo0wTBv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "# save off tokenizer file\n",
        "TOKENIZER_FILE = f'{DRIVE_DIR}/models/tf2-tokenizer.pkl'\n",
        "pickle.dump(tokenizer, open(TOKENIZER_FILE, 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUPeKU4XaqZy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "00bdb086-60d3-45b6-8b5e-3625f72e3f01"
      },
      "source": [
        "# save network history file\n",
        "HISTORY_FILE = f'{DRIVE_DIR}/models/amazon_reviews_us_Wireless_v1_00-preprocessed-110k-TF2-biGRU_1layer_attention-186-star_rating-history.pkl'\n",
        "pickle.dump(history, open(HISTORY_FILE, 'wb'))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-da1f6a2022b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mHISTORY_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{DRIVE_DIR}/models/amazon_reviews_us_Wireless_v1_00-preprocessed-110k-TF2-biGRU_1layer_attention-186-star_rating-history.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHISTORY_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: can't pickle _thread.RLock objects"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_38WITtSoq08",
        "colab_type": "text"
      },
      "source": [
        "# Loading the models\n",
        "\n",
        "We are going to load the models and re-create our networks in a couple different ways\n",
        "\n",
        "After that, we are going to run the same prediction as before. Results should be\n",
        "\n",
        "```\n",
        "array([4, 3, 4, 2, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 3, 4, 0, 4, 4, 3])\n",
        "```\n",
        "\n",
        "## First - let's load the model with weights and then run the same predictions as before to see if we get she same values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv3V5lf7olEh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3a118004-d32a-41fa-f8db-ba3fe5c85350"
      },
      "source": [
        "model_loaded = keras.models.load_model(MODEL_FILE, custom_objects={'AttentionLayer': AttentionLayer})"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "build.input_shape (None, 200, 1000)\n",
            "build.input_shape[-1] 1000\n",
            "build.features_dim 1000\n",
            "call.eij Tensor(\"attention_layer_1/Reshape_2:0\", shape=(None, 200), dtype=float32)\n",
            "call.weighted_input Tensor(\"attention_layer_1/mul:0\", shape=(None, 200, 1000), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awfSl71CpGgt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dd49985f-2e12-46a0-8996-edeb788a9a4f"
      },
      "source": [
        "np.argmax(model_loaded.predict(X_test[:20]), axis=1)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op __inference_keras_scratch_graph_37273 in device <unspecified>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 4, 4, 0, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 3, 4, 0, 4, 4, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4srK7soyqiYs",
        "colab_type": "text"
      },
      "source": [
        "## Second - let's load it using our json config and weights\n",
        "\n",
        "we will run the same tests as before and see how this looks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyouwRGSqcBM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6a50a188-aad2-4b08-8048-afc4c07cce73"
      },
      "source": [
        "import json\n",
        "\n",
        "with open(MODEL_JSON_FILE) as json_file:\n",
        "  json_config = json_file.read()\n",
        "model_from_json = keras.models.model_from_json(json_config, custom_objects={'AttentionLayer': AttentionLayer})\n",
        "model_from_json.load_weights(WEIGHTS_FILE)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "build.input_shape (None, 200, 1000)\n",
            "build.input_shape[-1] 1000\n",
            "build.features_dim 1000\n",
            "call.eij Tensor(\"attention_layer_2/Reshape_2:0\", shape=(None, 200), dtype=float32)\n",
            "call.weighted_input Tensor(\"attention_layer_2/mul:0\", shape=(None, 200, 1000), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su__rpWlrClC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9ffe4a76-dad5-4329-f27e-676e9d34d025"
      },
      "source": [
        "np.argmax(model_from_json.predict(X_test[:20]), axis=1)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing op __inference_keras_scratch_graph_38131 in device <unspecified>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 4, 4, 0, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 3, 4, 0, 4, 4, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxSlB6uUr_2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}