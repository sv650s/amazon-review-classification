{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec and FastText Embedding as Features\n",
    "\n",
    "I'm actually not entirely sure if I understand how to generate features using word2vec and fasttext so I wanted to start this notebook and run this by you before spending time to train aon our feature set and training models\n",
    "\n",
    "For this notebook, I will only be working with 100 reviews so it runs fast\n",
    "\n",
    "Purpose is to show snippeets of code to make sure that I am on the right track\n",
    "\n",
    "\n",
    "Steps I used for both are:\n",
    "    * convert reviews to tokenized array\n",
    "    * train either word2vec or fasttext using these tokenized arrays\n",
    "    * get word vector for each word in review\n",
    "    * generate new feature matrix by averaging all word vectors into a review - this becomes the feature vector for the review\n",
    "    * run feature matrices through lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the raw text from amazon reviews\n",
    "\n",
    "This data has already been pre-proocessed by [amazon_review_preprocessor.py](amazon_review_preprocessor.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-csv-100-preprocessed.csv\"\n",
    "df = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_df = df[\"review_body\"]\n",
    "y_df = df[\"star_rating\"]\n",
    "sample_size = rb_df.shape[0]\n",
    "\n",
    "# this doesn't actually work - I get a parameter error when I try to use corpus_file parameter for word2vec\n",
    "# write out the review_body in LineSentence format - docs says there should be performance gains here\n",
    "review_body_file = \"dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-csv-100-preprocessed-review_body.csv\"\n",
    "rb_df.to_csv(review_body_file, header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordPunctTokenizer\n",
    "wpt = WordPunctTokenizer()\n",
    "documents = [wpt.tokenize(value) for index, value in rb_df.iteritems()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train word2vec to get word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "feature_size = 200    # Word vector dimensionality  \n",
    "window_context = 30          # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "iter = 50 # number of iterations over corpus\n",
    "\n",
    "w2v_model = Word2Vec(documents, size=feature_size, \n",
    "                          window=window_context, min_count=min_word_count,\n",
    "                          sample=sample, iter=iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save off model to be used later\n",
    "w2v_model.save(f\"models/word2vec-{sample_size}-{feature_size}.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inspect how many words are in our vocabulary\n",
    "\n",
    "Looks like there are about a thousand words in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(996, 200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good: (200,)\n",
      "product: (200,)\n",
      "please: (200,)\n",
      "note: (200,)\n",
      "not: (200,)\n",
      "floating: (200,)\n",
      "case: (200,)\n",
      "do: (200,)\n",
      "not: (200,)\n",
      "clai: (200,)\n",
      "somehow: (200,)\n",
      "thinking: (200,)\n",
      "good: (200,)\n",
      "price: (200,)\n",
      "does: (200,)\n",
      "says: (200,)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "first_review = rb_df.iloc[0]\n",
    "vector_list = []\n",
    "for word in wpt.tokenize(first_review):\n",
    "#     print(f'{word}: {model.wv.get_vector(word)}')\n",
    "    word_vector = w2v_model.wv.get_vector(word)\n",
    "    print(f'{word}: {word_vector.shape}')\n",
    "    vector_list.append(word_vector)\n",
    "\n",
    "review_feature = np.average(vector_list, axis=0)\n",
    "print(review_feature.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we come up with a vector that respresents each of the reviews by averaging word vectors for every word in the review body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_review_vector(model, review):\n",
    "    # returns a list of word vectors for all words im review\n",
    "    word_vectors = [model.wv.get_vector(word) for word in wpt.tokenize(review)]\n",
    "#     print(len(word_vectors))\n",
    "    # average all word vectors to come up with final vector for the review\n",
    "    return np.average(word_vectors, axis=0)\n",
    "\n",
    "# generate new feature DF\n",
    "def get_feature_df(model, df:pd.DataFrame) -> pd.DataFrame:\n",
    "    f_df = pd.DataFrame()\n",
    "    for index, review in df.iteritems():\n",
    "        feature_vector = get_review_vector(model, review)\n",
    "        # turn this into dictionary so we can add it as row to DF\n",
    "        feature_dict = dict(enumerate(feature_vector))\n",
    "        f_df = f_df.append(feature_dict, ignore_index=True)\n",
    "    return f_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate our feature matrix\n",
    "w2v_x_df = get_feature_df(w2v_model, rb_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we train a model and see how we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinceluk/anaconda3/envs/capstone/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_9.4.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def train_lightGBM(x_df, y_df):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_df, y_df)\n",
    "\n",
    "    gb = lgb.LGBMClassifier(objective=\"multiclass\", num_threads=2,\n",
    "                            seed=1)\n",
    "\n",
    "    gb.fit(x_train, y_train)\n",
    "    y_predict = gb.predict(x_test)\n",
    "\n",
    "    report = classification_report(y_test, y_predict, output_dict=True)\n",
    "    return report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinceluk/anaconda3/envs/capstone/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/vinceluk/anaconda3/envs/capstone/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 3},\n",
       " '2': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n",
       " '3': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 3},\n",
       " '4': {'precision': 0.3333333333333333,\n",
       "  'recall': 1.0,\n",
       "  'f1-score': 0.5,\n",
       "  'support': 1},\n",
       " '5': {'precision': 0.8421052631578947,\n",
       "  'recall': 0.8888888888888888,\n",
       "  'f1-score': 0.8648648648648649,\n",
       "  'support': 18},\n",
       " 'accuracy': 0.68,\n",
       " 'macro avg': {'precision': 0.2350877192982456,\n",
       "  'recall': 0.37777777777777777,\n",
       "  'f1-score': 0.27297297297297296,\n",
       "  'support': 25},\n",
       " 'weighted avg': {'precision': 0.6196491228070176,\n",
       "  'recall': 0.68,\n",
       "  'f1-score': 0.6427027027027027,\n",
       "  'support': 25}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lightGBM(w2v_x_df, y_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 200    # Word vector dimensionality  \n",
    "window_context = 30          # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "iter = 50 # number of iterations over corpus\n",
    "\n",
    "ft_model = FastText(documents, size=feature_size, \n",
    "                          window=window_context, min_count=min_word_count,\n",
    "                          sample=sample, iter=iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "996\n"
     ]
    }
   ],
   "source": [
    "# check to see we have the same number of words as before\n",
    "print(len(ft_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Columns: 200 entries, 0 to 199\n",
      "dtypes: float64(200)\n",
      "memory usage: 156.3 KB\n"
     ]
    }
   ],
   "source": [
    "ft_x_df = get_feature_df(ft_model, rb_df)\n",
    "ft_x_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2},\n",
       " '2': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2},\n",
       " '3': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 3},\n",
       " '4': {'precision': 0.3333333333333333,\n",
       "  'recall': 0.5,\n",
       "  'f1-score': 0.4,\n",
       "  'support': 4},\n",
       " '5': {'precision': 0.5882352941176471,\n",
       "  'recall': 0.7142857142857143,\n",
       "  'f1-score': 0.6451612903225806,\n",
       "  'support': 14},\n",
       " 'accuracy': 0.48,\n",
       " 'macro avg': {'precision': 0.1843137254901961,\n",
       "  'recall': 0.24285714285714288,\n",
       "  'f1-score': 0.20903225806451614,\n",
       "  'support': 25},\n",
       " 'weighted avg': {'precision': 0.38274509803921575,\n",
       "  'recall': 0.48,\n",
       "  'f1-score': 0.4252903225806451,\n",
       "  'support': 25}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lightGBM(ft_x_df, y_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next\n",
    "\n",
    "* Use the same steps here and create features using word2vec on larger reviews sample set and generate feature vectors for each review - ie, 50k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "* looking at documentation looks like gensim has doc2vec - should I be using this instead?\n",
    "* parameters for word2vec I just took from your notebook - not sure what would be reasonable here?\n",
    "* I tried using corpus_file instead of sentences as suggested by documentation but got the following error message\n",
    "\n",
    "```\n",
    "    ---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "<ipython-input-15-eb0baade2204> in <module>\n",
    "      8 model = Word2Vec(corpus_file=review_body_file, size=feature_size, \n",
    "      9                           window=window_context, min_count=min_word_count,\n",
    "---> 10                           sample=sample, iter=iter)\n",
    "\n",
    "TypeError: __init__() got an unexpected keyword argument 'corpus_file'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
