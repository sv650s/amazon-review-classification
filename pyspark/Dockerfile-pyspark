# Creates a Docker Image with PySpark
#
# references:
# https://towardsdatascience.com/a-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2
# base of this docker file - but we will be using the version where spark and hadoop are bundled together
# https://github.com/sebnyberg/pyspark-alpine/blob/master/Dockerfile
# How to install spark/hadoop
# https://medium.com/@GalarnykMichael/install-spark-on-ubuntu-pyspark-231c45677de0

FROM openjdk:8-jdk-alpine

# Get these from docker-compose (with defaults)
ARG spark_version=2.4.4
ARG hadoop_version=2.7
ARG conda_version=4.3.14

ENV HADOOP_VERSION $hadoop_version
#ENV HADOOP_HOME /usr/hadoop-$HADOOP_VERSION
#ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

ENV SPARK_VERSION $spark_version
ENV SPARK_PACKAGE spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}
ENV SPARK_HOME /usr/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}
#ENV SPARK_DIST_CLASSPATH="$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*"
ENV PYSPARK_DRIVER_PYTHON jupyter
ENV PYSPARK_DRIVER_PYTHON_OPTS lab

ENV PYSPARK_PYTHON python3
ENV PYSPARK_DRIVER_PYTHON=python3

ENV PATH $PATH:$JAVA_HOME/bin:$SPARK_HOME/bin
#ENV PATH $PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$SPARK_HOME/bin

# Conda envs
ENV CONDA_DIR=/opt/conda CONDA_VER=${conda_version}
ENV PATH=$CONDA_DIR/bin:$PATH SHELL=/bin/bash LANG=C.UTF-8


# add specified user and group
# get args from docker-compose file
ARG user=pyspark
ARG group=pyspark

# create user and groups
RUN addgroup -S $group && adduser -S $user -G $group

COPY environment.yml /home/$user/
# TODO: remove this later
RUN echo "conda: `which conda`"
RUN echo "`ls /opt/conda/bin`"
RUN echo "`whoami`"
RUN echo "`ls /home/$user`"
RUN echo "`conda --help`"
#RUN /opt/conda/bin/conda env create --file /home/$user/environment.yml
# This hack is widely applied to avoid python printing issues in docker containers.
# See: https://github.com/Docker-Hub-frolvlad/docker-alpine-python3/pull/13
ENV PYTHONUNBUFFERED=1

# Update apk and Install Mini Conda
RUN set -ex \
  && apk add --no-cache bash \
  && apk add --virtual .fetch-deps --no-cache ca-certificates wget curl \
  \
  # install spark + hadoop
  && echo "*** Installing spark/hadoop ***" \
  && curl -sL --retry 3 "http://apache-mirror.8birdsvideo.com/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" | \
    tar -zx -C /usr \
  && chown -R root:root $SPARK_HOME
  \
  && echo "*** Installing mini conda ***" \
  && wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub \
  && wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.28-r0/glibc-2.28-r0.apk \
  && apk add --virtual .conda-deps glibc-2.28-r0.apk \
  \
  # install mini conda
  && mkdir -p $CONDA_DIR  \
  && echo export PATH=$CONDA_DIR/bin:'$PATH' > /etc/profile.d/conda.sh \
  && wget https://repo.continuum.io/miniconda/Miniconda3-${CONDA_VER}-Linux-x86_64.sh -O miniconda.sh \
  && bash miniconda.sh -f -b -p $CONDA_DIR \
  && rm miniconda.sh \
  \
  && conda update conda \
  \
   # this will rollback wget ca-cert and curl
  && apk del .fetch-deps


# this removes glibc-2.28-r0.apk
# if you run this it also removes conda for some reason
#  && apk del .conda-deps

# TODO: take this out later
RUN echo "conda: `which conda`"
RUN conda --version

# TODO: take this out later
RUN echo "conda: `which conda`"

# create virtual env using conda
RUN conda env create --file /home/$user/environment.yml

# TODO: remove this
RUN echo "`which bash`"

# swicthc default shell to bash so conda can run
SHELL [ "/bin/bash", "-c" ]

USER $user

# TODO: change this later to test out pyspark
#CMD jupyter lab
CMD source activate pyspark && jupyter lab
#CMD source activate pyspark && ${SPARK_HOME}/bin/pyspark




