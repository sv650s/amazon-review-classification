# Creates a Docker Image with PySpark
#
# references:
# https://towardsdatascience.com/a-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2
# base of this docker file - but we will be using the version where spark and hadoop are bundled together
# https://github.com/sebnyberg/pyspark-alpine/blob/master/Dockerfile
# How to install spark/hadoop
# https://medium.com/@GalarnykMichael/install-spark-on-ubuntu-pyspark-231c45677de0

FROM openjdk:8-jdk-alpine

# Get these from docker-compose (with defaults)
ARG spark_version=2.4.4
ARG hadoop_version=2.7
ARG conda_version=4.3.14

ENV HADOOP_VERSION $hadoop_version
#ENV HADOOP_HOME /usr/hadoop-$HADOOP_VERSION
#ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

ENV SPARK_VERSION $spark_version
ENV SPARK_PACKAGE spark-${SPARK_VERSION}-bin-without-hadoop
ENV SPARK_HOME /usr/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}
#ENV SPARK_DIST_CLASSPATH="$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*"

ENV PYSPARK_PYTHON python3
ENV PYSPARK_DRIVER_PYTHON=python3

ENV PATH $PATH:$JAVA_HOME/bin:$SPARK_HOME/bin
#ENV PATH $PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$SPARK_HOME/bin

# Conda envs
ENV CONDA_DIR=/opt/conda CONDA_VER=${conda_version}
ENV PATH=$CONDA_DIR/bin:$PATH SHELL=/bin/bash LANG=C.UTF-8


# add specified user and group
# get args from docker-compose file
ARG user=pyspark
ARG group=pyspark

# create user and groups
RUN addgroup -S $group && adduser -S $user -G $group

COPY environment.yml /home/$user/
RUN echo "conda: `which conda`"
RUN echo "`ls /opt/conda/bin`"
RUN echo "`whoami`"
RUN echo "`ls /home/$user`"
RUN echo "`conda --help`"
#RUN /opt/conda/bin/conda env create --file /home/$user/environment.yml

# Update apk and Install Mini Conda
RUN set -ex \
  && apk add --no-cache bash \
  && apk add --virtual .fetch-deps --no-cache ca-certificates wget curl \
  \
  && wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub \
  && wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.28-r0/glibc-2.28-r0.apk \
  && apk add --virtual .conda-deps glibc-2.28-r0.apk \
  \
  # install mini conda
  && mkdir -p $CONDA_DIR  \
  && echo export PATH=$CONDA_DIR/bin:'$PATH' > /etc/profile.d/conda.sh \
  && wget https://repo.continuum.io/miniconda/Miniconda3-${CONDA_VER}-Linux-x86_64.sh -O miniconda.sh \
  && bash miniconda.sh -f -b -p $CONDA_DIR \
  && rm miniconda.sh \
  \
  && conda update conda
#  \
#  && apk del .fetch-deps \
# if you run this - conda doesn't work later for some reason???
#  && apk del .conda-deps

# TODO: take this out later
RUN echo "conda: `which conda`"
RUN conda --version

# spark and hadoop comes bundled together now
# http://apache-mirror.8birdsvideo.com/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz
# TODO: remove this later and curl from source
COPY spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz /usr/
RUN cd /usr \
    && tar -zxf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
    && chown -R root:root $SPARK_HOME

# TODO: take this out later
RUN echo "conda: `which conda`"

# create virtual env using conda
RUN conda env create --file /home/$user/environment.yml

# run all commands from here on as $user
#USER $user

#RUN curl -sL --retry 3 \
#    "http://apache-mirror.8birdsvideo.com/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" \
#    | tar -xzvf -C /usr/ \
#    && chown -R root:root $SPARK_HOME


# Install hadoop
#RUN  curl -sL --retry 3 \
#  "http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz" \
#  | gunzip \
#  | tar -x -C /usr/ \
#  && rm -rf $HADOOP_HOME/share/doc \
#  && chown -R root:root $HADOOP_HOME \

# spark
#RUN curl -sL --retry 3 \
#  "https://www.apache.org/dyn/mirrors/mirrors.cgi?action=download&filename=spark/spark-${SPARK_VERSION}/${SPARK_PACKAGE}.tgz" \
#  | gunzip \
#  | tar x -C /usr/ \
#  && mv /usr/$SPARK_PACKAGE $SPARK_HOME \
#  && chown -R root:root $SPARK_HOME \
#  # cleanup

#  COPY config ${SPARK_HOME}/conf


RUN echo "`which activate`"

USER $user
#CMD conda activate pyspark-jupyter

#CMD ["/bin/bash"]
#CMD ["jupyter lab"]
# TODO: change this later to test out pyspark
CMD source activate pyspark && ${SPARK_HOME}/bin/pyspark




