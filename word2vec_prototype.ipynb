{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Embedding as Features\n",
    "\n",
    "I'm actually not entirely sure if I understand how to generate features using word2vec so I wanted to start this notebook and run this by you before spending time to train aon our feature set and training models\n",
    "\n",
    "For this notebook, I will only be working with 100 reviews\n",
    "\n",
    "Purpose is to show snippeets of code to make sure that I'm going out this in the right way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the raw text from amazon reviews\n",
    "\n",
    "This data has already been pre-proocessed by [amazon_review_preprocessor.py](amazon_review_preprocessor.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-csv-100-preprocessed.csv\"\n",
    "df = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_df = df[\"review_body\"]\n",
    "y_df = df[\"star_rating\"]\n",
    "sample_size = rb_df.shape\n",
    "# write out the review_body in LineSentence format - docs says there should be performance gains here\n",
    "review_body_file = \"dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00-csv-100-preprocessed-review_body.csv\"\n",
    "rb_df.to_csv(review_body_file, header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordPunctTokenizer\n",
    "wpt = WordPunctTokenizer()\n",
    "documents = [wpt.tokenize(value) for index, value in rb_df.iteritems()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train word2vec to get word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "feature_size = 200    # Word vector dimensionality  \n",
    "window_context = 30          # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "iter = 50 # number of iterations over corpus\n",
    "\n",
    "model = Word2Vec(documents, size=feature_size, \n",
    "                          window=window_context, min_count=min_word_count,\n",
    "                          sample=sample, iter=iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save off model to be used later\n",
    "model.save(f\"models/word2vec-{sample_size}-{feature_size}.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inspect how many words are in our vocabulary\n",
    "\n",
    "Looks like there are about a thousand words in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good: (200,)\n",
      "product: (200,)\n",
      "please: (200,)\n",
      "note: (200,)\n",
      "not: (200,)\n",
      "floating: (200,)\n",
      "case: (200,)\n",
      "do: (200,)\n",
      "not: (200,)\n",
      "clai: (200,)\n",
      "somehow: (200,)\n",
      "thinking: (200,)\n",
      "good: (200,)\n",
      "price: (200,)\n",
      "does: (200,)\n",
      "says: (200,)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "first_review = rb_df.iloc[0]\n",
    "vector_list = []\n",
    "for word in wpt.tokenize(first_review):\n",
    "#     print(f'{word}: {model.wv.get_vector(word)}')\n",
    "    word_vector = model.wv.get_vector(word)\n",
    "    print(f'{word}: {word_vector.shape}')\n",
    "    vector_list.append(word_vector)\n",
    "\n",
    "review_feature = np.average(vector_list, axis=0)\n",
    "print(review_feature.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we come up with a vector that respresents each of the reviews by averaging word vectors for every word in the review body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Columns: 200 entries, 0 to 199\n",
      "dtypes: float64(200)\n",
      "memory usage: 156.3 KB\n"
     ]
    }
   ],
   "source": [
    "def get_review_vector(review):\n",
    "    # returns a list of word vectors for all words im review\n",
    "    word_vectors = [model.wv.get_vector(word) for word in wpt.tokenize(review)]\n",
    "#     print(len(word_vectors))\n",
    "    # average all word vectors to come up with final vector for the review\n",
    "    return np.average(word_vectors, axis=0)\n",
    "\n",
    "# generate new feature DF\n",
    "print(len(rb_df))\n",
    "x_df = pd.DataFrame()\n",
    "for index, review in rb_df.iteritems():\n",
    "    feature_vector = get_review_vector(review)\n",
    "    # turn this into dictionary so we can add it as row to DF\n",
    "    feature_dict = dict(enumerate(feature_vector))\n",
    "    x_df = x_df.append(feature_dict, ignore_index=True)\n",
    "    \n",
    "x_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we train a model and see how we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinceluk/anaconda3/envs/capstone/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1},\n",
       " '2': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1},\n",
       " '3': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 5},\n",
       " '4': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 4},\n",
       " '5': {'precision': 0.5,\n",
       "  'recall': 0.6428571428571429,\n",
       "  'f1-score': 0.5625000000000001,\n",
       "  'support': 14},\n",
       " 'accuracy': 0.36,\n",
       " 'macro avg': {'precision': 0.1,\n",
       "  'recall': 0.1285714285714286,\n",
       "  'f1-score': 0.11250000000000002,\n",
       "  'support': 25},\n",
       " 'weighted avg': {'precision': 0.28,\n",
       "  'recall': 0.36,\n",
       "  'f1-score': 0.31500000000000006,\n",
       "  'support': 25}}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_df, y_df)\n",
    "\n",
    "gb = lgb.LGBMClassifier(objective=\"multiclass\", num_threads=2,\n",
    "                        seed=1)\n",
    "\n",
    "gb.fit(x_train, y_train)\n",
    "y_predict = gb.predict(x_test)\n",
    "\n",
    "report = classification_report(y_test, y_predict, output_dict=True)\n",
    "\n",
    "report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next\n",
    "\n",
    "* Use the same steps here and create features using word2vec on larger reviews sample set and generate feature vectors for each review - ie, 50k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "* looking at documentation looks like gensim has doc2vec - should I be using this instead?\n",
    "* parameters for word2vec I just took from your notebook - not sure what would be reasonable here?\n",
    "* I tried using corpus_file instead of sentences as suggested by documentation but got the following error message\n",
    "\n",
    "```\n",
    "    ---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "<ipython-input-15-eb0baade2204> in <module>\n",
    "      8 model = Word2Vec(corpus_file=review_body_file, size=feature_size, \n",
    "      9                           window=window_context, min_count=min_word_count,\n",
    "---> 10                           sample=sample, iter=iter)\n",
    "\n",
    "TypeError: __init__() got an unexpected keyword argument 'corpus_file'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
