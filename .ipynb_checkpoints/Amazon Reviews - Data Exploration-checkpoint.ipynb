{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Review Data Exploration\n",
    "\n",
    "https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt\n",
    "\n",
    "SAMPLE CONTENT:\n",
    "https://s3.amazonaws.com/amazon-reviews-pds/tsv/sample_us.tsv\n",
    "https://s3.amazonaws.com/amazon-reviews-pds/tsv/sample_fr.tsv\n",
    "\n",
    "|DATA COLUMNS:| Description |\n",
    "|-------------|--------|\n",
    "|marketplace       | 2 letter country code of the marketplace where the review was written. |\n",
    "|customer_id       | Random identifier that can be used to aggregate reviews written by a single author. |\n",
    "|review_id         | The unique ID of the review. |\n",
    "|product_id        | The unique Product ID the review pertains to. In the multilingual dataset the reviews                    for the same product in different countries can be grouped by the same product_id. |\n",
    "|product_parent    | Random identifier that can be used to aggregate reviews for the same product. |\n",
    "|product_title     | Title of the product. |\n",
    "|product_category  | Broad product category that can be used to group reviews (also used to group the dataset into coherent parts). |\n",
    "|star_rating       | The 1-5 star rating of the review. |\n",
    "|helpful_votes     | Number of helpful votes. |\n",
    "|total_votes       | Number of total votes the review received. |\n",
    "|vine              | Review was written as part of the Vine program. |\n",
    "|verified_purchase | The review is on a verified purchase. |\n",
    "|review_headline   | The title of the review. |\n",
    "|review_body       | The review text. |\n",
    "|review_date       | The date the review was written. |\n",
    "\n",
    "DATA FORMAT\n",
    "Tab ('\\t') separated text file, without quote or escape characters.\n",
    "First line in each file is header; 1 line corresponds to 1 record.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# global variables\n",
    "COLUMNS_TO_DROP=[\"marketplace\", \"vine\", \"verified_purchase\"]\n",
    "USE_PANDAS=False\n",
    "\n",
    "\n",
    "# sample file\n",
    "# DATA_FILE=\"dataset/amazon_reviews/sample_us.tsv\"\n",
    "\n",
    "# first 10k entries from Wireless category\n",
    "# DATA_FILE=\"dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00_10k.tsv\"\n",
    "\n",
    "# first 500k entries from Wireless category\n",
    "DATA_FILE=\"dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00_500k.tsv\"\n",
    "\n",
    "# first 1mil entries from Wireless category\n",
    "# DATA_FILE=\"dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00_1mil.tsv\"\n",
    "\n",
    "# full 9mil Wireless reviews - not enough memory locally to do this\n",
    "# DATA_FILE=\"dataset/amazon_reviews/amazon_reviews_us_Wireless_v1_00.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read using pandas\n",
    "\n",
    "## This doesn't seem to be working if you look down later for analysis for wc from headlines - lines that are saying that there are 22 columns it's not reading new lines properly even though I checked in vi and they have the right number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if USE_PANDAS == True:\n",
    "\n",
    "    # pandas is doing something weird. The lines that says have bad number of columns actually have the right columns???\n",
    "    reviews_df = pd.read_csv(DATA_FILE, sep=\"\\t\", \n",
    "                             parse_dates=[\"review_date\"], \n",
    "                             warn_bad_lines=True, \n",
    "                             error_bad_lines=False)\n",
    "    reviews_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try reading line by line and see if this will be better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 10000\n",
      "processed 20000\n",
      "processed 30000\n",
      "processed 40000\n",
      "processed 50000\n",
      "processed 60000\n",
      "processed 70000\n",
      "processed 80000\n",
      "processed 90000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-111b0114b8be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#                     print(splitted)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                     reviews = reviews.append(pd.Series(splitted, index=columns), \n\u001b[0;32m---> 20\u001b[0;31m                                    ignore_index=True)\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   6690\u001b[0m         return concat(to_concat, ignore_index=ignore_index,\n\u001b[1;32m   6691\u001b[0m                       \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6692\u001b[0;31m                       sort=sort)\n\u001b[0m\u001b[1;32m   6693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6694\u001b[0m     def join(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    227\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                        copy=copy, sort=sort)\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m             new_data = concatenate_block_managers(\n\u001b[1;32m    425\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m                 copy=self.copy)\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   2056\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_uniform_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2057\u001b[0m             b = join_units[0].block.concat_same_type(\n\u001b[0;32m-> 2058\u001b[0;31m                 [ju.block for ju in join_units], placement=placement)\n\u001b[0m\u001b[1;32m   2059\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2060\u001b[0m             b = make_block(\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mconcat_same_type\u001b[0;34m(self, to_concat, placement)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \"\"\"\n\u001b[1;32m    327\u001b[0m         values = self._concatenator([blk.values for blk in to_concat],\n\u001b[0;32m--> 328\u001b[0;31m                                     axis=self.ndim - 1)\n\u001b[0m\u001b[1;32m    329\u001b[0m         return self.make_block_same_class(\n\u001b[1;32m    330\u001b[0m             values, placement=placement or slice(0, len(values), 1))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# not quite ready\n",
    "if USE_PANDAS == False:\n",
    "\n",
    "    columns = [\"marketplace\", \"customer_id\", \"review_id\", \"product_id\",\n",
    "              \"product_parent\", \"product_title\", \"product_category\", \n",
    "              \"star_rating\", \"helpful_votes\", \"total_votes\", \"vine\",\n",
    "              \"verified_purchase\", \"review_headline\", \"review_body\",\n",
    "              \"review_date\"]\n",
    "    reviews = pd.DataFrame(columns = columns)\n",
    "    count = 0\n",
    "    with open(DATA_FILE, \"r\") as file:\n",
    "        for line in file:\n",
    "            if len(line) > 0:\n",
    "                line = line.rstrip(\"\\n\")\n",
    "                # skip the header\n",
    "                if count > 0:\n",
    "                    splitted = line.split(\"\\t\")\n",
    "#                     print(splitted)\n",
    "                    reviews = reviews.append(pd.Series(splitted, index=columns), \n",
    "                                   ignore_index=True)\n",
    "                count += 1\n",
    "                if count % 10000 == 0:\n",
    "                    print(f\"processed {count}\")\n",
    "#                 if count == 3:\n",
    "#                     break\n",
    "\n",
    "    print(len(reviews))\n",
    "\n",
    "    \n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like sometimes we have reviews with no body, no headline, and no review dates\n",
    "# let's drop those rows with missing data\n",
    "# also drop the following columns since they will always be the same\n",
    "clean_df = reviews_df.dropna().drop(COLUMNS_TO_DROP, axis = 1)\n",
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many products do we have?\n",
    "total_products = clean_df.groupby(\"product_parent\").count()\n",
    "print(clean_df.groupby(\"product_parent\").size().describe())\n",
    "# 170k products - most only have 1 reviews\n",
    "\n",
    "clean_df.groupby(\"product_parent\").size().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ??? I want to filter after group by to see what the day looks like, but this code commented below runs super slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ???is there a better way to do this????\n",
    "# can we do some type of quantile or adaptive binning that would make sense?\n",
    "\n",
    "\n",
    "# # how many products have 1 reviews\n",
    "# products_one_review = len(clean_df.groupby(\"product_parent\").filter(lambda x: len(x) == 1).groupby(\"product_parent\"))\n",
    "\n",
    "# # how many products have 2 to 3 reviews\n",
    "# products_two_reviews = len(clean_df.groupby(\"product_parent\").filter(lambda x: 2 <= len(x) <= 3).groupby(\"product_parent\"))\n",
    "\n",
    "# # only 39k products have > 3 reviews\n",
    "# products_three_plus_reviews = len(clean_df.groupby(\"product_parent\").filter(lambda x: len(x) > 3).groupby(\"product_parent\"))\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"1 review:\\t{round(products_one_review/total_products, 2) * 100}% ({products_one_review})\")\n",
    "# print(f\"2-3 review:\\t {round(products_two_reviews/total_products, 2) * 100}% ({products_two_reviews})\")\n",
    "# print(f\"3+ review:\\t {round(products_three_plus_reviews/total_products, 2) * 100}% ({products_three_plus_reviews})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying some type of binning\n",
    "\n",
    "products_df = pd.DataFrame(clean_df.groupby(\"product_parent\").size(), columns=[\"count\"])\n",
    "products_df = products_df.assign(bin=lambda x: pd.cut(x[\"count\"], [0, 1, 3, x[\"count\"].max()]))\n",
    "products_df.groupby(\"bin\").size().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's look at review distribution dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# distribution of months\n",
    "\n",
    "df = clean_df\n",
    "df[\"month\"] = df.review_date.dt.month\n",
    "df.groupby(\"month\").size().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check to see if this is correct\n",
    "\n",
    "print(df[\"review_date\"].min())\n",
    "print(df[\"review_date\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of days of the week\n",
    "# distribution of months\n",
    "\n",
    "df = clean_df\n",
    "df[\"day\"] = df.review_date.dt.dayofweek\n",
    "df.groupby(\"day\").size().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Stars Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at distribution of stars\n",
    "clean_df.groupby(\"star_rating\").size().plot(kind='bar')\n",
    "# reviews lean heavily towards 5-stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at what percentage of reviews have votes\n",
    "print(f\"{round(len(clean_df[(clean_df.total_votes > 0)]) / len(clean_df), 2)* 100} percent \"\\\n",
    "      f\"({len(clean_df[(clean_df.total_votes > 0)])}) has votes\")\n",
    "# let's look at what percentage of reviews have votes\n",
    "print(f\"{round(len(clean_df[(clean_df.helpful_votes > 0)]) / len(clean_df), 2)* 100} percent \"\\\n",
    "            f\"({len(clean_df[(clean_df.helpful_votes > 0)])}) has helpful votes\")\n",
    "\n",
    "\n",
    "# let's look at distribution of total_votes\n",
    "clean_df.describe()\n",
    "# looks like most do not have any votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the most frequent number of words in headline is 2\n",
    "\n",
    "50% have 2700 words or less in headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at wordcount for headlines\n",
    "df = clean_df\n",
    "# this doesn't work\n",
    "# df = df.apply(review_headline_wc=lambda x: len(x[\"review_headline\"].str.split()))\n",
    "# this doesn't work either - seems to be applying split across all headlines\n",
    "# df[\"review_headline_wc\"] = df[\"review_headline\"].str.split().count()\n",
    "\n",
    "\n",
    "df[\"review_headline_wc\"] = df[\"review_headline\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "grouped = df.groupby(\"review_headline_wc\")\n",
    "print(grouped.size().describe())\n",
    "\n",
    "grouped.size().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting.. I don't think Pandas is reading these rows correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_df\n",
    "for index, row in df[(df[\"review_headline_wc\"] > 2500)].head(1).iterrows():\n",
    "    print(f'{index} headline: [{row[\"review_headline\"]}]')\n",
    "    print(f'{index} body: [{row[\"review_body\"]}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_df\n",
    "\n",
    "# now let's look at distribution of wc for review body\n",
    "df[\"review_body_wc\"] = df[\"review_body\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Looks like most 50% of reviews have 6 words or less\n",
    "\n",
    "At 75% percentile we start seeing reviews with 6+ words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_df\n",
    "\n",
    "# now let's look at distribution of wc for review body\n",
    "df[\"review_body_wc\"] = df[\"review_body\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "grouped = df.groupby(\"review_body_wc\")\n",
    "print(grouped.size().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# let's plot by quantile\n",
    "\n",
    "quantile_list = [0, .10, .20, .30, .40, .50, .60, .70, .80, .90, 1.]\n",
    "quantiles = df[\"review_body_wc\"].quantile(quantile_list)\n",
    "\n",
    "print(quantiles)\n",
    "\n",
    "df = df.assign(review_body_quantile=lambda x: pd.cut(x[\"review_body_wc\"], quantiles.array))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"review_body_quantile\").size().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# let's try fixed binning - 10 fixed bins\n",
    "\n",
    "fixed_bin = df.assign(review_body_fixed_bin=lambda x: pd.cut(x[\"review_body_wc\"], \n",
    "                                                                   np.arange(0, 7000, 700)))\n",
    "fixed_bin.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_bin.groupby(\"review_body_fixed_bin\").size().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conslusion\n",
    "\n",
    "looks like Yelp reviews tends to be more verbose than Amazon\n",
    "\n",
    "90% of Amazon reviews have ~80 words or less compared to yelp reviews which is around ~55%\n",
    "\n",
    "## Update - 4/25 - Pandas is having issues reading the lines correctly for Amazon reviews so I think some of this data is incorrect\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
